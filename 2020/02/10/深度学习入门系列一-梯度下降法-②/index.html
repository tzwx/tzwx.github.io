<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>深度学习入门系列2: 梯度下降法 ② | 子规声里驻年光</title><meta name="description" content="深度学习入门系列2: 梯度下降法 ②"><meta name="keywords" content="DeepLearning"><meta name="author" content="桦七"><meta name="copyright" content="桦七"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/hq.png"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="深度学习入门系列2: 梯度下降法 ②"><meta name="twitter:description" content="深度学习入门系列2: 梯度下降法 ②"><meta name="twitter:image" content="https://user-images.githubusercontent.com/60562661/74164739-2257b980-4c5f-11ea-8f88-c66a8efc3aa7.jpg"><meta property="og:type" content="article"><meta property="og:title" content="深度学习入门系列2: 梯度下降法 ②"><meta property="og:url" content="http://yoursite.com/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/"><meta property="og:site_name" content="子规声里驻年光"><meta property="og:description" content="深度学习入门系列2: 梯度下降法 ②"><meta property="og:image" content="https://user-images.githubusercontent.com/60562661/74164739-2257b980-4c5f-11ea-8f88-c66a8efc3aa7.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://yoursite.com/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/"><link rel="prev" title="深度学习入门系列3: 模型的误差来源及其改进" href="http://yoursite.com/2020/02/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%AF%AF%E5%B7%AE%E6%9D%A5%E8%87%AA%E5%93%AA%E9%87%8C/"><link rel="next" title="深度学习入门系列1: 梯度下降法①" href="http://yoursite.com/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://huaqi.blue/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: true,
  copyright: {"languages":{"author":"Author: 桦七","link":"Link: http://yoursite.com/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/","source":"Source: 子规声里驻年光","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">子规声里驻年光</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-picture-o"></i><span> Gallery</span></a></div></div></span><span class="pull_right" id="search_button"><a class="site-page social-icon search"><i class="fa fa-search fa-fw"></i><span> Search</span></a></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/tx2.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">14</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">13</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">5</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-picture-o"></i><span> Gallery</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">Catalog</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#梯度下降法及其优化"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">梯度下降法及其优化</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Tip1：Adaptive-Learning-Rate"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text">Tip1：Adaptive Learning Rate</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Adagrad-Gradient-Descent"><span class="toc_mobile_items-number">1.2.</span> <span class="toc_mobile_items-text">Adagrad Gradient Descent</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#与Gradient-Descent的比较"><span class="toc_mobile_items-number">1.2.1.</span> <span class="toc_mobile_items-text">与Gradient Descent的比较</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#数学公式推导"><span class="toc_mobile_items-number">1.2.2.</span> <span class="toc_mobile_items-text">数学公式推导</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#其中的矛盾"><span class="toc_mobile_items-number">1.2.3.</span> <span class="toc_mobile_items-text">其中的矛盾</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#Adagrad总结"><span class="toc_mobile_items-number">1.2.4.</span> <span class="toc_mobile_items-text">Adagrad总结</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Tip2：Stochastic-Gradient-Descent"><span class="toc_mobile_items-number">1.3.</span> <span class="toc_mobile_items-text">Tip2：Stochastic Gradient Descent</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Tip3：Feature-scaling"><span class="toc_mobile_items-number">1.4.</span> <span class="toc_mobile_items-text">Tip3：Feature scaling</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Code-梯度下降法python实现"><span class="toc_mobile_items-number">1.5.</span> <span class="toc_mobile_items-text">Code:梯度下降法python实现</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#背景："><span class="toc_mobile_items-number">1.5.1.</span> <span class="toc_mobile_items-text">背景：</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#代码实现"><span class="toc_mobile_items-number">1.5.2.</span> <span class="toc_mobile_items-text">代码实现</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#代码中的一些细节"><span class="toc_mobile_items-number">1.5.3.</span> <span class="toc_mobile_items-text">代码中的一些细节</span></a></li></ol></li></ol></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#梯度下降法及其优化"><span class="toc-number">1.</span> <span class="toc-text">梯度下降法及其优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Tip1：Adaptive-Learning-Rate"><span class="toc-number">1.1.</span> <span class="toc-text">Tip1：Adaptive Learning Rate</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adagrad-Gradient-Descent"><span class="toc-number">1.2.</span> <span class="toc-text">Adagrad Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#与Gradient-Descent的比较"><span class="toc-number">1.2.1.</span> <span class="toc-text">与Gradient Descent的比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#数学公式推导"><span class="toc-number">1.2.2.</span> <span class="toc-text">数学公式推导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#其中的矛盾"><span class="toc-number">1.2.3.</span> <span class="toc-text">其中的矛盾</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Adagrad总结"><span class="toc-number">1.2.4.</span> <span class="toc-text">Adagrad总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tip2：Stochastic-Gradient-Descent"><span class="toc-number">1.3.</span> <span class="toc-text">Tip2：Stochastic Gradient Descent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tip3：Feature-scaling"><span class="toc-number">1.4.</span> <span class="toc-text">Tip3：Feature scaling</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Code-梯度下降法python实现"><span class="toc-number">1.5.</span> <span class="toc-text">Code:梯度下降法python实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#背景："><span class="toc-number">1.5.1.</span> <span class="toc-text">背景：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#代码实现"><span class="toc-number">1.5.2.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#代码中的一些细节"><span class="toc-number">1.5.3.</span> <span class="toc-text">代码中的一些细节</span></a></li></ol></li></ol></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(https://user-images.githubusercontent.com/60562661/74164739-2257b980-4c5f-11ea-8f88-c66a8efc3aa7.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">深度学习入门系列2: 梯度下降法 ②</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> Created 2020-02-10<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> Updated 2020-02-12</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%8A%80%E6%9C%AF/">技术</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%8A%80%E6%9C%AF/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>Word count:</span><span class="word-count">1.2k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>Reading time: 4 min</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>Post View:</span><span id="busuanzi_value_page_pv"></span><span class="post-meta__separator">|</span><i class="fa fa-comments-o post-meta__icon fa-fw" aria-hidden="true"></i><span>comments:</span><a href="/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count comment-count" data-xid="/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/" itemprop="commentCount"></span></a></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><html><head></head><body><h1 id="梯度下降法及其优化"><a href="#梯度下降法及其优化" class="headerlink" title="梯度下降法及其优化"></a>梯度下降法及其优化</h1><p>在上一篇文章中已经讲述了梯度下降法的基本流程，也讲了梯度下降法存在的问题，这篇文章就来继续讲解梯度下降法的后续。</p>
<p>前一篇文章提到学习率的问题，如果学习率过大会导致震荡而难以收敛，过小的话又会收敛太慢，耗费时间，因此出现了Adagrad.</p>
<h2 id="Tip1：Adaptive-Learning-Rate"><a href="#Tip1：Adaptive-Learning-Rate" class="headerlink" title="Tip1：Adaptive Learning Rate"></a>Tip1：Adaptive Learning Rate</h2><p>学习率对于参数调整影响很大，因此需要仔细地调整，手动调节参数肯定是不太现实，而自适应的调整参数有以下两个原则：</p>
<ul>
<li>开始阶段学习率比较大</li>
<li>随着迭代次数的增加越来越小</li>
</ul>
<h2 id="Adagrad-Gradient-Descent"><a href="#Adagrad-Gradient-Descent" class="headerlink" title="Adagrad Gradient Descent"></a>Adagrad Gradient Descent</h2><h3 id="与Gradient-Descent的比较"><a href="#与Gradient-Descent的比较" class="headerlink" title="与Gradient Descent的比较"></a>与Gradient Descent的比较</h3><p><a href="https://user-images.githubusercontent.com/60562661/74164696-17048e00-4c5f-11ea-8f27-b56011c83c1a.png" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt data-src="https://user-images.githubusercontent.com/60562661/74164696-17048e00-4c5f-11ea-8f27-b56011c83c1a.png" class="lazyload" title></a></p>
<h3 id="数学公式推导"><a href="#数学公式推导" class="headerlink" title="数学公式推导"></a>数学公式推导</h3><p>原始学习率变成了 <code>常数/梯度的平方和开根号</code>，这个公式也是又推导过程的，如下(笔记字比较丑)：</p>
<p><a href="https://user-images.githubusercontent.com/60562661/74164719-1cfa6f00-4c5f-11ea-985b-cca1a74e4a77.png" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt data-src="https://user-images.githubusercontent.com/60562661/74164719-1cfa6f00-4c5f-11ea-985b-cca1a74e4a77.png" class="lazyload" title></a></p>
<h3 id="其中的矛盾"><a href="#其中的矛盾" class="headerlink" title="其中的矛盾"></a>其中的矛盾</h3><p>观察最后一步，可以看出后面的  *<em>梯度 *</em> 和  *<em>分母的梯度的平方和开根号 *</em> （最后一行红笔画出来的）是矛盾的，梯度越大，分母也会越大，约束整个函数，但是效果会比较好。这个可以解释为，不只是考虑了一阶偏导数g(t),同时也考虑了二阶偏导数，所以结果会更加准确。(具体的可以参考李宏毅深度学习视频，前一篇文章有提到。)</p>
<h3 id="Adagrad总结"><a href="#Adagrad总结" class="headerlink" title="Adagrad总结"></a>Adagrad总结</h3><p>Adagrad梯度下降法其实是实现了自适应的去调节学习率，不在需要手动的仔细去调节。</p>
<h2 id="Tip2：Stochastic-Gradient-Descent"><a href="#Tip2：Stochastic-Gradient-Descent" class="headerlink" title="Tip2：Stochastic Gradient Descent"></a>Tip2：Stochastic Gradient Descent</h2><p>随机梯度下降法也是比较常见的，是梯度下降法的一种变体，改变也不多，它与梯度下降法的不同之处在于：</p>
<p>​        假设有十组训练数据，也就是十个函数，梯度下降法会把每一组参数带进去计算损失求和，每组参数的梯度也相应会进行求和嘛，然后才更新一次参数；随机梯度下降法不再需要求和这一过程，即随便一组数据带入损失函数求出梯度直接更新参数。</p>
<p>这种方法听起来也比较一般，它的主要优点在于<strong>更新参数快，</strong>  天下武功，唯快不破嘛。</p>
<h2 id="Tip3：Feature-scaling"><a href="#Tip3：Feature-scaling" class="headerlink" title="Tip3：Feature scaling"></a>Tip3：Feature scaling</h2><p>特征归一化。在做梯度下降法时，只要梯度分不一样，可以归一化之后在做，主要是因为如果某一个特征值非常大，那么它所占的比重就会非常大，这对学习非常不利。</p>
<p>常用的一个归一化方法：</p>
<p>对于特征 x1,x2,x3······xn,每种特征的某一个维度求一下这列数的均值和标准差，然后原始数据减去均值除以标准差即可归一化，如下图：</p>
<p><a href="https://user-images.githubusercontent.com/60562661/74164700-18ce5180-4c5f-11ea-880f-770b8016f100.png" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt data-src="https://user-images.githubusercontent.com/60562661/74164700-18ce5180-4c5f-11ea-880f-770b8016f100.png" class="lazyload" title></a></p>
<p><strong>这样做之后这一维数据就会服从均值为0，方差为1的高斯分布。</strong></p>
<h2 id="Code-梯度下降法python实现"><a href="#Code-梯度下降法python实现" class="headerlink" title="Code:梯度下降法python实现"></a>Code:梯度下降法python实现</h2><h3 id="背景："><a href="#背景：" class="headerlink" title="背景："></a>背景：</h3><p><strong>函数（model）</strong>：y = w1* x^2 + w2*x + b ,计算出参数 w1 w2 b，给顶的真值是w1_truth = 1.8，w2_truth=2.4，b_truth = 5.6</p>
<p><strong>loss function</strong>   ：均方误差，公式写出太麻烦，就是y的真实值减去y的预测值的平方和</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写一个深度学习的回归案例</span></span><br><span class="line"><span class="comment">#y = w1*x^2 + w2*x + b ,计算出参数 w1 w2 b</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成数据 </span></span><br><span class="line">x_data = np.random.rand(<span class="number">10</span>)</span><br><span class="line">w1_truth = <span class="number">1.8</span></span><br><span class="line">w2_truth = <span class="number">2.4</span></span><br><span class="line">b_truth = <span class="number">5.6</span></span><br><span class="line">y_data = np.random.rand(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    y_data[i] = w1_truth*x_data[i]*x_data[i] + w2_truth*x_data[i] + b_truth</span><br><span class="line">print(y_data.shape)</span><br><span class="line">print(x_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">w1 = <span class="number">6</span></span><br><span class="line">w2 = <span class="number">4</span></span><br><span class="line">b = <span class="number">8</span></span><br><span class="line">lr = <span class="number">1</span></span><br><span class="line">steps = <span class="number">100000</span></span><br><span class="line">lr_w1 = <span class="number">0</span></span><br><span class="line">lr_w2 = <span class="number">0</span></span><br><span class="line">lr_b = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(steps):</span><br><span class="line">    w1_grad = <span class="number">0</span></span><br><span class="line">    w2_grad = <span class="number">0</span></span><br><span class="line">    b_grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        w1_grad = w1_grad - <span class="number">2</span>*(y_data[i] - b-w1*x_data[i]*x_data[i]-w2*x_data[i])* (x_data[i]**<span class="number">2</span>)</span><br><span class="line">        w2_grad = w2_grad - <span class="number">2</span>*(y_data[i] - b-w1*x_data[i]*x_data[i]-w2*x_data[i])* x_data[i]</span><br><span class="line">        b_grad = b_grad - <span class="number">2</span>*(y_data[i] - b-w1*x_data[i]*x_data[i]-w2*x_data[i])* <span class="number">1.0</span></span><br><span class="line">    lr_w1 = lr_w1 + w1_grad**<span class="number">2</span></span><br><span class="line">    lr_w2 = lr_w2 + w2_grad**<span class="number">2</span></span><br><span class="line">    lr_b = lr_b + b_grad**<span class="number">2</span></span><br><span class="line">    <span class="comment"># update</span></span><br><span class="line">    w1 = w1 - lr/np.sqrt(lr_w1) * w1_grad</span><br><span class="line">    w2 = w2 - lr/np.sqrt(lr_w2) * w2_grad</span><br><span class="line">    b  = b  - lr/np.sqrt(lr_b) * b_grad</span><br><span class="line">print(<span class="string">"w1= %f,w2 = %f, b= %f"</span> % (w1,w2,b))</span><br><span class="line"><span class="comment">#以下是代码的输出结果，可以自己测试一下，完美找到了真实值</span></span><br><span class="line"><span class="comment"># w1= 1.800000,w2 = 2.400000, b= 5.600000</span></span><br></pre></td></tr></tbody></table></figure></div>



<h3 id="代码中的一些细节"><a href="#代码中的一些细节" class="headerlink" title="代码中的一些细节"></a><strong>代码中的一些细节</strong></h3><ul>
<li><p>以上代码如果只用一个学习率，收敛会很慢，而且10万个迭代也不会迭代导最终结果，差距还是比较大的，因此代码用的是Adagrad梯度下降法，纯粹手工实现的，可以完美的预测出结果</p>
</li>
<li><p>至于代码中的计算 w1 w2 b 的梯度公式，可以手工推导出来，如下图：</p>
</li>
</ul>
<p><a href="https://user-images.githubusercontent.com/60562661/74164747-24217d00-4c5f-11ea-8506-fdaf73f9c02d.png" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt data-src="https://user-images.githubusercontent.com/60562661/74164747-24217d00-4c5f-11ea-8506-fdaf73f9c02d.png" class="lazyload" title></a></p>
</body></html></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">桦七</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/">http://yoursite.com/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/DeepLearning/">DeepLearning    </a></div><div class="post_share"><div class="social-share" data-image="https://user-images.githubusercontent.com/60562661/74164739-2257b980-4c5f-11ea-8f88-c66a8efc3aa7.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg" alt="支付寶"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/02/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%AF%AF%E5%B7%AE%E6%9D%A5%E8%87%AA%E5%93%AA%E9%87%8C/"><img class="prev_cover lazyload" data-src="https://user-images.githubusercontent.com/60562661/74254491-7a5bf200-4d2b-11ea-841c-bb6c1fe2d0c4.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>深度学习入门系列3: 模型的误差来源及其改进</span></div></a></div><div class="next-post pull_right"><a href="/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"><img class="next_cover lazyload" data-src="https://user-images.githubusercontent.com/60562661/74164708-1b30ab80-4c5f-11ea-921d-071649d05842.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>深度学习入门系列1: 梯度下降法①</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/02/11/深度学习入门系列-深度学习的误差来自哪里/" title="深度学习入门系列3: 模型的误差来源及其改进"><img class="relatedPosts_cover lazyload"data-src="https://user-images.githubusercontent.com/60562661/74254491-7a5bf200-4d2b-11ea-841c-bb6c1fe2d0c4.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-11</div><div class="relatedPosts_title">深度学习入门系列3: 模型的误差来源及其改进</div></div></a></div><div class="relatedPosts_item"><a href="/2020/02/11/深度学习入门系列-逻辑回归/" title="深度学习入门系列4: Logistic Regression"><img class="relatedPosts_cover lazyload"data-src="https://user-images.githubusercontent.com/60562661/74258373-2ce28380-4d31-11ea-9573-5ed9e4e63fd1.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-11</div><div class="relatedPosts_title">深度学习入门系列4: Logistic Regression</div></div></a></div><div class="relatedPosts_item"><a href="/2020/02/10/深度学习入门系列一-梯度下降法/" title="深度学习入门系列1: 梯度下降法①"><img class="relatedPosts_cover lazyload"data-src="https://user-images.githubusercontent.com/60562661/74164708-1b30ab80-4c5f-11ea-921d-071649d05842.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-02-10</div><div class="relatedPosts_title">深度学习入门系列1: 梯度下降法①</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> Comment</span></div><div class="vcomment" id="vcomment"></div><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = false == true ? true : false;
var verify = false == true ? true : false;
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;

window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  appId:'s7Rhlvw79F57TPCoI7ybwiKJ-gzGzoHsz',
  appKey:'oSBw1EY1n1eGhYK9f5jsdual',
  placeholder:'Please leave your footprints',
  avatar:'monsterid',
  guest_info:guest_info,
  pageSize:'10',
  lang:'en',
  recordIP: true
});</script></div></div></main><footer id="footer" style="background-image: url(https://user-images.githubusercontent.com/60562661/74164739-2257b980-4c5f-11ea-8f88-c66a8efc3aa7.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2020 By 桦七</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="Scroll to comment"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script id="canvas_nest" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/canvas-nest.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>