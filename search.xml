<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>个人信息管理系统-JavaWeb项目总结分析</title>
      <link href="/2020/10/07/%E4%B8%AA%E4%BA%BA%E4%BF%A1%E6%81%AF%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F-Web%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93%E5%88%86%E6%9E%90/"/>
      <url>/2020/10/07/%E4%B8%AA%E4%BA%BA%E4%BF%A1%E6%81%AF%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F-Web%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><blockquote><p>最近学习 <code>javaweb</code> 阶段 ，基本学习了原始的 <code>jsp</code> 开发，完成了一个比较综合的案例，记录一下各种操作的逻辑，以及具体的代码实现细节。</p></blockquote><p><a href="https://user-images.githubusercontent.com/60562661/95356015-c9791780-08f8-11eb-9e70-574262bbdd03.gif" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/95356015-c9791780-08f8-11eb-9e70-574262bbdd03.gif" class="lazyload"></a></p><h3 id="技术概览："><a href="#技术概览：" class="headerlink" title="技术概览："></a>技术概览：</h3><p><strong>前端</strong> 页面采用 <strong>原生</strong>   <code>html</code>+ <code>css</code>+ <code>javascript</code> ;</p><p><strong>后端</strong> 采用  <strong>Java， Jsp</strong>开发，具体包含原生 <code>servlet</code>，<code>request</code>，<code>response</code>；</p><p><strong>该系统主要包含功能</strong>：</p><ul><li>登录</li><li>显示用户信息<ul><li>分页显示</li></ul></li><li>添加联系人</li><li>删除联系人：<ul><li>单条删除</li><li>多选删除</li><li>全选全部选</li></ul></li><li>修改联系人；</li><li>根据条件查询联系人；</li></ul><hr><h3 id="项目架构设计"><a href="#项目架构设计" class="headerlink" title="项目架构设计"></a>项目架构设计</h3><p><a href="https://user-images.githubusercontent.com/60562661/95356038-ced66200-08f8-11eb-8d0d-141a07eb8554.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/95356038-ced66200-08f8-11eb-8d0d-141a07eb8554.png" class="lazyload"></a></p><p>如图，首先按web三层结构（ <strong>浏览器 —> 服务器 —> 数据库 </strong>）来分</p><ul><li><strong>web 表示层</strong>  —-  包括  控制器 <strong>Servlet</strong>  <strong>Jsp页面</strong></li><li><strong>业务逻辑层</strong> —- 业务实现逻辑</li><li><strong>数据访问层</strong> —- 具体操作数据库</li></ul><p>首先拿到前端页面，修改一下逻辑</p><hr><h3 id="功能实现"><a href="#功能实现" class="headerlink" title="功能实现"></a>功能实现</h3><h4 id="登录-Login"><a href="#登录-Login" class="headerlink" title="登录 Login"></a>登录 Login</h4><ul><li><code>login.jsp</code> 表单提交到  <code>loginServlet</code> ;</li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">jsp</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight jsp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><form action="<span" class="string">"${pageContext.request.contextPath}/loginServlet" method=<span class="string">"post"</span>><br></form></span></pre></td></tr></tbody></table></figure></div><ul><li><code>loginServlet</code>  逻辑<ul><li>验证码校验：</li></ul></li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取填写的验证码值</span></span><br><span class="line">String verifycode = request.getParameter(<span class="string">"verifycode"</span>);</span><br><span class="line"><span class="comment">// 获取真实验证码值</span></span><br><span class="line">HttpSession session = request.getSession();</span><br><span class="line">String checkcode_server = (String) session.getAttribute(<span class="string">"CHECKCODE_SERVER"</span>);</span><br><span class="line">session.removeAttribute(<span class="string">"CHECKCODE_SERVER"</span>);</span><br><span class="line"><span class="comment">//判断 验证码错误转发到该页面</span></span><br><span class="line"><span class="comment">// 转发仅在当前页面</span></span><br><span class="line"><span class="keyword">if</span>(!checkcode_server.equalsIgnoreCase(verifycode)){</span><br><span class="line">    request.setAttribute(<span class="string">"login_msg"</span>,<span class="string">"验证码错误"</span>);</span><br><span class="line">    request.getRequestDispatcher(<span class="string">"/login.jsp"</span>).forward(request,response);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></div><ul><li><ul><li>获取参数，封装User对象，</li><li>传入 <code>service.login(user)</code></li><li>结果不为空，返回  <code>User</code>  对象，重定向到首页</li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response.sendRedirect(request.getContextPath()+"/index.jsp");</span><br></pre></td></tr></tbody></table></figure></div><ul><li>结果为空，设置错误信息，转发到当前页面</li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">request.getRequestDispatcher(<span class="string">"/login.jsp"</span>).forward(request,response);</span><br></pre></td></tr></tbody></table></figure></div></li><li><p><code>servlet.login</code>   数据库实现</p></li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">sql</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight sql"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">String sql = "<span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> username=? <span class="keyword">and</span> <span class="keyword">password</span>=?<span class="string">";</span></span><br><span class="line"><span class="string">User loginUser = template.queryForObject(sql, new BeanPropertyRowMapper<user>(User.class), user.getUsername(), user.getPassword());</user></span></span><br><span class="line"><span class="string">return user;</span></span><br></pre></td></tr></tbody></table></figure></div><h4 id="添加联系人"><a href="#添加联系人" class="headerlink" title="添加联系人"></a>添加联系人</h4><ul><li>添加联系人的页面  <code>add.jsp</code>  ，填写表单信息后提交到  <code>addInfoServlet</code> </li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">html</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag"><<span class="name">form</span> <span class="attr">action</span>=<span class="string">"${pageContext.request.contextPath}/addInfoServlet"</span> <span class="attr">method</span>=<span class="string">"post"</span> <span class="attr">id</span>=<span class="string">"form"</span>></span></span><br></pre></td></tr></tbody></table></figure></div><blockquote><p>填写信息之后要进行表单校验，采用 js 实现：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">javascript</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight javascript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">window</span>.onload = <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>{</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">nameChecked</span>(<span class="params"></span>) </span>{</span><br><span class="line">                <span class="keyword">var</span> name = <span class="built_in">document</span>.getElementById(<span class="string">"name"</span>).value;</span><br><span class="line">                <span class="keyword">var</span> reg_name = <span class="regexp">/^[\u4e00-\u9fa5]{1,8}$/</span>;</span><br><span class="line">                <span class="keyword">var</span> flag = reg_name.test(name);</span><br><span class="line">                <span class="keyword">var</span> block = <span class="built_in">document</span>.getElementById(<span class="string">"n_c"</span>);</span><br><span class="line">                <span class="keyword">if</span> (!flag) {</span><br><span class="line">                    block.innerHTML = <span class="string">"名字只能输入汉字哦！"</span>;</span><br><span class="line">                }<span class="keyword">else</span> {</span><br><span class="line">                    block.innerHTML = <span class="string">" "</span>;</span><br><span class="line"></span><br><span class="line">                }</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> flag;</span><br><span class="line">            }</span><br><span class="line">    <span class="built_in">document</span>.getElementById(<span class="string">"name"</span>).onblur = nameChecked;</span><br><span class="line">    <span class="built_in">document</span>.getElementById(<span class="string">"form"</span>).onsubmit = <span class="function"><span class="keyword">function</span> (<span class="params"></span>) </span>{</span><br><span class="line">                <span class="keyword">return</span> nameChecked();</span><br><span class="line">            }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></div></blockquote><p> <code>addInfoServlet</code>   逻辑</p><ul><li><p>获取参数信息，封装对象</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Map<string, string[]> infoMap = request.getParameterMap();</string,></span><br></pre></td></tr></tbody></table></figure></div></li><li><p>调用<code>service.addInfo(addUser)</code>  添加信息</p></li><li><p>重定向到展示信息页面</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response.sendRedirect(request.getContextPath()+<span class="string">"/findUserByPageServlet"</span>);</span><br></pre></td></tr></tbody></table></figure></div></li></ul><p>service.addInfo(addUser)`  数据库操作</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">String sql = <span class="string">"insert into user (name,gender,age,address,qq,email)  values "</span> +</span><br><span class="line">                <span class="string">"(?,?,?,?,?,?)"</span>;</span><br><span class="line">template.update(sql, user.getName(), user.getGender(), user.getAge(), user.getAddress(), user.getQq(), user.getEmail());</span><br></pre></td></tr></tbody></table></figure></div><h4 id="删除联系人"><a href="#删除联系人" class="headerlink" title="删除联系人"></a>删除联系人</h4><ul><li>单条信息删除，点击删除后跳转至 <code>deleteInfoServlet</code> ，并且通过后缀 <code>?id=${user.id}</code> 传入参数 </li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang"></div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><a href="${pageContext.request.contextPath}/deleteInfoServlet?id=${user.id}">删除</a></span><br></pre></td></tr></tbody></table></figure></div><ul><li>根据<strong>当前</strong><code>id</code> 删除该条信息</li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String sql = <span class="string">"delete from user where id=?"</span>;</span><br><span class="line">template.update(sql, id);</span><br></pre></td></tr></tbody></table></figure></div><ul><li>重定向到信息展示页面</li></ul><h4 id="修改联系人信息"><a href="#修改联系人信息" class="headerlink" title="修改联系人信息"></a>修改联系人信息</h4><ul><li>点击修改按钮，传入当前id，跳转至  <code>FindUserServlet</code>  </li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang"></div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><a class="btn btn-default btn-sm" href="${pageContext.request.contextPath}/FindUserServlet?id=${user.id}"> 修改 </a></span><br></pre></td></tr></tbody></table></figure></div><ul><li>获取当前联系人信息记录，将<code>User</code>对象设置到域对象，转发到 <code>update.jsp</code> 文件中，在该文件中信息输入框设置 <code>value</code>属性，将设置的Usre信息回显</li><li>将  <code>update.jsp</code>  表单信息提交到  <code>updateInfoServlet</code>  ，在该servlet中，与上述逻辑类似，更新信息。</li></ul><h4 id="分页显示信息"><a href="#分页显示信息" class="headerlink" title="分页显示信息"></a>分页显示信息</h4><p>封装page对象返回给浏览器，具体包含：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> totalCount; <span class="comment">// 总记录数</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> totalPage;  <span class="comment">// 总页数</span></span><br><span class="line"><span class="keyword">private</span> List<t> list;   <span class="comment">// 每页数据</span></t></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> currentPage; <span class="comment">// 当前页数</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span> rows; <span class="comment">// 每页的数目</span></span><br></pre></td></tr></tbody></table></figure></div><p>需要浏览器传给服务器 <code>currentPage</code> 和 <code>rows</code>  参数</p><p><a href="https://user-images.githubusercontent.com/60562661/95356011-c8e08100-08f8-11eb-9bd7-090d9e6639f4.png" data-fancybox="group" data-caption="image-20201007231426556" class="fancybox"><img alt="image-20201007231426556" title="image-20201007231426556" data-src="https://user-images.githubusercontent.com/60562661/95356011-c8e08100-08f8-11eb-9bd7-090d9e6639f4.png" class="lazyload"></a></p><p><a href="https://user-images.githubusercontent.com/60562661/95356003-c67e2700-08f8-11eb-8bc2-209bc3fa622f.png" data-fancybox="group" data-caption="image-20201007231724903" class="fancybox"><img alt="image-20201007231724903" title="image-20201007231724903" data-src="https://user-images.githubusercontent.com/60562661/95356003-c67e2700-08f8-11eb-8bc2-209bc3fa622f.png" class="lazyload"></a></p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@WebServlet</span>(<span class="string">"/findUserByPageServlet"</span>)</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FindUserByPageServlet</span> <span class="keyword">extends</span> <span class="title">HttpServlet</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doPost</span><span class="params">(HttpServletRequest request, HttpServletResponse response)</span> <span class="keyword">throws</span> ServletException, IOException </span>{</span><br><span class="line"></span><br><span class="line">        request.setCharacterEncoding(<span class="string">"utf-8"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        String currentPage = request.getParameter(<span class="string">"currentPage"</span>);</span><br><span class="line">        String rows = request.getParameter(<span class="string">"rows"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(currentPage == <span class="keyword">null</span> || <span class="string">""</span>.equals(currentPage) ){</span><br><span class="line">            currentPage = <span class="string">"1"</span>;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (rows==<span class="keyword">null</span> || <span class="string">""</span>.equalsIgnoreCase(rows)){</span><br><span class="line">            rows = <span class="string">"7"</span>;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        Map<string, string[]> conditions = request.getParameterMap();</string,></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        UserService service = <span class="keyword">new</span> UserServiceImpl();</span><br><span class="line">        PageBean<user> pageUsers =  service.findUserByPage(currentPage, rows, conditions);</user></span><br><span class="line"><span class="comment">//        System.out.println(pageUsers);</span></span><br><span class="line">        request.setAttribute(<span class="string">"pageUsers"</span>,pageUsers);</span><br><span class="line">        request.setAttribute(<span class="string">"condition"</span>,conditions);</span><br><span class="line">        request.getRequestDispatcher(<span class="string">"/list.jsp"</span>).forward(request,response);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">doGet</span><span class="params">(HttpServletRequest request, HttpServletResponse response)</span> <span class="keyword">throws</span> ServletException, IOException </span>{</span><br><span class="line">        <span class="keyword">this</span>.doPost(request, response);</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></div><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function"><span class="keyword">public</span> PageBean<user> <span class="title">findUserByPage</span><span class="params">(String _currentPage, String _rows, Map<string,string[]> condition)</string,string[]></span> </user></span>{</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> currentPage = Integer.parseInt(_currentPage);</span><br><span class="line">        <span class="keyword">int</span> rows = Integer.parseInt(_rows);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(currentPage<=<span class="number">0</span>){</span><br><span class="line">            currentPage = <span class="number">1</span>;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        PageBean<user> page = <span class="keyword">new</span> PageBean<>();</user></span><br><span class="line"></span><br><span class="line">        page.setRows(rows);</span><br><span class="line"></span><br><span class="line">        page.setCurrentPage(currentPage);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> totalCount = dao.findTotalCount(condition);</span><br><span class="line">        page.setTotalCount(totalCount);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> start = (currentPage - <span class="number">1</span>) * rows;</span><br><span class="line">        List<user> userList = dao.findUserByPage(start, rows, condition);</user></span><br><span class="line">        page.setList(userList);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> totalPage = totalCount % rows == <span class="number">0</span> ? totalCount / rows : totalCount / rows + <span class="number">1</span>;</span><br><span class="line">        page.setTotalPage(totalPage);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> page;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></div><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findTotalCount</span><span class="params">(Map<string, string[]> condition)</string,></span> </span>{</span><br><span class="line"></span><br><span class="line">       String sql = <span class="string">"select count(*) from user where 1=1 "</span>;</span><br><span class="line">       StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">       sb.append(sql);</span><br><span class="line"></span><br><span class="line">       List<object> params = <span class="keyword">new</span> ArrayList<object>();<br><span class="line"></span><br><span class="line">       <span class="keyword">for</span> (String key : condition.keySet()) {</span><br><span class="line"></span><br><span class="line">           <span class="keyword">if</span> (<span class="string">"currentPage"</span>.equals(key) || <span class="string">"rows"</span>.equals(key)) {</span><br><span class="line">               <span class="keyword">continue</span>;</span><br><span class="line">           }</span><br><span class="line"></span><br><span class="line">           String value = condition.get(key)[<span class="number">0</span>];</span><br><span class="line">           <span class="keyword">if</span> (value != <span class="keyword">null</span> && !<span class="string">""</span>.equals(value)) {</span><br><span class="line">               sb.append(<span class="string">" and "</span> + key + <span class="string">" like ? "</span>);</span><br><span class="line">               params.add(<span class="string">"%"</span> + value.replaceAll(<span class="string">" "</span>,<span class="string">""</span>) + <span class="string">"%"</span>);</span><br><span class="line">           }</span><br><span class="line">       }</span><br><span class="line">       <span class="keyword">return</span> template.queryForObject(sb.toString(), Integer<span class="class">.<span class="keyword">class</span>, <span class="title">params</span>.<span class="title">toArray</span>())</span>;</span><br><span class="line">   }</span><br></object></object></span></pre></td></tr></tbody></table></figure></div><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List<user> <span class="title">findUserByPage</span><span class="params">(<span class="keyword">int</span> start, <span class="keyword">int</span> rows, Map<string, string[]> condition)</string,></span> </user></span>{</span><br><span class="line">    String sql = <span class="string">"select * from user where 1 = 1 "</span>;</span><br><span class="line">    StringBuilder sb = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">    sb.append(sql);</span><br><span class="line"></span><br><span class="line">    List<object> params = <span class="keyword">new</span> ArrayList<object>();<br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (String key : condition.keySet()) {</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="string">"currentPage"</span>.equals(key) || <span class="string">"rows"</span>.equals(key)) {</span><br><span class="line">            <span class="keyword">continue</span>;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        String value = condition.get(key)[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">if</span> (value != <span class="keyword">null</span> && !<span class="string">""</span>.equals(value)) {</span><br><span class="line">            sb.append(<span class="string">" and "</span> + key + <span class="string">" like ? "</span>);</span><br><span class="line">            params.add(<span class="string">"%"</span> + value.replaceAll(<span class="string">" "</span>,<span class="string">""</span>)  + <span class="string">"%"</span>);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    params.add(start);</span><br><span class="line">    params.add(rows);</span><br><span class="line">    sb.append(<span class="string">" limit ?, ? "</span>);</span><br><span class="line">    List<user> userList = template.query(sb.toString(), <span class="keyword">new</span> BeanPropertyRowMapper<user>(User<span class="class">.<span class="keyword">class</span>), <span class="title">params</span>.<span class="title">toArray</span>())</span>;</user></user></span><br><span class="line">    <span class="keyword">return</span> userList;</span><br><span class="line">}</span><br></object></object></span></pre></td></tr></tbody></table></figure></div></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Java开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java后端开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JavaWeb简要总结--Mysql,JDBC</title>
      <link href="/2020/08/31/JavaWeb%E7%AE%80%E8%A6%81%E6%80%BB%E7%BB%93-Mysql-JDBC/"/>
      <url>/2020/08/31/JavaWeb%E7%AE%80%E8%A6%81%E6%80%BB%E7%BB%93-Mysql-JDBC/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p><strong>Summarize</strong>     总结给自己看，泛泛学过去很多东西记不住，这里记录一下最常用的东西，最核心的。</p><h2 id="Mysql-极简操作"><a href="#Mysql-极简操作" class="headerlink" title="Mysql 极简操作"></a>Mysql 极简操作</h2><blockquote><p>Mysql 首次安装登录出现问题，</p><p>ERROR 1045 (28000): Access denied for user ‘ODBC’@’localhost’ (using password: YES)</p><p>解决问题见：<a href="https://blog.csdn.net/tiankongbubian/article/details/77119751" target="_blank" rel="noopener">https://blog.csdn.net/tiankongbubian/article/details/77119751</a></p><p>适用于 <strong>windows</strong> 环境</p></blockquote><hr><h3 id="常用SQL命令"><a href="#常用SQL命令" class="headerlink" title="常用SQL命令"></a><strong>常用SQL命令</strong></h3><p><strong>分类：DML,DQL,DCL,DDL</strong></p><ul><li><strong>Data Definition Language</strong> (DDL数据定义语言) 如：建库，建表</li><li><strong>Data Manipulation Language</strong>(DML数据操纵语言)，如：对表中的记录操作增删改</li><li><strong>Data Query Language</strong>(DQL 数据查询语言)，如：对表中的查询操作</li><li><strong>Data Control Language</strong>(DCL 数据控制语言)，如：对用户权限的设置</li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -proot //登录mysql</span><br></pre></td></tr></tbody></table></figure></div><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">show databases; -- 显示数据库</span><br><span class="line"></span><br><span class="line">use test;</span><br><span class="line"></span><br><span class="line">show tables; -- 显示表</span><br><span class="line"></span><br><span class="line">create database 库名; -- 建库</span><br><span class="line"></span><br><span class="line">drop database 库名; -- 删库</span><br><span class="line"></span><br><span class="line">drop table if exists `create`;</span><br></pre></td></tr></tbody></table></figure></div><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">创建表实例</span><br><span class="line">*/</span><br><span class="line">CREATE TABLE student3 ( </span><br><span class="line">    id INT, </span><br><span class="line">NAME VARCHAR(20), </span><br><span class="line">age INT, </span><br><span class="line">sex VARCHAR(5), </span><br><span class="line">address VARCHAR(100), </span><br><span class="line">math INT,  </span><br><span class="line">english INT</span><br><span class="line">);  </span><br><span class="line"></span><br><span class="line">-- 修改表相关</span><br><span class="line">alter table 表名 rename to 新的表名; -- 1. 修改表名</span><br><span class="line"></span><br><span class="line">alter table 表名 character set 字符集名称; -- 2. 修改表的字符集</span><br><span class="line"></span><br><span class="line">alter table 表名 add 列名 数据类型; -- 3. 添加一列</span><br><span class="line"></span><br><span class="line">alter table 表名 change 列名 新列别 新数据类型; -- 4. 修改列名称 类型</span><br><span class="line">alter table 表名 modify 列名 新数据类型;</span><br><span class="line"></span><br><span class="line">alter table 表名 drop 列名; -- 5. 删除列</span><br></pre></td></tr></tbody></table></figure></div><p>增删改表数据：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">Code</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight plain"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">-- 添加记录</span><br><span class="line">insert into 表名(列名1,列名2,...列名n) values(值1,值2,...值n); -- 指定列</span><br><span class="line">insert into 表名 values(值1,值2,...值n); -- 所有列</span><br><span class="line">-- 删除记录</span><br><span class="line">delete from 表名 [where 条件]</span><br><span class="line"> TRUNCATE TABLE 表名; -- 删除表建议操作</span><br><span class="line"> -- 修改记录</span><br><span class="line"> update 表名 set 列名1 = 值1, 列名2 = 值2,... [where 条件];</span><br><span class="line"> --查询记录</span><br><span class="line"> select * from 表名;</span><br><span class="line"> select 字段名1，字段名2... from 表名；</span><br><span class="line"> </span><br><span class="line">-- '_'表示一个占位符，'%'表示不确定占位符</span><br><span class="line"> -- 查询姓马的有哪些？ like</span><br><span class="line">SELECT * FROM student WHERE NAME LIKE '马%';</span><br><span class="line">-- 查询姓名第二个字是化的人</span><br><span class="line">SELECT * FROM student WHERE NAME LIKE "_化%";</span><br><span class="line">-- 查询姓名是3个字的人</span><br><span class="line">SELECT * FROM student WHERE NAME LIKE '___';</span><br></pre></td></tr></tbody></table></figure></div><p>以上就是最常写的sql命令了。<strong>Mysql极简入门，可以查询基本语法。其他高级部分用到再面向百度编程。</strong></p><hr><p><a href="https://user-images.githubusercontent.com/60562661/91742293-a2884100-ebe8-11ea-9fe3-fb7627f853a8.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/91742293-a2884100-ebe8-11ea-9fe3-fb7627f853a8.jpg" class="lazyload"></a></p><h2 id="JDBC—Java连接数据库"><a href="#JDBC—Java连接数据库" class="headerlink" title="JDBC—Java连接数据库"></a>JDBC—Java连接数据库</h2><p>记录一下<code>JDBC</code>最常用的 一套流程，其他冗余的就不多做记录了。</p><ul><li><strong>原生 JDBC 连接操作流程</strong></li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DemoJDBC</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>{</span><br><span class="line"></span><br><span class="line">        <span class="comment">//  注册驱动 ，内部自动执行静态代码块</span></span><br><span class="line">        Class.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取数据库连接对象</span></span><br><span class="line">        <span class="comment">//本机可以简写  jdbc:mysql:///test_multi</span></span><br><span class="line">        Connection conn = DriverManager.getConnection(<span class="string">"jdbc:mysql://localhost:3306/test_multi"</span>, <span class="string">"root"</span>, <span class="string">"123"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 定义 sql 语句</span></span><br><span class="line">        String sql = <span class="string">"update user set password = 'lol'"</span>;</span><br><span class="line">        String sql2 = <span class="string">"select * from user"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 获取sql执行对象</span></span><br><span class="line">        <span class="comment">// 2. 事务管理</span></span><br><span class="line">        Statement statement = conn.createStatement();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 执行sql对象</span></span><br><span class="line">        <span class="comment">// executeUpdate (执行DML语句, DDL语句</span></span><br><span class="line">       <span class="keyword">int</span> count = statement.executeUpdate(sql);</span><br><span class="line">        ResultSet results = statement.executeQuery(sql2);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//遍历结果</span></span><br><span class="line">        <span class="keyword">while</span> (results.next()){</span><br><span class="line">            <span class="keyword">int</span> id = results.getInt(<span class="string">"uid"</span>);</span><br><span class="line">            String userName = results.getString(<span class="string">"username"</span>);</span><br><span class="line">            String passWord = results.getString(<span class="string">"password"</span>);</span><br><span class="line">            Date birth = results.getDate(<span class="string">"birthday"</span>);</span><br><span class="line"></span><br><span class="line">            System.out.println(<span class="string">"id: "</span> + id +</span><br><span class="line">                               <span class="string">"userName: "</span> + userName +</span><br><span class="line">                               <span class="string">"passWord: "</span> + passWord +</span><br><span class="line">                               <span class="string">"birthDay: "</span> + birth);</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 释放资源</span></span><br><span class="line">        statement.close();</span><br><span class="line">        conn.close();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></div><ul><li><p><strong>JDBCTemplate 流程 — 最常用</strong></p><blockquote><p>Spring框架对JDBC的简单封装。提供了一个JDBCTemplate对象简化JDBC的开发</p></blockquote></li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义工具类，采用阿里的Druid连接池</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JDBCPoolUtils</span> </span>{</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> DataSource ds;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化数据源</span></span><br><span class="line">    <span class="keyword">static</span> {</span><br><span class="line">        <span class="keyword">try</span> {</span><br><span class="line">            Properties pro = <span class="keyword">new</span> Properties();</span><br><span class="line">            pro.load(JDBCPoolUtils.class.getClassLoader().getResourceAsStream("druid.properties"));</span><br><span class="line">            ds = DruidDataSourceFactory.createDataSource(pro);</span><br><span class="line">        } <span class="keyword">catch</span> (IOException e) {</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        } <span class="keyword">catch</span> (Exception e) {</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取连接对象</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">()</span> <span class="keyword">throws</span> SQLException </span>{</span><br><span class="line">        <span class="keyword">return</span> ds.getConnection();</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">//释放资源</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(PreparedStatement statement, Connection con)</span> </span>{</span><br><span class="line">        <span class="keyword">if</span> (statement != <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">try</span> {</span><br><span class="line">                statement.close();</span><br><span class="line">            } <span class="keyword">catch</span> (SQLException throwables) {</span><br><span class="line">                throwables.printStackTrace();</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">if</span> (con != <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">try</span> {</span><br><span class="line">                statement.close();</span><br><span class="line">            } <span class="keyword">catch</span> (SQLException throwables) {</span><br><span class="line">                throwables.printStackTrace();</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">//释放资源重载</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(ResultSet res, PreparedStatement statement, Connection con)</span> </span>{</span><br><span class="line">        <span class="keyword">if</span>(res!=<span class="keyword">null</span>){</span><br><span class="line">            <span class="keyword">try</span> {</span><br><span class="line">                res.close();</span><br><span class="line">            } <span class="keyword">catch</span> (SQLException throwables) {</span><br><span class="line">                throwables.printStackTrace();</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">if</span> (statement != <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">try</span> {</span><br><span class="line">                statement.close();</span><br><span class="line">            } <span class="keyword">catch</span> (SQLException throwables) {</span><br><span class="line">                throwables.printStackTrace();</span><br><span class="line">            }</span><br><span class="line"></span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">if</span> (con != <span class="keyword">null</span>) {</span><br><span class="line">            <span class="keyword">try</span> {</span><br><span class="line">                statement.close();</span><br><span class="line">            } <span class="keyword">catch</span> (SQLException throwables) {</span><br><span class="line">                throwables.printStackTrace();</span><br><span class="line">            }</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> DataSource <span class="title">getDataSource</span><span class="params">()</span></span>{</span><br><span class="line">        <span class="keyword">return</span> ds;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></div><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">java</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight java"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//JDBCTemplate流程</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DemoJDBCTemplate</span> </span>{</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>{</span><br><span class="line">        <span class="comment">//创建JdbcTemplate对象</span></span><br><span class="line">        JdbcTemplate template = <span class="keyword">new</span> JdbcTemplate(JDBCPoolUtils.getDataSource());</span><br><span class="line">        <span class="comment">// 定义sql语句</span></span><br><span class="line">        String sql = <span class="string">"select * from user"</span>;</span><br><span class="line">        String sql2 = <span class="string">"select count(id) from user"</span>;        </span><br><span class="line">        <span class="comment">// 执行查询返回javabean对象装载到列表</span></span><br><span class="line">        <span class="comment">//User 对象 与 数据库列一一对应</span></span><br><span class="line">        List<user> zyl = template.query(sql, <span class="keyword">new</span> BeanPropertyRowMapper<user>(User<span class="class">.<span class="keyword">class</span>))</span>; <span class="comment">// 返回 JavaBean</span></user></user></span><br><span class="line">        Long count = template.queryForObject(sql2,Long<span class="class">.<span class="keyword">class</span>)</span>; <span class="comment">// 执行聚合函数</span></span><br><span class="line">        System.out.println(count);</span><br><span class="line">        <span class="keyword">for</span>(User map : zyl){</span><br><span class="line">            System.out.println(map);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure></div><hr><p><strong>2020-9-1日夜更新。</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Java开发 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java后端开发 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>计算机网络-IP地址&amp;端口</title>
      <link href="/2020/08/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-IP%E5%9C%B0%E5%9D%80-%E7%AB%AF%E5%8F%A3/"/>
      <url>/2020/08/23/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-IP%E5%9C%B0%E5%9D%80-%E7%AB%AF%E5%8F%A3/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="IP-地址"><a href="#IP-地址" class="headerlink" title="IP 地址"></a>IP 地址</h2><p>ip地址作为计算机的唯一标识符，每个计算机有唯一的IP地址。</p><p><strong>IPV4</strong> 用 <strong>32位(bit)</strong>表示ip地址，也就是<strong>4字节（byte）</strong>.</p><p>为了便于表示，用 <strong>a.b.c.d</strong> 表示。每个二进制都为8bit，十进制为<strong>0~255</strong>.  </p><script type="math/tex; mode=display">2^8=255</script><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> //本机ip</span><br><span class="line">localhost//本机</span><br></pre></td></tr></tbody></table></figure></div><p>根据最高位地址划分为ABCDE类。</p><p><strong>IPV6</strong> 用  8 组，每组16位（bit）表示ip地址，一共128位</p><h2 id="端口号"><a href="#端口号" class="headerlink" title="端口号"></a>端口号</h2><p>端口号是计算机之间通信时，每个通信软件有固定的端口号。例如qq，不同计算机上qq的端口号相同，消息才得以互通。</p><p>长度16位，<strong>0~65535</strong></p><p>1000以下已经分配。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">80</span> 为www服务端口</span><br><span class="line"><span class="number">3306</span> mysql服务端口</span><br><span class="line"><span class="number">8080</span> www代理服务</span><br><span class="line"><span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span> //本机ip</span><br><span class="line">localhost//本机</span><br></pre></td></tr></tbody></table></figure></div></body></html>]]></content>
      
      
      <categories>
          
          <category> Java后端开发 </category>
          
          <category> 网络编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IP,Port </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>再探形变卷积</title>
      <link href="/2020/08/19/%E5%86%8D%E6%8E%A2%E5%BD%A2%E5%8F%98%E5%8D%B7%E7%A7%AF/"/>
      <url>/2020/08/19/%E5%86%8D%E6%8E%A2%E5%BD%A2%E5%8F%98%E5%8D%B7%E7%A7%AF/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>前一篇博文写过关于形变卷积的部分，但是写的比较粗略，故再来分析、整理一下形变卷积相关知识。</p><blockquote><ul><li>因为网络最终用的形变卷积，所以是非看不可。</li><li>好读书，不求甚解。 不愧是我</li></ul></blockquote><h3 id="初衷"><a href="#初衷" class="headerlink" title="初衷"></a>初衷</h3><p><strong>适应几何变换</strong></p><p>传统卷积卷积核是正方形，扫描不规则图形也只能用方形区域，而实际任务中，很多东西都不是规则的，例如生物、汽车等。传统卷积核无法适应比较大的变换，故提出了可以变形以适应几何变换的形变卷积。</p><hr><h3 id="卷积过程计算"><a href="#卷积过程计算" class="headerlink" title="卷积过程计算"></a>卷积过程计算</h3><ul><li><strong>传统卷积计算</strong></li></ul><script type="math/tex; mode=display">y(p_0) = \sum_{p_n \in R} w(p_n) * x(p_0 + p_n)</script><p>其中，<code>p0</code>表示输出特征图的一个点；<code>pn</code>则是卷积核的坐标。</p><ul><li><strong>形变卷积计算</strong></li></ul><script type="math/tex; mode=display">y(p_0) = \sum_{p_n \in R} w(p_n) * x(p_0 + p_n + \Delta p_n)</script><p>其中。最后加了一项<code>delta</code> 表示偏移量， 取值从1-N，N是卷积核尺寸。</p><blockquote><p>这里P0表示输出特征图的点，之所以pn要加p0，因为是3X3卷积核，原始图片9个像素对应新特征图一个像素。</p><p>2020-8-19更新</p></blockquote><ul><li><strong>采样</strong></li></ul><p>此时采样的点加了一项<code>delta</code>, 可能是小数，因此需要用插值算法来确定采样位置，此处应用的是双线性插值来确定采样的位置，如下：</p><script type="math/tex; mode=display">p = p_0 + p_n + \Delta p_n</script><script type="math/tex; mode=display">x(p) = \sum_q G(q,p)*x(q)</script><p>大致意思是 ：先枚举输入特征图x所有的点q，用插值核G做个运算再乘以x(q).</p><blockquote><p> 总之这一步就是确定位置。之前看过有关书上讲解双线性采样的问题，可以用公式直接算出来，这边看的不是很明白，但是没那么重要。</p><p>从形变卷积公式可以看出，需要学习的参数有两个：卷积核的weight，偏移量<code>Delta</code>, 偏移量显得至关重要</p></blockquote><ul><li><strong>反向传播学习</strong></li></ul><script type="math/tex; mode=display">\frac{\partial y(p_0)}{\partial \Delta p_n} = \sum_{p_n \in R}w(p_n)*\frac{\partial x(p_0 + p_n + \Delta p_n)}{\partial \Delta p_n}= \sum_{p_n \in R} \left[w(p_n) * \sum_q\frac{\partial G(q,p_0+p_n+\Delta p_n)}{\partial \Delta p_n} * x(q)\right]</script><blockquote><p>偏移量梯度即在上述计算公式中有偏移量这一项的都计算微分。</p></blockquote><hr><h3 id="输出特征图尺寸计算"><a href="#输出特征图尺寸计算" class="headerlink" title="输出特征图尺寸计算"></a>输出特征图尺寸计算</h3><p>形变卷积与普通卷积无异，输出特征图尺寸计算需要先知道卷积核大小，再根据公式来计算。卷积核大小与是否使用空洞有关系。</p><ul><li><strong>空洞卷积卷积核尺寸计算</strong></li></ul><script type="math/tex; mode=display">K = k+(k-1)*(r-1)</script><blockquote><p>r 为空洞率， r=1时卷积核尺寸不变，故Pytorch <code>nn.Conv2D</code> 默认 dilated=1.</p><p><strong>这里吐槽一下，看了三四个人写的博客，都说dlitated=1表示注入一个空洞，这是错的。不可信！</strong></p></blockquote><ul><li><strong>特征图尺寸计算</strong></li></ul><script type="math/tex; mode=display">W = \frac{w - K +2*P}{S} +1 = \frac{w - [k+(K-1)*(r-1)] + 2P}{S} +1</script><script type="math/tex; mode=display">H = \frac{h - K +2*P}{S} +1 = \frac{h - [k+(K-1)*(r-1)] + 2P}{S} +1</script><blockquote><p>把空洞卷积卷积核带进去即可。</p></blockquote><hr><h3 id="关于卷积"><a href="#关于卷积" class="headerlink" title="关于卷积"></a>关于卷积</h3><p>理论上，卷积核感受野随着卷积层的增加而线性增加；</p><p>事实上，有效的感受野是呈高斯分布，即一个卷积核中心的区域的比较有效。</p><p>因此实际中，有效的感受野随着卷积层的增加呈平方根增加，是远远不够的。空洞卷积恰好处理这个问题。</p><p><strong>2020.7.13更新！</strong></p><p><strong>FRY</strong></p><hr><h3 id="2020-8-19-第二次更新"><a href="#2020-8-19-第二次更新" class="headerlink" title="2020-8-19 第二次更新"></a>2020-8-19 第二次更新</h3><h4 id="Deformable-Convolution-V2"><a href="#Deformable-Convolution-V2" class="headerlink" title="Deformable Convolution V2"></a>Deformable Convolution V2</h4><p>可形变卷积V2版本，给原始每个偏移的采样点加入了权重，即：</p><script type="math/tex; mode=display">y(p) = \sum_{k=1}^K w_k \times x(p + p_k + \Delta p_k)\times\Delta m_k</script><p>​    K表示采样位置；最后多了一项每个采样像素权重的控制。</p><p><strong>有个地方需要注意</strong>：</p><ul><li><p>V1和V2版本，<code>Offsets</code> 输出通道数均为<code>2K</code>, 因为每个点偏移需要x，y两个分量来表示，故用2个通道</p></li><li><p>V2版本中输出通道数为<code>K</code> ，每个点一个权重，故一共K个权重</p></li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ubuntu16/18可道云云盘搭建</title>
      <link href="/2020/08/06/Ubuntu16-18%E5%8F%AF%E9%81%93%E4%BA%91%E4%BA%91%E7%9B%98%E6%90%AD%E5%BB%BA/"/>
      <url>/2020/08/06/Ubuntu16-18%E5%8F%AF%E9%81%93%E4%BA%91%E4%BA%91%E7%9B%98%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>实验室有自己有一台主机闲置，想着闲置太浪费，但是一直不会用。所以就把这个主机当成一个web服务器😝，这个主机系统是<strong>Ubuntu18.04</strong>， <strong>Ubuntu16.04也已经测试</strong>。</p><h3 id="安装-apache2"><a href="#安装-apache2" class="headerlink" title="安装 apache2"></a>安装 apache2</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 下载安装</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install apache2</span><br><span class="line">// 防火墙</span><br><span class="line">sudo ufw allow 'Apache Full'</span><br><span class="line">//启动</span><br><span class="line">sudo ufw enable</span><br></pre></td></tr></tbody></table></figure></div><p>Apache2服务启动后，可以输入本机ip看到对应的页面表示安装成功。</p><p>参考链接：<a href="https://www.cnblogs.com/lfri/p/10522392.html" target="_blank" rel="noopener">https://www.cnblogs.com/lfri/p/10522392.html</a></p><h3 id="安装mysql和php"><a href="#安装mysql和php" class="headerlink" title="安装mysql和php"></a>安装mysql和php</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install php-mysql</span><br><span class="line">sudo apt-get install phpmyadmin</span><br></pre></td></tr></tbody></table></figure></div><h3 id="安装可道云"><a href="#安装可道云" class="headerlink" title="安装可道云"></a>安装可道云</h3><p>下载可道云源码，解压到：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">shell</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/var/www/html/cloud</span><br></pre></td></tr></tbody></table></figure></div><p>就可以通过ip来访问：<strong>10.21.7.216/cloud</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/89437771-94c2d580-d77a-11ea-83c1-ff81522c5fdd.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/89437771-94c2d580-d77a-11ea-83c1-ff81522c5fdd.png" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cloud </tag>
            
            <tag> Web Application </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积之空洞卷积与形变卷积</title>
      <link href="/2020/07/09/%E5%8D%B7%E7%A7%AF%E4%B9%8B%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%BD%A2%E5%8F%98%E5%8D%B7%E7%A7%AF/"/>
      <url>/2020/07/09/%E5%8D%B7%E7%A7%AF%E4%B9%8B%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF%E4%B8%8E%E5%BD%A2%E5%8F%98%E5%8D%B7%E7%A7%AF/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>最近看了空洞卷积核形变卷积，这里大致记录一下。</p><h2 id="Convolution-Filter"><a href="#Convolution-Filter" class="headerlink" title="Convolution  Filter"></a>Convolution  Filter</h2><p>普通卷积中，卷积核是方形的，例如 3X3，在图片上扫描得到特征图。个人认为大小卷积核，3*3，9X9区别在于：</p><ul><li><p><strong>感受野变化</strong></p><ul><li>9*9 感受野明显是非常大的，对应的提取到的特征更加的整体性；</li><li>3*3 卷积是可以堆叠形成9X9的感受野，但是更多的卷积层堆叠，提取到的特征也就会更加的细腻</li></ul><blockquote><p>对于姿态估计任务来说，看到的更多，人更整体似乎会更好</p></blockquote></li><li><p><strong>计算量的变化</strong></p><p>3*3卷积核参数为9，9X9参数为81，参数翻了9倍，计算量大，需要更多的显存</p></li></ul><h2 id="Dilated-Convolutions"><a href="#Dilated-Convolutions" class="headerlink" title="Dilated Convolutions"></a>Dilated Convolutions</h2><p>参考：<a href="https://blog.csdn.net/qq_36338754/article/details/96969208?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase" target="_blank" rel="noopener">https://blog.csdn.net/qq_36338754/article/details/96969208?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.nonecase</a></p><h3 id="初步认识"><a href="#初步认识" class="headerlink" title="初步认识"></a>初步认识</h3><p>空洞卷积，也称扩张卷积、膨胀卷积，可以在不增加参数量的情况下起到大卷积核的效果。最早是用于语义分割的，</p><p>顾名思义，在卷积核中注入空洞，扩大了感受野，</p><p><a href="https://pic2.zhimg.com/50/v2-4959201e816888c6648f2e78cccfd253_hd.webp?source=1940ef5c" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://pic2.zhimg.com/50/v2-4959201e816888c6648f2e78cccfd253_hd.webp?source=1940ef5c" class="lazyload"></a></p><h3 id="几个要点"><a href="#几个要点" class="headerlink" title="几个要点"></a>几个要点</h3><p><a href="https://user-images.githubusercontent.com/60562661/87059662-24d53400-c23c-11ea-8a26-acebd128f957.png" data-fancybox="group" data-caption="1594301948766" class="fancybox"><img alt="1594301948766" title="1594301948766" data-src="https://user-images.githubusercontent.com/60562661/87059662-24d53400-c23c-11ea-8a26-acebd128f957.png" class="lazyload"></a></p><p><strong>dilated</strong>设置为2时，3*3卷积核感受野为7X7, 设置为4，感受野为9X9</p><p><strong>Note：</strong></p><ul><li><p>红点表示感受野中心</p></li><li><p>外围图像表示感受野</p></li><li><p>扩张卷积不会影响图像尺寸，卷积之后图像尺寸计算依然是：</p><script type="math/tex; mode=display">w_{new} = \frac{W-F+2*P}{S} + 1</script></li></ul><h3 id="Pytorch-中空洞卷积实现"><a href="#Pytorch-中空洞卷积实现" class="headerlink" title="Pytorch 中空洞卷积实现"></a>Pytorch 中空洞卷积实现</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>, dilation=<span class="number">1</span>)  <span class="comment"># 普通卷积</span></span><br><span class="line">conv2 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>, dilation=<span class="number">2</span>)  <span class="comment"># dilation就是空洞率，即间隔</span></span><br></pre></td></tr></tbody></table></figure></div><h2 id="Deform-Convolution"><a href="#Deform-Convolution" class="headerlink" title="Deform Convolution"></a>Deform Convolution</h2><p>形变卷积顾名思义，卷积核不再一定是方形了，在原来方形的基础上增加了一个<code>offest</code> 偏移量，于是可以发生形变，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/87059675-2868bb00-c23c-11ea-8050-55e08777e5cc.png" data-fancybox="group" data-caption="1594303449207" class="fancybox"><img alt="1594303449207" title="1594303449207" data-src="https://user-images.githubusercontent.com/60562661/87059675-2868bb00-c23c-11ea-8050-55e08777e5cc.png" class="lazyload"></a></p><p>卷积核可以任意的变形。</p><p><a href="https://user-images.githubusercontent.com/60562661/87059677-2999e800-c23c-11ea-8c16-42794f51e06b.png" data-fancybox="group" data-caption="1594303485745" class="fancybox"><img alt="1594303485745" title="1594303485745" data-src="https://user-images.githubusercontent.com/60562661/87059677-2999e800-c23c-11ea-8c16-42794f51e06b.png" class="lazyload"></a></p><p>偏移量是通过一个普通的卷积层计算得到。</p><p><a href="https://user-images.githubusercontent.com/60562661/87059683-2a327e80-c23c-11ea-8e0d-fb77fc1567b9.png" data-fancybox="group" data-caption="1594307447854" class="fancybox"><img alt="1594307447854" title="1594307447854" data-src="https://user-images.githubusercontent.com/60562661/87059683-2a327e80-c23c-11ea-8e0d-fb77fc1567b9.png" class="lazyload"></a></p><blockquote><p>看以看出随着训练，形变卷积核采样位置的变化，在形成一个个形状，而不在是方形了。</p></blockquote><h2 id="多尺度"><a href="#多尺度" class="headerlink" title="多尺度"></a>多尺度</h2><blockquote><p>借鉴于别人—-在深度学习很多计算机视觉任务中，都用到了多尺度，这是在模仿人眼观察物体，</p><ul><li>近大远小</li><li>远景完整，但是模糊</li><li>近景细节丰富，但是不完整</li></ul></blockquote><hr><h1 id="以上是对空对卷积、形变卷积的初步理解，后续有改进、错误会及时更新"><a href="#以上是对空对卷积、形变卷积的初步理解，后续有改进、错误会及时更新" class="headerlink" title="以上是对空对卷积、形变卷积的初步理解，后续有改进、错误会及时更新"></a>以上是对空对卷积、形变卷积的初步理解，后续有<strong>改进、错误</strong>会及时更新</h1></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch Dataloader 用法</title>
      <link href="/2020/07/09/Pytorch-Dataloader-%E7%94%A8%E6%B3%95/"/>
      <url>/2020/07/09/Pytorch-Dataloader-%E7%94%A8%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="DataLoder"><a href="#DataLoder" class="headerlink" title="DataLoder"></a>DataLoder</h2><p>Pytorch在训练前一步数据读取时，要使用 <code>DataLoader</code> 加载数据, 可以<code>shuffle</code>  、多线程读取等。记录一下如何使用。</p><blockquote><p>自黑一下，之前我写的第一个工程，完全没写DataLoader，直接把原图，标签放在两个大列表里面来循环，😂🤣，所以当时16g内存都直接溢出😆，所以说不写DataLoader也不是不可以，就是有点不可描述</p><p><strong>镇楼</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/87008741-c89bf100-c1f6-11ea-86b1-86d221a1de78.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/87008741-c89bf100-c1f6-11ea-86b1-86d221a1de78.jpg" class="lazyload"></a></p></blockquote><h2 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h2><h3 id="定义Dataset"><a href="#定义Dataset" class="headerlink" title="定义Dataset"></a>定义Dataset</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.utils.data.Dataset</span><br></pre></td></tr></tbody></table></figure></div><p>首先自定义<code>dataset</code>类，以上述<code>Dataset</code>为父类，必须重写<code>__getitem__()</code> 方法，即获取数据逻辑；</p><p>可选重写<code>__len()__</code>方法，获取数据长度信息。</p><p>类似如下，是我们最近做的一个研究中<code>Dataset</code> 片段，重点关注它<code>return</code>的结果。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data.Dataset </span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 Dataset</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JointsDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ........)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self, )</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.db)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        省略代码块</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> input_data_numpy, input_sup_A_data_numpy, input_sup_B_data_numpy, target_heatmaps, target_weight, meta</span><br></pre></td></tr></tbody></table></figure></div><h3 id="创建-DataLoader"><a href="#创建-DataLoader" class="headerlink" title="创建 DataLoader"></a>创建 DataLoader</h3><p>先看一下Dataloader类定义</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.utils.data.DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, sampler=<span class="literal">None</span>, batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=<span class="literal">None</span>, pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>, worker_init_fn=<span class="literal">None</span>, multiprocessing_context=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure></div><p>一般用的几个参数：</p><ul><li>dataset 为上述的自定义的类</li><li>batch_size </li><li>shuffle 打乱数据</li><li>num_workers 多线程</li><li>pin_memory 更快的发送数据到显存 （不太清楚）</li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = JointsDataset(</span><br><span class="line">        cfg, cfg.DATASET.ROOT, cfg.DATASET.TRAIN_SET, <span class="literal">True</span>,</span><br><span class="line">        cfg.DATASET.TRAIN_NPY_DIR,</span><br><span class="line">        transform=transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            normalize,</span><br><span class="line">        ])</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">        train_dataset,</span><br><span class="line">        batch_size=cfg.TRAIN.BATCH_SIZE_PER_GPU * len(cfg.GPUS),</span><br><span class="line">        shuffle=cfg.TRAIN.SHUFFLE,</span><br><span class="line">        num_workers=cfg.WORKERS,</span><br><span class="line">        pin_memory=cfg.PIN_MEMORY</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure></div><h3 id="迭代获取数据"><a href="#迭代获取数据" class="headerlink" title="迭代获取数据"></a>迭代获取数据</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (input, input_sup_A, input_sup_B, target, target_weight, meta) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></tbody></table></figure></div><p>关于获取的数据可以看到和自定义<code>dataset</code>类<code>__getitem()__</code>方法返回的东西是一样的。</p><hr><p>总结一下，定义<code>dataset</code>, 创建<code>dataloder</code>, 迭代获取进行训练。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python-库-you-get下载视频</title>
      <link href="/2020/07/04/python-%E5%BA%93-you-get%E4%B8%8B%E8%BD%BD%E8%A7%86%E9%A2%91/"/>
      <url>/2020/07/04/python-%E5%BA%93-you-get%E4%B8%8B%E8%BD%BD%E8%A7%86%E9%A2%91/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>You-get 是一个可以下载很多在线视频的使用的Python库，我最近主要利用这个库来下载bilibili的视频，下面做一些使用的简单记录。</p><h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>和其他python库安装相同，直接使用pip即可。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">activate deep  <span class="comment"># 激活当前环境</span></span><br><span class="line">pip install you-get  <span class="comment"># 安装you-get库</span></span><br></pre></td></tr></tbody></table></figure></div><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><ol><li>分析视频信息</li></ol><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">you-get -i url <span class="comment"># url 为视频链接地址</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for example</span></span><br><span class="line">you-get -i you-get -i https://www.bilibili.com/video/BV1ST4y1E7FW?from=search&seid=13389194302262225891</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">输出信息：</span></span><br><span class="line"><span class="string">site:                Bilibili</span></span><br><span class="line"><span class="string">title:               【4k60帧】真正的奥利给，“后浪风”演讲——看见的力量</span></span><br><span class="line"><span class="string">streams:             # Available quality and codecs</span></span><br><span class="line"><span class="string">    [ DASH ] ____________________________________</span></span><br><span class="line"><span class="string">    - format:        dash-flv</span></span><br><span class="line"><span class="string">      container:     mp4</span></span><br><span class="line"><span class="string">      quality:       高清 1080P</span></span><br><span class="line"><span class="string">      size:          46.7 MiB (48993131 bytes)</span></span><br><span class="line"><span class="string">    # download-with: you-get --format=dash-flv [URL]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    - format:        dash-flv720</span></span><br><span class="line"><span class="string">      container:     mp4</span></span><br><span class="line"><span class="string">      quality:       高清 720P</span></span><br><span class="line"><span class="string">      size:          32.3 MiB (33909997 bytes)</span></span><br><span class="line"><span class="string">    # download-with: you-get --format=dash-flv720 [URL]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    - format:        dash-flv480</span></span><br><span class="line"><span class="string">      container:     mp4</span></span><br><span class="line"><span class="string">      quality:       清晰 480P</span></span><br><span class="line"><span class="string">      size:          16.4 MiB (17216475 bytes)</span></span><br><span class="line"><span class="string">    # download-with: you-get --format=dash-flv480 [URL]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    - format:        dash-flv360</span></span><br><span class="line"><span class="string">      container:     mp4</span></span><br><span class="line"><span class="string">      quality:       流畅 360P</span></span><br><span class="line"><span class="string">      size:          11.4 MiB (11943322 bytes)</span></span><br><span class="line"><span class="string">    # download-with: you-get --format=dash-flv360 [URL]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    [ DEFAULT ] _________________________________</span></span><br><span class="line"><span class="string">    - format:        flv</span></span><br><span class="line"><span class="string">      container:     flv</span></span><br><span class="line"><span class="string">      quality:       高清 1080P</span></span><br><span class="line"><span class="string">      size:          75.9 MiB (79590049 bytes)</span></span><br><span class="line"><span class="string">    # download-with: you-get --format=flv [URL]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    - format:        flv720</span></span><br><span class="line"><span class="string">      container:     flv</span></span><br><span class="line"><span class="string">      quality:       高清 720P</span></span><br><span class="line"><span class="string">      size:          51.8 MiB (54269960 bytes)</span></span><br><span class="line"><span class="string">    # download-with: you-get --format=flv720 [URL]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    - format:        flv480</span></span><br><span class="line"><span class="string">      container:     flv</span></span><br><span class="line"><span class="string">      quality:       清晰 480P</span></span><br><span class="line"><span class="string">      size:          25.2 MiB (26397350 bytes)</span></span><br><span class="line"><span class="string">    # download-with: you-get --format=flv480 [URL]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    - format:        flv360</span></span><br><span class="line"><span class="string">      container:     flv</span></span><br><span class="line"><span class="string">      quality:       流畅 360P</span></span><br><span class="line"><span class="string">      size:          11.5 MiB (12080563 bytes)</span></span><br><span class="line"><span class="string">    # download-with: you-get --format=flv360 [URL]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'seid' 不是内部或外部命令，也不是可运行的程序</span></span><br><span class="line"><span class="string">或批处理文件。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></tbody></table></figure></div><ol><li>下载视频</li></ol><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang"></div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># download</span></span><br><span class="line">you-get https://www.bilibili.com/video/BV1ST4y1E7FW?\from=search&seid=13389194302262225891</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定路径 -o</span></span><br><span class="line">you-get  -o z:/video https://www.bilibili.com/video/BV1ST4y1E7FW?\from=search&seid=13389194302262225891</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定分辨率 -format</span></span><br><span class="line"></span><br><span class="line">you-get -format==flv720 https://www.bilibili.com/video/BV1ST4y1E7FW?\from=search&seid=13389194302262225891</span><br></pre></td></tr></tbody></table></figure></div><h2 id="注意的问题"><a href="#注意的问题" class="headerlink" title="注意的问题"></a>注意的问题</h2><p>我是<code>win10 Home</code>版系统，会遇到问题：</p><p><strong>‘seid’ 不是内部或外部命令，也不是可运行的程序或批处理文件。</strong></p><p><strong>解决方案：</strong></p><p>将 <code>&seid=</code> 替换为<code>3D</code> 即可</p><p>一条完整的下载视频命令：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang"></div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">you-get  -o z:/video -format==flv720 https://www.bilibili.com/video/BV1ST4y1E7FW?\from=search&seid=13389194302262225891</span><br></pre></td></tr></tbody></table></figure></div></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 实用工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch中的GPU调用</title>
      <link href="/2020/06/16/Pytorch%E4%B8%AD%E7%9A%84GPU%E8%B0%83%E7%94%A8/"/>
      <url>/2020/06/16/Pytorch%E4%B8%AD%E7%9A%84GPU%E8%B0%83%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>一般用Pytorch编程训练模型肯定是要用GPU，多GPU有时候会一直出问题，这里记录一下。</p><h2 id="Pytorch用GPU计算"><a href="#Pytorch用GPU计算" class="headerlink" title="Pytorch用GPU计算"></a>Pytorch用GPU计算</h2><p>可以用Python的方法把程序发送到GPU：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"2"</span> <span class="comment"># 用第三个显卡来计算</span></span><br><span class="line"> </span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0，1，2"</span> <span class="comment"># 用第三个显卡来计算</span></span><br></pre></td></tr></tbody></table></figure></div><p>注意这种方法只能用到一张卡根据我的测试，不能并行计算，要并行计算的话需要一下代码：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用三张显卡并行的跑数据</span></span><br><span class="line">model_cuda = torch.nn.DataParallel(model.cuda(), device_ids=(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"><span class="comment"># batch_size的设置</span></span><br><span class="line">batch_size = origin_batch_size * len(cfg.GPUS)</span><br></pre></td></tr></tbody></table></figure></div><hr><p>注意，一般情况下不用<code>os.environ</code>的话，所有的初始张量、模型都会加载在显卡0，故此时如果0显卡被占满会一直提示<code>CUDA OUT OF MEMORY</code> 错误。</p><hr><h2 id="第二次更新"><a href="#第二次更新" class="headerlink" title="第二次更新"></a>第二次更新</h2><p>Pytorch一般情况主GPU是第一块显卡，可以用以下命令来指定主显卡设备：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.set_device(<span class="number">2</span>)  <span class="comment"># 指定GPU2伟主要计算GPU</span></span><br></pre></td></tr></tbody></table></figure></div><p>建议用这个命令代替python命令。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch加载部分模型参数</title>
      <link href="/2020/06/16/Pytorch%E5%8A%A0%E8%BD%BD%E9%83%A8%E5%88%86%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0/"/>
      <url>/2020/06/16/Pytorch%E5%8A%A0%E8%BD%BD%E9%83%A8%E5%88%86%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="Pytroch模型的加载"><a href="#Pytroch模型的加载" class="headerlink" title="Pytroch模型的加载"></a>Pytroch模型的加载</h2><p><code>Pytorch</code> 模型的加载流程：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = HRNet() <span class="comment"># init model</span></span><br><span class="line">model_file = <span class="string">'state_8.pth'</span> </span><br><span class="line">checkpoint_dict = torch.load(model_state_file) <span class="comment"># load model file</span></span><br><span class="line">model.load_state_dict(checkpoint_dict) <span class="comment"># load parameters</span></span><br><span class="line">model.cuda()</span><br><span class="line">out = model(input) <span class="comment"># calculate results</span></span><br></pre></td></tr></tbody></table></figure></div><p>这样加载模型一般用于测试，会加载模型的所有参数，但是如果需要加载某一层的参数就行不通了，需要后续操作。</p><hr><h2 id="Pytorch模型部分参数加载"><a href="#Pytorch模型部分参数加载" class="headerlink" title="Pytorch模型部分参数加载"></a>Pytorch模型部分参数加载</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model_file = <span class="string">'state_8.pth'</span> </span><br><span class="line"><span class="keyword">if</span> os.path.exists(model_feature):</span><br><span class="line">pre_dict = torch.load(model_feature)</span><br><span class="line">pre_dict = pre_dict[<span class="string">'state_dict'</span>] <span class="comment"># 这里与模型的保存有关系，['state_dict']表示参数</span></span><br><span class="line">need_param = {} <span class="comment"># 待加载的参数集合</span></span><br><span class="line"><span class="keyword">for</span> name,m <span class="keyword">in</span> pre_dict.items(): <span class="comment"># name为参数名称， m为参数</span></span><br><span class="line">    <span class="keyword">if</span> name.split(<span class="string">'.'</span>)[<span class="number">0</span>] == <span class="string">'feature_global'</span>: <span class="comment"># 找到名称为feature_global的层加载</span></span><br><span class="line">     need_param[name] = m</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 加载过滤后的model  strict表示不需要一一对应，找到对应的加载即可</span></span><br><span class="line">        self.load_state_dict(need_param,strict=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure></div><p>总结来说，就是根据层名称来过滤，这里有一个trick就是可以把自己需要改的模型层写到容器<code>nn.Sequence()</code>就可以批量加载，例如上述<code>feature_global</code>:</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">self.feature_global = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, <span class="number">6</span>, <span class="number">2</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">512</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">384</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">384</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">384</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">256</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br></pre></td></tr></tbody></table></figure></div><p>这样便可加载任意层参数，根据层名称加载就有个要求<strong>不能重名</strong>。</p><hr><p>以上就是Pytorch模型的加载，后续有其他问题再更新。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch参数的初始化与冻结部分参数训练</title>
      <link href="/2020/06/09/PyTorch%E5%86%BB%E7%BB%93%E9%83%A8%E5%88%86%E5%8F%82%E6%95%B0%E8%AE%AD%E7%BB%83/"/>
      <url>/2020/06/09/PyTorch%E5%86%BB%E7%BB%93%E9%83%A8%E5%88%86%E5%8F%82%E6%95%B0%E8%AE%AD%E7%BB%83/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>在平时训练中，经常会有冻结某些层的参数，单独训练其他参数以及载入预训练模型来初始化参数的需求，最近也是研究了一下如何实现，记录一下相关的知识。</p><h2 id="参数的初始化"><a href="#参数的初始化" class="headerlink" title="参数的初始化"></a>参数的初始化</h2><ul><li>根据层类型来初始化，例如卷积层，<code>BatchNormal</code> ,<code>ConvTranspose2d</code> 等。</li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">       <span class="comment"># 初始化卷积层参数，判断当前是否为卷积层</span></span><br><span class="line">           <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">               <span class="keyword">if</span> <span class="literal">True</span>:</span><br><span class="line">                   <span class="comment"># 初始化卷积层权重，标准差为0.001的高斯分布</span></span><br><span class="line">                   nn.init.normal_(m.weight, std=<span class="number">0.001</span>)</span><br><span class="line">                   <span class="keyword">for</span> name, _ <span class="keyword">in</span> m.named_parameters():</span><br><span class="line">                       <span class="keyword">if</span> name <span class="keyword">in</span> [<span class="string">'bias'</span>]: </span><br><span class="line">                           nn.init.constant_(m.bias, <span class="number">0</span>) <span class="comment"># 偏置初始化为0</span></span><br><span class="line"> </span><br><span class="line">           <span class="comment"># 初始化批归一化层参数，判断当前是否为BN层</span></span><br><span class="line">           <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">               nn.init.constant_(m.weight, <span class="number">1</span>) <span class="comment"># weight初始化为1</span></span><br><span class="line">               nn.init.constant_(m.bias, <span class="number">0</span>) <span class="comment"># bias 初始化为0 </span></span><br><span class="line"></span><br><span class="line">           <span class="comment"># 初始化反卷积层参数，判断当前是否为反卷积层</span></span><br><span class="line">           <span class="keyword">elif</span> isinstance(m, nn.ConvTranspose2d):</span><br><span class="line">               <span class="comment"># 初始化权重</span></span><br><span class="line">               nn.init.normal_(m.weight, std=<span class="number">0.001</span>)</span><br><span class="line">               <span class="keyword">for</span> name, _ <span class="keyword">in</span> m.named_parameters():</span><br><span class="line">                   <span class="keyword">if</span> name <span class="keyword">in</span> [<span class="string">'bias'</span>]:</span><br><span class="line">                       nn.init.constant_(m.bias, <span class="number">0</span>) <span class="comment"># bias 初始化为0</span></span><br><span class="line">                       <span class="comment">### freezing weights</span></span><br><span class="line">                       <span class="keyword">if</span> self.freeze_weights:</span><br><span class="line">                           m.bias.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure></div><p>故可以采用实例化的方法来初始化模型参数。</p><h2 id="模型的冻结与解冻"><a href="#模型的冻结与解冻" class="headerlink" title="模型的冻结与解冻"></a>模型的冻结与解冻</h2><h3 id="PyTorch中参数的冻结"><a href="#PyTorch中参数的冻结" class="headerlink" title="PyTorch中参数的冻结"></a>PyTorch中参数的冻结</h3><p>​    模型的冻结即参数不再更新即可，可以通过梯度来控制，Pytorch中有几种方法，这里先来总结一下：</p><ul><li><p><strong>Tensor.requires_grad</strong></p><p>这个属性非常重要，在模型<strong>训练</strong>时，<code>requires_grad_()</code>函数会改变<code>Tensor</code>的<code>requires_grad</code>属性并返回<code>Tensor</code>，其默认参数为<code>requires_grad=True</code>。<code>requires_grad=True</code>时，自动求导会记录对<code>Tensor</code>的操作，<code>requires_grad_()</code>的主要用途是告诉自动求导开始记录对<code>Tensor</code>的操作。故只要把该参数设置成False即可冻结。 </p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.freeze_weights:</span><br><span class="line">    m.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">    m.bias.requires_grad = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure></div></li><li><p><strong>with torch.no_grad()</strong></p><p>在该语句块内的代码都会禁止梯度的计算，加快计算速度，一般用于<code>Inference</code>阶段，可以减少计算内存的使用量。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = HRNet()</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    ootput = model(input) <span class="comment"># 不会改变模型参数</span></span><br></pre></td></tr></tbody></table></figure></div></li></ul><ul><li><p><strong>detach()</strong></p><p>detach()函数会函数会返回一个新的Tensor对象<code>b</code>，并且新<code>Tensor</code>是与当前的计算图分离的，其<code>requires_grad</code>属性为<code>False</code>，反向传播时不会计算其梯度。<code>b</code>与<code>a</code>共享数据的存储空间，二者指向同一块内存。</p><p>该函数一般用于<strong>中间需要打印张量</strong>结果，或者是保存图片,使用时应如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = images[i].cpu().detach().numpy()*<span class="number">255</span></span><br></pre></td></tr></tbody></table></figure></div></li><li><p><strong>model.eval()</strong></p><p>训练完train_datasets之后，model要来测试样本了。在model(test_datasets)之前，需要加上<strong>model.eval()</strong>. 否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有<code>batch normalization</code>层所带来的的性质。该参数冻结需要依靠<code>eval()函数</code>，即在<code>Inference</code>时，</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = HRNet()</span><br><span class="line">model.eval() </span><br><span class="line"><span class="keyword">with</span> torch.no_grad:</span><br><span class="line">    output = model(input) <span class="comment"># 如果有BN层，此时不会改变模型参数</span></span><br></pre></td></tr></tbody></table></figure></div><p>保险起见，在<strong>推理</strong>时都需要加上<code>model.eval()</code></p></li><li><p><strong>grad_fn</strong></p><p> 表示积分的方法名</p></li></ul><h3 id="冻结参数的trick"><a href="#冻结参数的trick" class="headerlink" title="冻结参数的trick"></a>冻结参数的trick</h3><p>​    如果一个模型非常巨大，而只改了其中一小块，就可以先把全部冻结，再把一部分解冻，例如</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设model所有参数 requires_grad==False</span></span><br><span class="line"><span class="comment"># 单独的feature_global模块、fuse_layer_def层可以更新参数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parameters_unfreeze</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> para <span class="keyword">in</span> model.feature_global.parameters():</span><br><span class="line">        para.requires_grad = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> para <span class="keyword">in</span> model.fuse_layer_def.parameters():</span><br><span class="line">        para.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></tbody></table></figure></div></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>朴素贝叶斯分类器</title>
      <link href="/2020/06/08/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"/>
      <url>/2020/06/08/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="分类流程"><a href="#分类流程" class="headerlink" title="分类流程"></a>分类流程</h2><p>（1）模型建立</p><script type="math/tex; mode=display">P(C_1|X) = \frac{P(C_1)*P(X|C_1)}{P(X)} = \frac{P(C_1)*P(X|C_1)}{\sum _{i=1}^nP(C_i)*P(X|C_i)}</script><p>其中，X为观测到的样本，求x属于C1的概率，同理多分类只是分母x计算发生变化</p><p>（2）P(x)对所有的分类计算概率来说都只是个常数，并不影响最终结果，故可省略</p><p>（3）分子中的P(C1)是先验概率，需要人为定义，一般假设等概率,亦可其他</p><p>（4）综合<code>2，3</code> 此时变化的量只剩$P(x|C_1)$了，即从C1分布中采样出x的概率，只需要最大化这一项。</p><p>假设C1服从高斯分布，那就是求出<code>均值、方差</code> 这两个参数使得$P(x|C_1)$最大化，也就是最大似然估计：</p><script type="math/tex; mode=display">P(X|C_1) = \prod_{k=1}^n P(x_k|C_1)</script><ul><li>上述用似然估计计算有一个假设就是假设X的属性之间相互独立</li></ul><p><strong>这里的xi表示的是样本的属性！</strong></p><blockquote><p>我一直奇怪这里的小x到底是啥，X是观测到的样本，xk是样本的各个属性，而C1类别对各个属性有个分布，假设是高斯分布，就是调整参数X使得这些个属性满足C1!</p></blockquote><p>（5）对未知样本<strong>X</strong>分类，也就是对每个类<em>C</em>i，计算</p><script type="math/tex; mode=display">P(C_i)*P(X|C_i)</script><p><strong>最终挑选出一个最大的值对应的类别即是X所属的分类。</strong></p><h2 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h2><p>贝叶斯分类器中，x表示属性。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯分类器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从分类到逻辑回归</title>
      <link href="/2020/06/08/%E4%BB%8E%E5%88%86%E7%B1%BB%E5%88%B0%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
      <url>/2020/06/08/%E4%BB%8E%E5%88%86%E7%B1%BB%E5%88%B0%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>关于分类，我博客里面写过很多 相关的：</p><ul><li><a href="https://huaqi.blue/2020/02/25/Classification/" target="_blank" rel="noopener">逻辑回归的由来</a></li><li><a href="https://huaqi.blue/2020/03/23/softmax%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/" target="_blank" rel="noopener">softmax</a></li><li><a href="https://huaqi.blue/2020/02/26/Cross-Entropy-%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/" target="_blank" rel="noopener">交叉熵</a></li></ul><p>但是这些并没有串起来，所以一直是零散的，这里来串一下彻底理解分类相关的东西。</p><blockquote><p>​    一句题外话，开始学习的时候我一直说自己是做回归的不做分类，所以不去学。但是现在看来，作为一个研究CV方向的人，学习分类原理等等完全都是基本素养，需要用心学习。</p></blockquote><h2 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h2><p>首先在分类中，有概率判别式模型和概率生成式模型，现在用朴素贝叶斯来做二分类(属于概率生成式模型)。</p><h3 id="朴素贝叶斯分类器"><a href="#朴素贝叶斯分类器" class="headerlink" title="朴素贝叶斯分类器"></a>朴素贝叶斯分类器</h3><ul><li>模型建立</li></ul><script type="math/tex; mode=display">P(C_1|X) = \frac{P(C_1)*P(X|C_1)}{P(X)} = \frac{P(C_1)*P(X|C_1)}{\sum _{i=1}^2P(C_i)*P(X|C_i)}</script><p>其中，X为观测到的样本，求x属于C1的概率</p><ul><li>假设C1服从高斯分布，那就是求出<code>均值、方差</code> 这两个参数使得$P(x|C_1)$最大化，也就是最大似然估计：</li></ul><script type="math/tex; mode=display">P(X|C_1) = \prod_{k=1}^n P(x_k|C_1)</script><ul><li>选择最好的函数</li></ul><h3 id="二元高斯分布下的贝叶斯分类器转换为回归问题"><a href="#二元高斯分布下的贝叶斯分类器转换为回归问题" class="headerlink" title="二元高斯分布下的贝叶斯分类器转换为回归问题"></a><strong>二元高斯分布下的贝叶斯分类器转换为回归问题</strong></h3><p>具体推导文章开头第一篇文章有讲，这里只给结论：</p><p>通过推导可以看出，分类问题和回归问题是统一的，我们不必再通过计算μ1μ1,μ2μ2,Σ1Σ1,Σ2Σ2,N1N1,N2N2来计算概率，而可以通过DeepLearning的手段直接计算w和b，从而求得概率，而与线性回归不同的是，我们需要将输出结果用σ()函数转化到0~1之间，这就是<strong>逻辑回归</strong>。</p><h2 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h2><p><strong>Note:</strong> 引自<a href="https://qqtoyota.github.io/post/从分类到逻辑回归/" target="_blank" rel="noopener">blog</a> ：写的很棒！</p><p><code>softmax文章开始也有提到，是在深度学习中常用的多分类函数。</code></p><p>多分类计算有两种方式：</p><ul><li>K个独立的二元分类器——用于<strong>不互斥</strong>的分类</li><li>softmax——用于<strong>互斥</strong>的分类</li></ul><p><strong>softmax VS k个二元分类器</strong></p><p>如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？ 这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。） 如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。 现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。(i) 假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用sofmax回归还是 3个logistic 回归分类器呢？ (ii) 现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 softmax 回归还是多个 logistic 回归分类器呢？ 在第一个例子中，三个类别是互斥的，因此更适于选择softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。</p><h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2><p>在交叉熵一文中举的例子就用的逻辑回归，解释了为什么逻辑回归用交叉熵当损失函数，至此，一切都顺理成章了。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>爬虫入门</title>
      <link href="/2020/06/07/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/"/>
      <url>/2020/06/07/%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>近期出于某些原因突然对爬虫来了兴趣，于是花了几个小时研究了一下爬虫，重新捡起来了，理清了爬虫的一个流程，以前学了很久一直不是很懂，不会自己去爬需要的东西。理清楚之后很简单，很多东西一目了然。在此记录一下简单的爬虫流程。</p><h3 id="爬虫的大致流程"><a href="#爬虫的大致流程" class="headerlink" title="爬虫的大致流程"></a>爬虫的大致流程</h3><ul><li><strong>抓取到整个url的html代码</strong></li><li><strong>解析代码中需要的部分</strong></li><li><strong>数据保存</strong></li><li>高级爬虫（进阶）</li></ul><p>上述流程中，我自己只学了前两个部分，对我自己来说也是够用了，第三部分对于不以爬虫为工作的，我认为就不用专门去学习了，可以写入txt文件或者直接下载图片。。至于最后一部分就涉及到爬虫的框架，分布式爬虫等等，这个对于个人来说我觉得也是不必要的。</p><h3 id="具体知识点"><a href="#具体知识点" class="headerlink" title="具体知识点"></a>具体知识点</h3><h4 id="抓取url"><a href="#抓取url" class="headerlink" title="抓取url"></a>抓取url</h4><p>我用的是方便、简单的<code>requests</code>库，操作如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'https://www.baidu.com'</span></span><br><span class="line">headers = {<span class="string">'User-Agent'</span>: <span class="string">"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50"</span>}</span><br><span class="line">response = requests.get(url=url, headers=headers)</span><br><span class="line">reponse.status_code <span class="comment"># 200表示正常访问</span></span><br><span class="line"><span class="comment">#抓取到的html的所有代码</span></span><br><span class="line">html_text = response.text</span><br></pre></td></tr></tbody></table></figure></div><p>第一步比较简单，几行代码，当然其中也有高级操作，比如代理之类的，后面在讲。</p><h4 id="解析html-重点"><a href="#解析html-重点" class="headerlink" title="解析html 重点"></a>解析html <strong>重点</strong></h4><p>xpath比较全的一个博客：<a href="https://www.jianshu.com/p/85a3004b5c06" target="_blank" rel="noopener">https://www.jianshu.com/p/85a3004b5c06</a></p><p>拿到html内容后，就要分析一下，找出自己需要的内容，html解析有<code>xpath</code>、<code>BeautifulSoup</code> 等库，由于<code>Bs4</code>我并不会（虽然学过），所以这里就用<code>xpath</code>~</p><div class="table-container"><table><thead><tr><th style="text-align:left">表达式</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left">nodename</td><td style="text-align:left">选取此节点的所有子节点</td></tr><tr><td style="text-align:left">/</td><td style="text-align:left">从当前节点选区直接子节点</td></tr><tr><td style="text-align:left">//</td><td style="text-align:left">全局查找某个节点</td></tr><tr><td style="text-align:left">.</td><td style="text-align:left">选取当前节点</td></tr><tr><td style="text-align:left">..</td><td style="text-align:left">选取当前节点的父节点</td></tr><tr><td style="text-align:left">@</td><td style="text-align:left">选取属性</td></tr><tr><td style="text-align:left">and</td><td style="text-align:left">多个属性同时匹配</td></tr></tbody></table></div><p>我认为会这些基本就够了，可以抓取下来任何自己想要的了，至少解析不会有问题。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lxml.etree</span><br><span class="line">parser = etree.HTMLParser(encoding=<span class="string">'utf-8'</span>) <span class="comment"># 创建解释器</span></span><br><span class="line">html = etree.parse(<span class="string">'baidu.html'</span>, parser=parser) <span class="comment"># 解析html</span></span><br><span class="line"><span class="comment"># xpath返回的是list</span></span><br><span class="line"><span class="comment"># 全局搜索id为pins的ul标签下面所有的li标签下面的a标签中的href(链接)</span></span><br><span class="line"><span class="comment"># @为精确匹配</span></span><br><span class="line">link = html.xpath(<span class="string">"//ul[@id='pins']//li/a/@href"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 全局搜索class为"item-0"的li标签下面的第一个a标签中的文本 用/text()获取</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[@class="item-0"]/a/text()'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># and表示同时满足多个属性</span></span><br><span class="line">result = html.xpath(<span class="string">'//li[contains(@class, "li") and @name="item"]/a/text()'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模糊匹配 全局搜索class名称包含'li'的li标签下面的第一个a标签中的文本  </span></span><br><span class="line">result = html.xpath(<span class="string">'//li[contains(@class, "li")]/a/text()'</span>)</span><br></pre></td></tr></tbody></table></figure></div><p>至此就解析完成，拿到了自己想要的，爬虫基本也随之结束了。</p><p>拿到的是一堆字符串，要下载要提取就用python的方法处理即可，关于下载图片可以用requests直接保存：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">h = open(img_name,<span class="string">'wb'</span>) <span class="comment"># img_name 为要保存的图片名称</span></span><br><span class="line">h.write(requests.get(img_src,headers=header).content) <span class="comment">#img_src为字符串</span></span><br><span class="line">h.close()</span><br></pre></td></tr></tbody></table></figure></div><h4 id="问题、解决："><a href="#问题、解决：" class="headerlink" title="问题、解决："></a>问题、解决：</h4><ul><li><p><strong>抓取url</strong></p><p>在抓取url时(<strong>get</strong>)，有时候非常慢，或者是下载多了ip会直接被封。有以下几个解决方案:</p><ul><li><p>爬虫时例如下载图片时，一定要有时间间隔</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure></div></li><li><p>动态获取ip：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># # @Time    : 2020/6/7 下午 8:09</span></span><br><span class="line"><span class="comment"># # @Author  : fry</span></span><br><span class="line"><span class="comment"># @FileName: ip.py</span></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pnw</span><span class="params">()</span>:</span>  <span class="comment"># 定义一个循环的函数</span></span><br><span class="line">    headers = {</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 "</span></span><br><span class="line">                      <span class="string">"(KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1"</span>}</span><br><span class="line">    <span class="comment"># 依次遍历生成2-99</span></span><br><span class="line">    ip_address = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">100</span>):</span><br><span class="line">        url = <span class="string">"https://www.kuaidaili.com/free/inha/"</span> + str(i) + <span class="string">"/"</span>  <span class="comment"># 爬取的免费ip</span></span><br><span class="line">        response = requests.get(url, headers=headers).text  <span class="comment"># 获得网页文本数据</span></span><br><span class="line">        response_xpath = etree.HTML(response)  <span class="comment"># 转换为xpath可用结构</span></span><br><span class="line">        ips = response_xpath.xpath(<span class="string">'//*[@id="list"]/table/tbody/tr/td[1]/text()'</span>)  <span class="comment"># ip的信息</span></span><br><span class="line">        dks = response_xpath.xpath(<span class="string">'//*[@id="list"]/table/tbody/tr/td[2]/text()'</span>)  <span class="comment"># 端口的信息</span></span><br><span class="line">        https = response_xpath.xpath(<span class="string">'//*[@id="list"]/table/tbody/tr[2]/td[4]/text()'</span>)  <span class="comment"># http信息</span></span><br><span class="line">        <span class="keyword">for</span> ip, dk, http <span class="keyword">in</span> zip(ips, dks, https):</span><br><span class="line">            proxy = <span class="string">"http://"</span> + ip + <span class="string">":"</span> + dk  <span class="comment"># 拼接ip</span></span><br><span class="line">            proxies = {<span class="string">"http"</span>: proxy}</span><br><span class="line">            ip_address.append(proxies)</span><br><span class="line">    <span class="keyword">return</span> ip_address</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    ips = pnw()</span><br><span class="line">    print(ips)</span><br></pre></td></tr></tbody></table></figure></div><p>以上代码可以获取ip，使用时:</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip = pnw()</span><br><span class="line">pic_page = requests.get(url=url_pic, headers=header,proxies=ip[<span class="number">0</span>]) <span class="comment">#使用ip代理</span></span><br></pre></td></tr></tbody></table></figure></div></li></ul></li></ul><p>  基本如此，掌握之后可以随心所欲爬取自己所需。</p><hr><ul><li><p>如果想要爬一个网站所有的东西，<strong>需要自己去分析一下url规律</strong>，比如哪个标签里面有所有的链接等等</p></li><li><p>关于反爬机制，用好代理</p><p><strong>最后，关于爬虫的合法性：</strong></p><p><code>url</code>拼接上<a href="https://www.taobao.com/robots.txt" target="_blank" rel="noopener">/robots.txt</a>  可以看到网站的爬虫协议：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">yaml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight yaml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">User-agent:</span>  <span class="string">Baiduspider</span> <span class="comment"># 允许百度爬虫引擎</span></span><br><span class="line"><span class="attr">Allow:</span>  <span class="string">/article</span> <span class="comment"># 允许访问/article.htm，/article/12345.com</span></span><br><span class="line"><span class="attr">Allow:</span>  <span class="string">/oshtml</span>      </span><br><span class="line"><span class="attr">Allow:</span>  <span class="string">/wenzhang</span></span><br><span class="line"><span class="attr">Disallow:</span>  <span class="string">/product/</span> <span class="comment"># 禁止访问/product/12345.com</span></span><br><span class="line"><span class="attr">Disallow:</span>  <span class="string">/</span>         <span class="comment"># 禁止了访问除Allow规定页面的其他所有页面</span></span><br><span class="line"></span><br><span class="line"><span class="attr">User-Agent:</span>  <span class="string">Googlebot</span>  <span class="comment"># 谷歌爬虫引擎</span></span><br><span class="line"><span class="attr">Allow:</span>  <span class="string">/article</span></span><br><span class="line"><span class="attr">Allow:</span>  <span class="string">/oshtml</span></span><br><span class="line"><span class="attr">Allow:</span>  <span class="string">/product</span> <span class="comment"># 允许访问/product.htm，/product/12345.com</span></span><br><span class="line"><span class="attr">Allow:</span>  <span class="string">/spu</span></span><br><span class="line"><span class="attr">Allow:</span>  <span class="string">/dianpu</span></span><br><span class="line"><span class="attr">Allow:</span>  <span class="string">/wenzhang</span></span><br><span class="line"><span class="attr">Allow:</span>  <span class="string">/oversea</span></span><br><span class="line"><span class="attr">Disallow:</span>  <span class="string">/</span></span><br></pre></td></tr></tbody></table></figure></div><p>但是这只是网站的协议，具体的：</p><p><strong>从目前的情况来看，如果抓取的数据属于个人使用或科研范畴，基本不存在问题; 而如果数据属于商业盈利范畴，就要就事而论，有可能属于违法行为，也有可能不违法。</strong></p></li></ul><hr><p>  最近写了一个爬虫可以抓取某网站<strong>所有</strong>的图片(<strong>增加科研兴趣</strong>，现有网络的代码只能抓取某几页)，具体代码就不放出来了。需要可以通过关于我z红的<code>email</code>  、<code>微信</code>联系我。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> python爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git错误与代理相关</title>
      <link href="/2020/06/06/git%E9%94%99%E8%AF%AF%E4%B8%8E%E4%BB%A3%E7%90%86%E7%9B%B8%E5%85%B3/"/>
      <url>/2020/06/06/git%E9%94%99%E8%AF%AF%E4%B8%8E%E4%BB%A3%E7%90%86%E7%9B%B8%E5%85%B3/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>前一篇文章，我写到了git代理设置相关的东西,实际上应该根据自己的vpn选择开哪一种代理。<code>ssr</code>用sock5代理是可以的，但是<code>clash</code>就行不通，需要把git代理取消，然后设置成http代理。</p><p>代理的端口号要根据vpn来设置，并不是通用的。</p><p>我自己的git突然不能用，错误大致如下：</p><p><code>OpenSSL......Error443</code></p><p>解决如下：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git config --global --<span class="built_in">unset</span> http.proxy</span><br><span class="line">git config --global --<span class="built_in">unset</span> https.proxy</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置 http代理</span></span><br><span class="line">git config --global https.proxy http://127.0.0.1:根据vpn选择端口号</span><br><span class="line"></span><br><span class="line">git config --global https.proxy https://127.0.0.1:根据vpn选择端口号</span><br></pre></td></tr></tbody></table></figure></div><hr><p><code>Error 403  request denail</code></p><p>该错误表示登录错了账号，拒绝推送，我自己也是记混了git账号，所以出现了这个问题。</p><hr><p>另外，安利一款梯子：<code>Clash</code></p><ul><li>自动选择节点</li><li>界面舒适</li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Contrastive Loss (对比损失)分析</title>
      <link href="/2020/05/18/Contrastive-Loss-%E5%AF%B9%E6%AF%94%E6%8D%9F%E5%A4%B1-%E5%88%86%E6%9E%90/"/>
      <url>/2020/05/18/Contrastive-Loss-%E5%AF%B9%E6%AF%94%E6%8D%9F%E5%A4%B1-%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="定义及分析"><a href="#定义及分析" class="headerlink" title="定义及分析"></a>定义及分析</h2><p><strong>Contrastive loss</strong> 即对比损失，其表达式为：</p><script type="math/tex; mode=display">L = \frac{1}{2N}\sum_{n=1}^Nyd^2 + (1-y)max(margin-d, 0)^2</script><p>其中，</p><ul><li><p><code>y</code>表示标签值，取值为 <code>1，0</code>; </p></li><li><p><code>margin</code>是人为定义的边界距离；</p></li><li>d表示两个样本的<code>欧氏距离</code></li></ul><p>输入为 ( <strong>dataA, dataB, label</strong> )</p><p><a href="https://user-images.githubusercontent.com/60562661/82234076-790e1700-9963-11ea-8535-c50868b4e5c4.jpg" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" style="zoom: 35%;" title="img" data-src="https://user-images.githubusercontent.com/60562661/82234076-790e1700-9963-11ea-8535-c50868b4e5c4.jpg" class="lazyload"></a></p><p>该损失函数可以有效的处理<strong>孪生神经网络</strong>中的<strong>paired data</strong>的关系，具体逻辑是：</p><ul><li>若一对数据是同一组，那么<code>label = 1</code> 即<code>y = 1</code> ，只有前半部分起作用，后半部分值为0；此时最小化loss, 两者的欧氏距离变小，就会使两者更相似；</li><li>若一对数据非同一组，那么<code>label = 0</code> 即<code>y = 0</code>， 只有后半部分起作用，前半部分值为0，此时最小化损失函数，即最小化<code>margin - d</code>, 就等于最大化<code>d</code> ,即两者的欧氏距离变大，更不相似</li></ul><h2 id="梯度求解"><a href="#梯度求解" class="headerlink" title="梯度求解"></a>梯度求解</h2><p><strong>y = 1</strong>时， </p><script type="math/tex; mode=display">L = \frac{1}{2N}\sum_{n=1}^NyD^2 =  \frac{1}{2N}\sum_{n=1}^N(||x_1-x_2||_2))^2</script><script type="math/tex; mode=display">\frac{\partial L}{\partial X} = \frac{\partial D_w}{\partial X} = ||x_1-x_2||_2*\partial D/\partial X</script><p>此处省略，以后补充</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># # @Time    : 2020/5/17 上午 11:19</span></span><br><span class="line"><span class="comment"># # @Author  : fry</span></span><br><span class="line"><span class="comment"># @FileName: clac.py</span></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">计算欧式距离</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">euclidean_dist_vector</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param x: [100]</span></span><br><span class="line"><span class="string">    :param y: [100]</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    dist = torch.sqrt(torch.sum(torch.pow((x-y), <span class="number">2</span>)))</span><br><span class="line">    <span class="keyword">return</span> dist</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">euclidean_dist</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      x: pytorch Variable, with shape [m, d]</span></span><br><span class="line"><span class="string">      y: pytorch Variable, with shape [n, d]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      dist: pytorch Variable, with shape [m, n]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m, n = x.size(<span class="number">0</span>), y.size(<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># xx经过pow()方法对每单个数据进行二次方操作后，在axis=1 方向（横向，就是第一列向最后一列的方向）加和，此时xx的shape为(m, 1)，经过expand()方法，扩展n-1次，此时xx的shape为(m, n)</span></span><br><span class="line">    xx = torch.pow(x, <span class="number">2</span>).sum(<span class="number">1</span>, keepdim=<span class="literal">True</span>).expand(m, n)</span><br><span class="line">    <span class="comment"># yy会在最后进行转置的操作</span></span><br><span class="line">    yy = torch.pow(y, <span class="number">2</span>).sum(<span class="number">1</span>, keepdim=<span class="literal">True</span>).expand(n, m).t()</span><br><span class="line">    dist = xx + yy</span><br><span class="line">    <span class="comment"># torch.addmm(beta=1, input, alpha=1, mat1, mat2, out=None)，这行表示的意思是dist - 2 * x * yT</span></span><br><span class="line">    dist.addmm_(<span class="number">1</span>, <span class="number">-2</span>, x, y.t())</span><br><span class="line">    <span class="comment"># clamp()函数可以限定dist内元素的最大最小范围，dist最后开方，得到样本之间的距离矩阵</span></span><br><span class="line">    dist = dist.clamp(min=<span class="number">1e-12</span>).sqrt()  <span class="comment"># for numerical stability</span></span><br><span class="line">    <span class="keyword">return</span> dist</span><br></pre></td></tr></tbody></table></figure></div><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># # @Time    : 2020/5/11 下午 7:52</span></span><br><span class="line"><span class="comment"># # @Author  : fry</span></span><br><span class="line"><span class="comment"># @FileName: MatchLoss.py</span></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> utils.clac <span class="keyword">import</span> euclidean_dist,euclidean_dist_vector</span><br><span class="line"><span class="comment"># match loss function</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ContrastiveLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Contrastive loss function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Based on:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, margin=<span class="number">1.2</span>)</span>:</span></span><br><span class="line">        super(ContrastiveLoss, self).__init__()</span><br><span class="line">        self.margin = margin</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_type_forward</span><span class="params">(self, in_types)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> len(in_types) == <span class="number">3</span></span><br><span class="line">        <span class="comment">#[1,1,10,10] [1,1,10,10] [10]</span></span><br><span class="line">        x0_type, x1_type, y_type = in_types</span><br><span class="line">        <span class="keyword">assert</span> x0_type.size() == x1_type.shape</span><br><span class="line">        <span class="comment"># assert x1_type.size()[1] == y_type.shape[0]</span></span><br><span class="line">        <span class="comment"># assert x1_type.size()[0] > 0</span></span><br><span class="line">        <span class="keyword">assert</span> x0_type.dim() == <span class="number">4</span></span><br><span class="line">        <span class="keyword">assert</span> x1_type.dim() == <span class="number">4</span></span><br><span class="line">        <span class="comment"># assert y_type.dim() == 1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x0, x1, y)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.check_type_forward((x0, x1, y))</span></span><br><span class="line"></span><br><span class="line">        data_1 = x0 <span class="comment"># [1,10,10]</span></span><br><span class="line">        data_2 = x1 <span class="comment"># [1,10,10]</span></span><br><span class="line">        <span class="comment"># euclidian distance</span></span><br><span class="line">        distance = euclidean_dist_vector(data_1, data_2)</span><br><span class="line">        <span class="comment"># distance = euclidean_dist(data_1, data_2)</span></span><br><span class="line">        md = self.margin - distance</span><br><span class="line">        dist = torch.clamp(md, min=<span class="number">0.0</span>)</span><br><span class="line">        loss = y * torch.pow(distance, <span class="number">2</span>) + (<span class="number">1</span>-y)*torch.pow(dist, <span class="number">2</span>)</span><br><span class="line">        loss = torch.mean(loss/<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># loss = torch.sum(loss) / 2.0</span></span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line"><span class="comment"># if __name__ == '__main__':</span></span><br><span class="line"><span class="comment">#     citer = ContrastiveLoss()</span></span><br><span class="line"><span class="comment">#     a = torch.randn(10,10)</span></span><br><span class="line"><span class="comment">#     b = torch.randn(10,10)</span></span><br><span class="line"><span class="comment">#     y = torch.ones(1)</span></span><br><span class="line"><span class="comment">#     loss = citer(a,b,y)</span></span><br><span class="line"><span class="comment">#     print(loss)</span></span><br></pre></td></tr></tbody></table></figure></div></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Contrastive Loss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVM支持向量机</title>
      <link href="/2020/05/03/SVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
      <url>/2020/05/03/SVM%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>李宏毅老师视频：<a href="https://www.bilibili.com/video/BV14W411u78t?from=search&seid=16299976979408181063" target="_blank" rel="noopener">https://www.bilibili.com/video/BV14W411u78t?from=search&seid=16299976979408181063</a></p><p>知乎讲解  ：<a href="https://zhuanlan.zhihu.com/p/49331510" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49331510</a></p><p>数学完整推导视频：<a href="https://www.bilibili.com/video/BV12J41157qV?p=6" target="_blank" rel="noopener">https://www.bilibili.com/video/BV12J41157qV?p=6</a> （浙大机器学习课程）</p><p>文中几张图引自改知乎文章。</p><h1 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>整体上，支持向量机是一个分类算法，即寻找能分开两类数据的超平面，并且这个超平面满足距离超平面最近的数据点的到超平面距离最大。</p><p><a href="https://user-images.githubusercontent.com/60562661/80908414-43a5de80-8d52-11ea-85e0-4c37fbcf497a.png" data-fancybox="group" data-caption="1588485540815" class="fancybox"><img alt="1588485540815" title="1588485540815" data-src="https://user-images.githubusercontent.com/60562661/80908414-43a5de80-8d52-11ea-85e0-4c37fbcf497a.png" class="lazyload"></a></p><p>如图，H2和H3都实现了正确的分类，但是SVM会选择H3，因为两类数据中距离H3最近的点的距离比其他所有的点到H3的距离都要大，满足一开始所述的条件。</p><p><a href="https://user-images.githubusercontent.com/60562661/80908422-486a9280-8d52-11ea-9567-210a53e582f8.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/80908422-486a9280-8d52-11ea-9567-210a53e582f8.jpg" class="lazyload"></a></p><p> 目前求解目标是使得图中<code>margin</code>最大,经过推导得出最终的抽象的公式：</p><script type="math/tex; mode=display">min_{W,b}\frac{1}{2}||W||^2,s.t. y_i*(X_i^TW+B) >= 1</script><p>这个公式也就是硬匹配规则，简单解释一下：</p><ul><li>yi表示第i个数据的真实类别，取值为 1，-1；</li><li>第i笔数据真实类别与计算出来的值同号，即保证分类准确的前提下，最大化边界。</li></ul><p><a href="https://user-images.githubusercontent.com/60562661/80908416-456fa200-8d52-11ea-9f08-6e017d7a1002.png" data-fancybox="group" data-caption="1588486535546" class="fancybox"><img alt="1588486535546" title="1588486535546" data-src="https://user-images.githubusercontent.com/60562661/80908416-456fa200-8d52-11ea-9f08-6e017d7a1002.png" class="lazyload"></a></p><p>这些处于边界的几个数据点称为支持向量，<strong>在决定最佳超平面时只有支持向量起作用，而其他数据点并不起作用。</strong></p><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p><code>SVM = Hingle_loss + Kernel_Method</code></p><h3 id="Hingle-Loss"><a href="#Hingle-Loss" class="headerlink" title="Hingle Loss"></a>Hingle Loss</h3><script type="math/tex; mode=display">l(f(x_n),\hat y_n) = max(0,1-\hat y_n*f(x_n))</script><p>其中，</p><script type="math/tex; mode=display">\hat y_n = 1,-1</script><p>当真值与预测值同号，并且乘积比比正确的好过一段距离，即乘积大于等于1时，loss为0，好过的这段距离即为margin。参考下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/80908417-46083880-8d52-11ea-8411-64b067eb54d7.png" data-fancybox="group" data-caption="1588487132298" class="fancybox"><img alt="1588487132298" title="1588487132298" data-src="https://user-images.githubusercontent.com/60562661/80908417-46083880-8d52-11ea-8411-64b067eb54d7.png" class="lazyload"></a></p><h3 id="Linear-SVM"><a href="#Linear-SVM" class="headerlink" title="Linear SVM"></a>Linear SVM</h3><ul><li><strong>Model:</strong></li></ul><script type="math/tex; mode=display">f(x) = \sum_iw_ix_i +b =  \begin{bmatrix}   w\\   b \\  \end{bmatrix} ·\begin{bmatrix}   x\\   1 \\  \end{bmatrix}= W^Tx</script><ul><li><strong>Loss function</strong></li></ul><script type="math/tex; mode=display">L(f) = \sum_nl(f(x_n),\hat y_n) + \lambda||w||_2</script><p>损失函数是一个凸函数，因此可以用梯度下降法来求解，具体看李宏毅老师视频。</p><p>这里的SVM损失函数和一开始概述里面提到的目标函数似乎不太一样？经过如下转换，就得到了最初的形式。</p><p><a href="https://user-images.githubusercontent.com/60562661/80908419-46a0cf00-8d52-11ea-92f7-659896a8e9be.png" data-fancybox="group" data-caption="1588487368252" class="fancybox"><img alt="1588487368252" title="1588487368252" data-src="https://user-images.githubusercontent.com/60562661/80908419-46a0cf00-8d52-11ea-92f7-659896a8e9be.png" class="lazyload"></a></p><p>最后面1减去了一项，表示是软匹配，不一定能达到1的情况。</p><h3 id="Kernel-Method"><a href="#Kernel-Method" class="headerlink" title="Kernel Method"></a>Kernel Method</h3><p>首先，要找的W*(w,b)是数据点的线性组合：</p><script type="math/tex; mode=display">w^* = \sum_na^*_n*x^n</script><p>为了解释这件事，一般就引入了拉格朗日方法来算一通，这里用另外一种思路：</p><p><a href="https://user-images.githubusercontent.com/60562661/80908420-47396580-8d52-11ea-8ebc-e4f024e97b2d.png" data-fancybox="group" data-caption="1588488346353" class="fancybox"><img alt="1588488346353" title="1588488346353" data-src="https://user-images.githubusercontent.com/60562661/80908420-47396580-8d52-11ea-8ebc-e4f024e97b2d.png" class="lazyload"></a></p><p>用梯度下降法求解时，cn(w)由于hingleloss的原因，会有一部分值为0，即不是所有的xn都会对w有影响，w初始化为0时很多数据点对更新w的值毫无用处，即a*是稀疏的。</p><p>当a* != 0时，对应的数据点就叫做 <code>Support vector</code>.支持向量机名字也由此得来。</p><p>此时w可以换一种形式写：</p><script type="math/tex; mode=display">w = \sum_na_nx_n = X \alpha</script><p>故，Model：</p><script type="math/tex; mode=display">f(x) = w^Tx = \alpha^TX^Tx =\sum_na_n(x_n·x) = \sum_na_nK(x_n,x)</script><p>找一个最好的 <strong>α</strong> ，最小化损失函数：</p><script type="math/tex; mode=display">L(f) =\sum_nl(f(x_n),\hat y_n) = \sum_nl(\sum_{n'}a_{n'}K(x_{n'},x_n),\hat y_n)</script><p>这里不需要知道具体的x，只要能求出<code>K(x,z)</code>的值即可。</p><h3 id="Kernel-Trick"><a href="#Kernel-Trick" class="headerlink" title="Kernel Trick"></a>Kernel Trick</h3><p>上述的损失函数如果直接用x，z是不好的，需要用他们的特征。用核方法就不用管他们的 特征是什么样子的。</p><p>即两个向量转化为特征向量再求内积是比较麻烦的，直接计算两个向量的内积再求特征会更快。</p><p>此时需要定义一个核函数，就可以简化计算。这里的</p><script type="math/tex; mode=display">K(x,z)</script><p>就是一个核函数，<strong>可以选取不同的核函数。</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/80908421-47d1fc00-8d52-11ea-8c1b-0d4d26919f99.png" data-fancybox="group" data-caption="1588490434815" class="fancybox"><img alt="1588490434815" title="1588490434815" data-src="https://user-images.githubusercontent.com/60562661/80908421-47d1fc00-8d52-11ea-8c1b-0d4d26919f99.png" class="lazyload"></a></p><p>而核函数计算就是把原始数据投影到高维的一个内积，即在做分类时，先把二维数据投影到高维空间，在做一个 线性分类，上图就很明显了。</p><h2 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h2><ul><li>Kernel Trick 是投影到高维的一个计算？</li><li>逻辑是计算出K，就可以找出α了。而投影到高维只是计算K的一个方法，为什么可以代表svm呢?</li><li>这些疑问以后再看。</li></ul><hr><p>2020-6-7日更新：</p><p><strong>SVM通过数学理解更加合理！</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>小感</title>
      <link href="/2020/04/30/%E8%BF%87%E5%8E%BB%EF%BC%8C%E7%8E%B0%E5%9C%A8%EF%BC%8C%E5%B0%86%E6%9D%A5%EF%BC%9F-%E8%87%B4%E8%87%AA%E5%B7%B1/"/>
      <url>/2020/04/30/%E8%BF%87%E5%8E%BB%EF%BC%8C%E7%8E%B0%E5%9C%A8%EF%BC%8C%E5%B0%86%E6%9D%A5%EF%BC%9F-%E8%87%B4%E8%87%AA%E5%B7%B1/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>新冠肺炎使我再读书阶段经历了最长的一段假期，但是假期越久，舒适环境久了就再也不想回去了。研究生压力很大，事情很多，一直在磨着我的性子。</p><p>本科的时候我一直觉得自己经常在学习，但在现在看来，我一直在骗自己。从痴迷于平面设计到视频动效设计，为此学习了各种软件，<code>PhotoShop、Premiere、AfterEffects</code>, <code>Cinema4D, ParticleFlow</code>, 甚至也去接触了<code>Houdini</code>，但是直到最后我却没有像样的作品拿出手，<strong>我才真正看清了自己所谓的努力</strong>，只是以学完一个东西为目标，这种出发点一开始就是错的。对于设计，软件只是工具，出发点一直应该是<code>惊艳的作品</code>。我自己非常的缺乏独立思考，在动效里面，完全可以自己去模仿别人制作，但是总是找借口，<strong>先是一直说电脑太差，然后换了一个(+1)；然后有觉得做一个片子太久了自己没有时间，事实上我所谓没有时间是因为三四个四五个小时打lol就过去了。</strong> 理性分析一下，还是因为大学觉悟太低，别人总会说我努力，然后时间久了我自己也慢慢相信这一笑话了。想想真是可笑，自己陶醉于自己的谎言里，如此可悲。</p><p>本科当然并非一无所获，本科直到现在，自己最为自豪的技能就是<code>PhotoShop</code> 了，<code>PS</code>已经内化为我的一个技能了，熟稔于心了，基本日常需求是忘不了了。反思原因，大学的时候大一到大二期间我一直在学PS，PS本身东西是挺多的，关键是实践出真知，加上坚持，那时候很痴迷于图片合成，基本每天都要做一张图，看着别人的教程去做，久而久之我自己都没意识到已经很熟悉的掌握了。同时当时的练习作品有14G左右。</p><p>然而正是有了本科的不足，研究生的经历，才会更加怀念本科时光。本科时候也是我最快乐的时光了，压力很小，和大家一起焦虑、一起迷茫，想学的时候学，想玩就玩，和同学约说出去就出去，说唱歌就唱歌，那段时间自由，散漫，说话也是口无遮拦。和现在非常卑微的自己形成鲜明的对比：没有周末，说话小心翼翼，经常被怼，烦心的事情一件又一件。</p><p>本科有多美好现在就有多苦，出来混，早晚要还的。苍天饶过谁？</p><p>​                                                                                                ————致碌碌无为的自己，间歇性踌躇满志、持续性混吃等死的自己。</p><p><a href="https://user-images.githubusercontent.com/60562661/80718656-0280b500-8b2d-11ea-8a87-5445f758a2cd.JPG" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/80718656-0280b500-8b2d-11ea-8a87-5445f758a2cd.JPG" class="lazyload"></a></p><p>固然现在的自己很差，还是应该理性分析一下问题出在哪里，指定符合自己的策略。</p><hr><h2 id="不完美的现在"><a href="#不完美的现在" class="headerlink" title="不完美的现在"></a>不完美的现在</h2><ul><li>彻底反思一下自己，<strong>一直效率不高</strong>。关键在于不能集中精力去做一件事，经常会分心，分心原因在于 <strong>手机、电脑</strong>，如果做什么都可以全力去做，会比现在好很多。</li><li><strong>三分钟热度</strong>，总是觉得什么都可以学，但是事实上并不是这样，经过了大学四年的试验，已经深刻体会到，什么都分开一段段去做，真的很容易放弃，很容易半途而废，而且自己还会觉得很爽，或者是自信心又会受到打击，又会责怪自己。除非是自己真心所热爱的，否则对于我自己来说放弃是分分钟的。</li><li>有时候会安慰自己，现实大概率是高斯分布，既然我是这样，那么很多人应该也是。事实上并不是这样，比你努力的人多了去了。</li><li>我经常自嘲智商不够高，自己不够聪明，情商太低。事实上内心深处自己才知道自己对自己的期望，对于专业知识、日常交流等等，和一般人比，我不比一般人差，但是人情世故我可能确实是木讷了点儿，这一点我也并未在意。既然智商正常，为什么还是只到了现在这种程度？还是因为假装努力。每天bilibili刷两三个小时，三国杀两三个小时，白天4-6小时就过去了，吃饭耽误三个小时，9个小时没了，有多少时间提升专业技能呢？</li></ul><blockquote><p>目前缺点很多，唯一优点就是至少还有上进心，不是一点上进心都没有。</p></blockquote><p>前面提到很难坚持一件事情，但是如果是以钱为导向呢？研究生毕业后如果能进大厂，那么最白菜的工资也是年薪<code>25W</code>，对于一个农村人真的是诱惑力很大，所以此时<code>对钱的渴望 > 兴趣</code>, 也可以说是坚持下去的很强的一个理由。</p><h2 id="冲出现在"><a href="#冲出现在" class="headerlink" title="冲出现在"></a>冲出现在</h2><p>为了走出现在的困境，我制定以下计划：</p><h3 id="效率不高"><a href="#效率不高" class="headerlink" title="效率不高"></a>效率不高</h3><p>工作的时候，先摆脱手机，把手机放的离自己远一点；电脑尽量不去找壁纸，不去刷网页，不被广告所困扰。尝试全神贯注，集中注意力去做一件事。</p><h3 id="半途而废"><a href="#半途而废" class="headerlink" title="半途而废"></a>半途而废</h3><p>对于专业技能，根据学习路线，一段时间内就攻克一个东西，不要着急去学完，重要的是学精，学会，学完代表不了什么，能学会是最重要的，每一个知识点都如此。</p><p>然后不去并行的很多知识一起学习，这样会很容易放弃。一段时间就搞一个。</p><h3 id="欺骗自己"><a href="#欺骗自己" class="headerlink" title="欺骗自己"></a>欺骗自己</h3><p>这个就不要再假装学习了，到最后受害者是自己啊！直面自己，没什么的。各种技能一定要先确定正确的出发点，</p><p align="center">不要以学完为目的！</p><p align="center">不要以学完为目的！</p><p align="center">不要以学完为目的！</p><p><strong>切记切记</strong>。所有的东西以PS的熟练度为终点。</p><h3 id="时间安排"><a href="#时间安排" class="headerlink" title="时间安排"></a>时间安排</h3><p>bilibili已经有了依赖性，卸载不现实，三国杀准备卸载。</p><p>lol已经戒掉。</p><p>一定不要再找壁纸，找软件了!!! 切记切记！</p><h2 id="未来啊"><a href="#未来啊" class="headerlink" title="未来啊"></a>未来啊</h2><p>以金钱为导向，我选出了两个工作方向：</p><ul><li>CV算法工程师<ul><li>机器学习算法熟悉</li><li>A类论文</li><li>英语基础学习</li></ul></li><li>JAVA开发工程师<ul><li>以现在已经定好的学习路线为准，学习JAVA开发</li></ul></li></ul><p>以后所有的技能都针对于这两个，虽然现在依然喜欢动效，但是同样热爱Java开发。所以以Java为目标！搞起来！</p><p>期待未来的自己能做出改变，哪怕是一点点。</p><hr><h1 id="2020-4-30夜"><a href="#2020-4-30夜" class="headerlink" title="2020/4/30夜"></a>2020/4/30夜</h1></body></html>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 写给自己 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>win10环境配置maskrcnn_benchmark</title>
      <link href="/2020/04/28/win10%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AEmaskrcnn-benchmark/"/>
      <url>/2020/04/28/win10%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AEmaskrcnn-benchmark/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>最近在找实例分割代码，我在win10环境下配置，配置了很久很久，很多坑，记录一下</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p><code>Maskrcnn_Benchmark</code> 是 <code>MaskRCNN</code> 的 <strong>Pytorch</strong> 实现版本，<strong>MaskRCNN</strong>是本身是<strong>Tensorflow</strong>实现,用于实例分割。</p><p>项目地址：<a href="https://github.com/facebookresearch/maskrcnn-benchmark?spm=a2c4e.10696291.0.0.76e419a4t4nrMc" target="_blank" rel="noopener">https://github.com/facebookresearch/maskrcnn-benchmark?spm=a2c4e.10696291.0.0.76e419a4t4nrMc</a></p><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p><strong>win10 Home</strong> + <strong>Anaconda3</strong>  + <strong>CUDA 9.0</strong> + <strong>CUDNN 7.1</strong> + <strong>visualcppbuildtools_full</strong> </p><p>最后一个软件看可以代替 <code>Visual Studio</code></p><p><strong>下载地址</strong>：<a href="https://pan.baidu.com/s/1J0CAz_d9semPyiEWu8nIbg" target="_blank" rel="noopener">https://pan.baidu.com/s/1J0CAz_d9semPyiEWu8nIbg</a>     <strong>提取码</strong>：k490</p><h3 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h3><blockquote><p> 按照官方教程以及一些改动即可。</p></blockquote><p>参照这个博客： <a href="https://www.jianshu.com/p/e9680d0bfa5c" target="_blank" rel="noopener">https://www.jianshu.com/p/e9680d0bfa5c</a></p><p>下述是官方安装教程搬运，具体参照上述博客更好。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">open a cmd and change to desired installation directory</span><br><span class="line">from now on will be refered as INSTALL_DIR</span><br><span class="line">conda create --name maskrcnn_benchmark</span><br><span class="line">conda activate maskrcnn_benchmark</span><br><span class="line"></span><br><span class="line"><span class="comment"># this installs the right pip and dependencies for the fresh python</span></span><br><span class="line">conda install ipython</span><br><span class="line"></span><br><span class="line"><span class="comment"># maskrcnn_benchmark and coco api dependencies</span></span><br><span class="line">pip install ninja yacs cython matplotlib tqdm opencv-python</span><br><span class="line"></span><br><span class="line"><span class="comment"># follow PyTorch installation in https://pytorch.org/get-started/locally/</span></span><br><span class="line"><span class="comment"># we give the instructions for CUDA 9.0</span></span><br><span class="line"><span class="comment">## Important : check the cuda version installed on your computer by running the command in the cmd :</span></span><br><span class="line">nvcc -- version</span><br><span class="line">conda install -c pytorch pytorch-nightly torchvision cudatoolkit=9.0</span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/cocodataset/cocoapi.git</span><br><span class="line"></span><br><span class="line">    <span class="comment">#To prevent installation error do the following after commiting cocooapi :</span></span><br><span class="line">    <span class="comment">#using file explorer  naviagate to cocoapi\PythonAPI\setup.py and change line 14 from:</span></span><br><span class="line">    <span class="comment">#extra_compile_args=['-Wno-cpp', '-Wno-unused-function', '-std=c99'],</span></span><br><span class="line">    <span class="comment">#to</span></span><br><span class="line">    <span class="comment">#extra_compile_args={'gcc': ['/Qstd=c99']},</span></span><br><span class="line">    <span class="comment">#Based on  https://github.com/cocodataset/cocoapi/issues/51</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> cocoapi/PythonAPI</span><br><span class="line">python setup.py build_ext install</span><br><span class="line"></span><br><span class="line"><span class="comment"># navigate back to INSTALL_DIR</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"><span class="comment"># install apex</span></span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/NVIDIA/apex.git</span><br><span class="line"><span class="built_in">cd</span> apex</span><br><span class="line">python setup.py install --cuda_ext --cpp_ext</span><br><span class="line"><span class="comment"># navigate back to INSTALL_DIR</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"><span class="comment"># install PyTorch Detection</span></span><br><span class="line"></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/Idolized22/maskrcnn-benchmark.git</span><br><span class="line"><span class="built_in">cd</span> maskrcnn-benchmark</span><br><span class="line"></span><br><span class="line"><span class="comment"># the following will install the lib with</span></span><br><span class="line"><span class="comment"># symbolic links, so that you can modify</span></span><br><span class="line"><span class="comment"># the files if you want and won't need to</span></span><br><span class="line"><span class="comment"># re-build it</span></span><br><span class="line">python setup.py build develop</span><br></pre></td></tr></tbody></table></figure></div><h3 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h3><p>我卡了三天，各种安装卸载<code>Visual Studio</code>,总结以下问题：</p><ul><li><p>可以不需要<code>Visual Studio</code><strong>！！！</strong></p></li><li><p>pytorch1.0中，torchvision安装错误</p><blockquote><p>出问题先<code>import torch</code> <code>import torchvision</code> 看是否出错，我的<code>torchvision</code>是<code>pillow库</code>出错，换了个版本的pillow库解决的。</p></blockquote></li><li><p>编译时找不到文件</p><blockquote><p>上述提到到编译软件需要添加环境变量</p></blockquote></li></ul><ul><li><p>CUDA编译错误</p><blockquote><p>CUDA文件需要按照上述安装步骤中做相应改动，我文件改错了所以导致cuda一直出错</p></blockquote></li><li><p>Link Error 链接错误</p><blockquote><p>直接百度错误，复制两个文件到一个目录即可</p></blockquote></li></ul><h3 id="Demo运行"><a href="#Demo运行" class="headerlink" title="Demo运行"></a>Demo运行</h3><p><code>Jupyter Notebook</code> 程序我这边没跑起来，但是摄像头demo、自己的本地demo跑起来了，<code>Jupyter Notebook</code>  demo跑不起来可以用以下的代码代替：运行直接 <code>python demo.py</code> .</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!--*-- coding:utf-8 --*--</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.pylab <span class="keyword">as</span> pylab</span><br><span class="line"> </span><br><span class="line"><span class="comment">#import requests</span></span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line">pylab.rcParams[<span class="string">'figure.figsize'</span>] = <span class="number">20</span>, <span class="number">12</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">from</span> maskrcnn_benchmark.config <span class="keyword">import</span> cfg</span><br><span class="line"><span class="keyword">from</span> predictor <span class="keyword">import</span> COCODemo</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 参数配置文件</span></span><br><span class="line">config_file = <span class="string">"../configs/caffe2/e2e_mask_rcnn_R_50_FPN_1x_caffe2.yaml"</span></span><br><span class="line"> </span><br><span class="line">cfg.merge_from_file(config_file)</span><br><span class="line">cfg.merge_from_list([<span class="string">"MODEL.DEVICE"</span>, <span class="string">"cpu"</span>])</span><br><span class="line"><span class="comment"># cfg.MODEL.WEIGHT = '../pretrained/e2e_mask_rcnn_R_50_FPN_1x.pth'</span></span><br><span class="line"> </span><br><span class="line">coco_demo = COCODemo(cfg, min_image_size=<span class="number">800</span>, confidence_threshold=<span class="number">0.7</span>, )</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">imgfile = <span class="string">'../images/1.jpg'</span></span><br><span class="line">pil_image = Image.open(imgfile).convert(<span class="string">"RGB"</span>)</span><br><span class="line"> </span><br><span class="line">image = np.array(pil_image)[:, :, [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># forward predict</span></span><br><span class="line">predictions = coco_demo.run_on_opencv_image(image)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># vis</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.imshow(image[:,:,::<span class="number">-1</span>])</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line"> </span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.imshow(predictions[:,:,::<span class="number">-1</span>])</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure></div><h3 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h3><p><a href="https://user-images.githubusercontent.com/60562661/80507700-b5280a80-89a9-11ea-9c85-aefe6cac61fb.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:150%;" data-src="https://user-images.githubusercontent.com/60562661/80507700-b5280a80-89a9-11ea-9c85-aefe6cac61fb.png" class="lazyload"></a></p><p><a href="https://user-images.githubusercontent.com/60562661/80507708-b6f1ce00-89a9-11ea-9b0b-df448d01b9b0.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/80507708-b6f1ce00-89a9-11ea-9b0b-df448d01b9b0.png" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 软件环境配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Maskrcnn_benchmark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN Receptive Field(感受野)</title>
      <link href="/2020/04/20/CNN-Receptive-Field-%E6%84%9F%E5%8F%97%E9%87%8E/"/>
      <url>/2020/04/20/CNN-Receptive-Field-%E6%84%9F%E5%8F%97%E9%87%8E/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>最近阅读文献碰到了感受野，但是详细的一直没有深入理解，这里记录一下。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>感受野用来表示网络内部的不同神经元对原图像的感受范围的大小。换句话说，<strong>就是Feature Map一个像素映射到原始图像上所对应的范围。</strong></p><ul><li>神经元感受野的值越大表示其能接触到的原始图像范围就越大，也意味着它可能蕴含更为全局，语义层次更高的特征；</li><li>神经元感受野的值越小则表示其所包含的特征越趋向局部和细节。因此感受野的值可以用来大致判断每一层的抽象层次。</li></ul><p>对应到姿态估计就可以利用感受野来控制不同分辨率的特征。</p><h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><blockquote><p>感受野可以通过下图更形象的理解。</p></blockquote><p><a href="https://user-images.githubusercontent.com/60562661/79768730-c34caa00-835d-11ea-8f23-a2b8a00fdd67.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/79768730-c34caa00-835d-11ea-8f23-a2b8a00fdd67.jpg" class="lazyload"></a></p><ul><li>Raw Image是原始图像；</li><li>用<code>kerneal_size = 3*3</code>，<code>stride = 2</code>的卷积核扫过得到conv1，故conv1中一个像素映射到原始图像是<code>3*3</code>；</li><li>conv2中一个像素对应conv1<code>2*2</code>，映射回原始图像为<code>5*5</code></li></ul><p><a href="https://user-images.githubusercontent.com/60562661/79768735-c5af0400-835d-11ea-879e-a7f84ae5f22a.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/79768735-c5af0400-835d-11ea-879e-a7f84ae5f22a.jpg" class="lazyload"></a></p><p>上图则更加直观的看出了感受野的大小变花，感受野与卷积核尺寸、步长都有关系。</p><p>下面给出计算公式：</p><script type="math/tex; mode=display">r_n = r_{n-1} + (k_n - 1)\prod_{i=1}^{n-1}S_i</script><p>其中，rn表示第n层的接受野；Si表示第i层的步长</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Receptive Field </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从 HMM隐马尔可夫模型 到 CRF条件随机域</title>
      <link href="/2020/04/18/HMM%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B-%E5%88%B0-CRF%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9F%9F/"/>
      <url>/2020/04/18/HMM%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B-%E5%88%B0-CRF%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9F%9F/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>李宏毅老师课程：<a href="https://www.bilibili.com/video/av82565851/" target="_blank" rel="noopener">https://www.bilibili.com/video/av82565851/</a></p><p>知乎文章：<a href="https://zhuanlan.zhihu.com/p/70067113" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/70067113</a></p><h2 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h2><h3 id="整体分析"><a href="#整体分析" class="headerlink" title="整体分析"></a>整体分析</h3><p>以词性标注为例，从问题入手，输入<strong>X（word）</strong>，输出<strong>Y(tag)</strong> ，这里的词性Y是<code>Hidden</code>隐藏的,X是可观测到的<code>Observed</code>。</p><p>隐马尔可夫链模型（HMM），着手于建模<code>P(X,Y)</code>即两个序列的联合概率（可以看出来是生成式模型），有条件概率转为联合概率过程如下：</p><script type="math/tex; mode=display">y = argmax_yP(y|x) \\ \space= argmax_y{\frac {P(x,y)}{P(x)}} \\  \space= argmax_yP(x,y)</script><p>目前为题转化为 <strong>穷举所有的y，使得<code>P(x,y)</code>最大。</strong></p><p>这里直接计算复杂度会很大 ，所以采用<code>维特比算法</code> 可以求解出这个问题，并且复杂度不是很高。</p><hr><h3 id="层层深入"><a href="#层层深入" class="headerlink" title="层层深入"></a>层层深入</h3><p>这里的P(x,y)如何求解呢？</p><script type="math/tex; mode=display">P(x,y) = P(x|y)*P(y) \\\space =\prod_{i=1}^NP(x_i|y_i)*\prod_{i=1}^{N}P(y_i|y_{i-1})</script><p>其中，</p><ul><li><p>第一项条件概率称为<strong>发射概率</strong>，指从一个词性中抽到某个单词的概率，它基于<strong>观测独立性假设</strong>，即假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关。</p><p><a href="https://user-images.githubusercontent.com/60562661/79639405-33242e80-81be-11ea-8a66-65045d0690a9.png" data-fancybox="group" data-caption="1587209079787" class="fancybox"><img alt="1587209079787" title="1587209079787" data-src="https://user-images.githubusercontent.com/60562661/79639405-33242e80-81be-11ea-8a66-65045d0690a9.png" class="lazyload"></a></p></li><li><p>第二项则是由<strong><a href="https://huaqi.blue/2020/04/18/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE-Markov-Chain/" target="_blank" rel="noopener">马尔可夫假设</a></strong>得到，如下图：</p></li></ul><p><a href="https://user-images.githubusercontent.com/60562661/79639402-315a6b00-81be-11ea-9ec9-7915395dc030.png" data-fancybox="group" data-caption="1587209050077" class="fancybox"><img alt="1587209050077" title="1587209050077" data-src="https://user-images.githubusercontent.com/60562661/79639402-315a6b00-81be-11ea-9ec9-7915395dc030.png" class="lazyload"></a></p><p>即马尔可夫最终学习的就是<code>发射概率矩阵(x*y)</code>和·<code>转移概率矩阵(y*y)</code>,对应起来就是 当前词性产生下一个词性的概率；从当前词性中选出某个词的概率。</p><hr><h3 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h3><p>求解时，目标是：</p><script type="math/tex; mode=display">P(x,\hat y) > P(x,y)</script><p>即真实概率大于预测概率，但是HMM并不会保证这一点。</p><p>故HMM存在的一个缺陷是会<code>脑补很多东西</code>，具体可以看李宏毅老师视频。脑部在数据不充分时比较好，但是数据量大依然会凭空捏造，这样效果就不太好。</p><h2 id="Condition-Random-Field-条件随机域"><a href="#Condition-Random-Field-条件随机域" class="headerlink" title="Condition Random Field 条件随机域"></a>Condition Random Field 条件随机域</h2><h3 id="由HMM到CRF"><a href="#由HMM到CRF" class="headerlink" title="由HMM到CRF"></a>由HMM到CRF</h3><p>在CRF中，</p><script type="math/tex; mode=display">P(x,y) \approx \exp(W·\phi(x,y))</script><p>即这两项成正比，其中：</p><ul><li><strong>W</strong>是权重向量，从训练数据中学得</li><li>Φ(x,y) 是一个特征向量</li><li>exp()由于不能保证小于1，故是正比于</li></ul><p>依然从问题入手，输入X输出Y，求最大化的P(Y|X)：</p><script type="math/tex; mode=display">P(y|x) = \frac {P(x,y)}{P(x)} = \frac {P(x,y)}{\sum_{y'}P(x,y')}\\</script><p>由上述两者成正比可以推导得：</p><script type="math/tex; mode=display">P(x,y) = \frac{\exp(W·\phi(x,y))}{R}</script><p>故：</p><script type="math/tex; mode=display">P(y|x) =\frac {P(x,y)}{\sum_{y'}P(x,y')}\\=\frac{\exp(W·\phi(x,y))}{R} * \frac{R}{\sum_{y'}\exp(W·\phi(x,y'))}= \frac{\exp(W·\phi(x,y))}{\sum_{y'}\exp(W·\phi(x,y'))}</script><p><strong>现在问题来了，为什么P(x,y)可以写成后面那一项？······具体推导省略，过程可以看李宏毅老师视频</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/79639406-34555b80-81be-11ea-9d97-8e825f62671e.png" data-fancybox="group" data-caption="1587214233709" class="fancybox"><img alt="1587214233709" title="1587214233709" data-src="https://user-images.githubusercontent.com/60562661/79639406-34555b80-81be-11ea-9d97-8e825f62671e.png" class="lazyload"></a></p><p>其中，</p><ul><li>权重向量<strong>W</strong>可以对应为HMM中计算的概率取log</li><li>Ns,t(x,y)则是这一对出现的次数，记作Φ(z,y)</li></ul><p>Φ(x,y)特征向量由两部分组成，<code>一部分是词性产生词性的pair的次数，另一部分是词性中产生相应词汇的次数，</code>形状如下：</p><p><a href="https://user-images.githubusercontent.com/60562661/79639408-361f1f00-81be-11ea-9d91-d5b9fbd6052a.png" data-fancybox="group" data-caption="1587214519278" class="fancybox"><img alt="1587214519278" style="zoom: 80%;" title="1587214519278" data-src="https://user-images.githubusercontent.com/60562661/79639408-361f1f00-81be-11ea-9d91-d5b9fbd6052a.png" class="lazyload"></a></p><p><a href="https://user-images.githubusercontent.com/60562661/79639411-37504c00-81be-11ea-910c-db843615e2ea.png" data-fancybox="group" data-caption="1587214625067" class="fancybox"><img alt="1587214625067" style="zoom: 80%;" title="1587214625067" data-src="https://user-images.githubusercontent.com/60562661/79639411-37504c00-81be-11ea-910c-db843615e2ea.png" class="lazyload"></a></p><hr><h3 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h3><p>上述特征向量<strong>Φ可以自定义</strong>，故要求的参数即为<strong>权重w</strong>，即：</p><script type="math/tex; mode=display">W^* = \arg max_wO(w)</script><p>其中，对象方程为：</p><script type="math/tex; mode=display">O(w) = \sum_{n=1}^N\log P(\hat y_n|x_n)</script><p>其中：</p><script type="math/tex; mode=display">\log P(\hat y_n|x_n) = \log P(x_n,\hat y_n) - \log {\sum_{y'}P(x_n,y')}</script><p>所以目标就是最大化上述公式的前一项，最小化上述公式后一项。而前一项是观测出现的概率，后一项是没有观测到即随机组合出现的概率，这在直观上也是正确的。</p><p>然后采用梯度下降法来求解：</p><p><a href="https://user-images.githubusercontent.com/60562661/79639412-38817900-81be-11ea-831d-847fd366c8f3.png" data-fancybox="group" data-caption="1587215759813" class="fancybox"><img alt="1587215759813" title="1587215759813" data-src="https://user-images.githubusercontent.com/60562661/79639412-38817900-81be-11ea-831d-847fd366c8f3.png" class="lazyload"></a></p><p>即：</p><script type="math/tex; mode=display">\Delta O(W) = \phi(x_n,y_n) - \sum_{y'}P(y'|x_n)\phi(x_n,y')</script><p>由于是最大化所以采用 <strong>Stochastic Gradient Ascent</strong> 更新参数：</p><script type="math/tex; mode=display">w = w + \eta(\phi(x_n,y_n) - \sum_{y'}P(y'|x_n)\phi(x_n,y'))</script><p>在Inference时，</p><script type="math/tex; mode=display">y = argmax_yP(y|x) \\ \space= \arg max_y{\frac {P(x,y)}{P(x)}} \\  \space= \arg max_yP(x,y)\\ \space= \arg max_y\exp(W·\phi(x,y))= \arg max_yW·\phi(x,y)</script><h2 id="CRF与HMM"><a href="#CRF与HMM" class="headerlink" title="CRF与HMM"></a>CRF与HMM</h2><p>CRF在增加标签对出现的概率的同时，<strong>也减小了其他随意组合的对出现的概率</strong>，这就抑制了HMM容易脑补的问题。</p><p>HMM是生成式模型，最终在求解联合概率；</p><p>CRF实际上是判别式模型，从Inference可以看出，是直接求出了条件概率。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HMM </tag>
            
            <tag> CRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习中判别式模型和生成式模型</title>
      <link href="/2020/04/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/"/>
      <url>/2020/04/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>参考：<a href="https://www.cnblogs.com/nolonely/p/6435213.html" target="_blank" rel="noopener">https://www.cnblogs.com/nolonely/p/6435213.html</a></p><h3 id="判别式模型"><a href="#判别式模型" class="headerlink" title="判别式模型"></a>判别式模型</h3><p>直接对条件概率<code>P(Y|X)</code>进行建模，常见的有线性回归等。可以认为是找出了判别的边界。</p><p><strong>举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。</strong></p><hr><h3 id="生成式模型"><a href="#生成式模型" class="headerlink" title="生成式模型"></a>生成式模型</h3><p>对x和y的联合分布p(x,y)建模，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi</p><p><strong>举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/79593553-3829a500-810e-11ea-915b-7ee70ebbb22f.jpg" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://user-images.githubusercontent.com/60562661/79593553-3829a500-810e-11ea-915b-7ee70ebbb22f.jpg" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 建模方式 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>马尔可夫链 Markov Chain</title>
      <link href="/2020/04/18/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE-Markov-Chain/"/>
      <url>/2020/04/18/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE-Markov-Chain/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>首先推荐一篇文章：<a href="https://www.zhihu.com/question/26665048/answer/157852228" target="_blank" rel="noopener">https://www.zhihu.com/question/26665048/answer/157852228</a></p><h2 id="Basic-Theory"><a href="#Basic-Theory" class="headerlink" title="Basic Theory"></a>Basic Theory</h2><p>马尔可夫是链<strong>随机过程中的一个过程</strong>。随机，就表示状态是不确定的，例如交通红绿灯就不适合用马尔可夫模型来计算。但是对于天气系统预测，马尔可夫可以用来建模、分析不同时间的天气状态，即使产生的结果不会完全准确我们也会接受这种假设。</p><p>一个马尔科夫过程是状态间的转移仅依赖于前n个状态的过程。这个过程被称之为<code>n阶马尔科夫模型</code>，其中n是影响下一个状态选择的（前）n个状态。</p><p>最简单的马尔科夫过程是一阶模型，它的状态选择仅与前一个状态有关，本文也着重讲解一阶马尔科夫链。</p><h2 id="Calculate"><a href="#Calculate" class="headerlink" title="Calculate"></a>Calculate</h2><p>对于有<code>M</code>个状态的一阶马尔可夫模型，共有<code>M*M</code> 个状态转移，<strong>因为任何一个状态都有可能是所有状态的下一个转移状态。</strong></p><p>每一个状态转移都有一个概率值，称为<strong>状态转移概率</strong>——这是从一个状态转移到另一个状态的概率。所有的<code>M*M</code>个概率可以用一个状态转移矩阵表示。<strong>注意这些概率并不随时间变化而不同</strong>——这是一个非常重要（但常常不符合实际）的假设。转移概率矩阵一个示意图如下图，有三种状态，所以有九种状态转移概率：</p><p><a href="https://user-images.githubusercontent.com/60562661/79635819-c2711800-81a5-11ea-9e9d-82a1f3d69285.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/79635819-c2711800-81a5-11ea-9e9d-82a1f3d69285.jpg" class="lazyload"></a></p><p>计算时用<code>当前状态 * 转移概率矩阵（矩阵乘法）</code>,来从当前时刻状态计算出下一时刻状态。</p><p>简单来说，一阶马尔可夫模型就是对联合概率做了简化。联合概率中：</p><script type="math/tex; mode=display">P(x_1,x_2,x_3,···，x_n) = P(x_1)*P(x_2|x_1)*P(x_3|x_1,x_2)*···*P(x_n|x_1,x_2,···x_{n-1})</script><p>按时序信息理解即xn时刻的状态与前面所有的时刻都有关，而马尔科夫链则假设每一时刻状态仅仅与前一个时刻状态有关，即：</p><script type="math/tex; mode=display">P(X) = \prod_{i=1}^n  P(x_i|x_{x-1})</script><h2 id="Summarize"><a href="#Summarize" class="headerlink" title="Summarize"></a>Summarize</h2><ul><li>马尔可夫是一个随机过程</li><li>假设当前状态仅仅与前一时间状态有关</li><li>假设有着唯一的转移概率矩阵</li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Markov Chain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>联合概率</title>
      <link href="/2020/04/18/%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87/"/>
      <url>/2020/04/18/%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>推荐一篇知乎文章：<a href="https://zhuanlan.zhihu.com/p/53005534" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/53005534</a></p><h2 id="联合概率"><a href="#联合概率" class="headerlink" title="联合概率"></a>联合概率</h2><p>联合概率P(x,y)表示同时满足x和y的概率，公式如下：</p><script type="math/tex; mode=display">P(x,y) = p(x)*p(y|x)</script><p>对于多变量的，</p><script type="math/tex; mode=display">P(x,y,z) = P(x,y)*P(z|x,y) = P(x)*P(y|x)*p(z|x,y)</script><p>同理，</p><script type="math/tex; mode=display">P(x_1,x_2,x_3,···，x_n) = P(x_1)*P(x_2|x_1)*P(x_3|x_1,x_2)*···*P(x_n|x_1,x_2,···x_{n-1})</script><h2 id="联合概率和条件概率"><a href="#联合概率和条件概率" class="headerlink" title="联合概率和条件概率"></a>联合概率和条件概率</h2><p>联合概率<code>P(x,y)</code>,x和y是对等的，没有相互依赖，可以理解为它的范围是<code>x的所有取值和y的所有取值</code>；</p><p>条件概率<code>p(x|y)</code> 则是以y为条件，是<code>限定在y的范围内，在寻找对应的x</code>，它的范围更小</p></body></html>]]></content>
      
      
      <categories>
          
          <category> Math </category>
          
          <category> 概率论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> joint probability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>牛顿法</title>
      <link href="/2020/04/08/%E7%89%9B%E9%A1%BF%E6%B3%95/"/>
      <url>/2020/04/08/%E7%89%9B%E9%A1%BF%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>牛顿法一般有两个用途：方程求根；最优化。</p><h2 id="求根问题"><a href="#求根问题" class="headerlink" title="求根问题"></a>求根问题</h2><p>对于高次方程，求根公式要么没有要么就很复杂，此时可以用牛顿法来求解。首先根据泰勒公式，把f(x)在x0出展开到一阶（<strong>随机取一个x0求f(x)</strong>）：</p><script type="math/tex; mode=display">f(x) = f(x_0) + f'(x_0)(x-x_0)</script><p>令f(x) = 0得：</p><script type="math/tex; mode=display">x = x_0 - \frac{f(x_0)}{f'(x_0)} = x_1</script><p>由于f(x)用泰勒公式展开到一阶，严格意义上展开式并不相等，只是可以说<strong>f(x_1)比f(x_0)更接近0</strong> 而已，由此迭代便自然而然了:</p><script type="math/tex; mode=display">x_{n+1} = x_n - f(x_n) / f'(x_n)</script><p>通过迭代，必然可以在<code>f(x*) = 0</code> ,x=x*时收敛。也就是求出了根。</p><p>把上面得泰勒公式展开求解迭代过程在几何表示如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/78808595-3ff39600-79f8-11ea-900e-3b95acbc690a.jpg" data-fancybox="group" data-caption="img" class="fancybox"><img alt="img" title="img" data-src="https://user-images.githubusercontent.com/60562661/78808595-3ff39600-79f8-11ea-900e-3b95acbc690a.jpg" class="lazyload"></a></p><p>这也是牛顿法的基本原理。</p><h2 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h2><h3 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h3><p><strong>海塞矩阵</strong>：是一个由多变量<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E5%AE%9E%E5%87%BD%E6%95%B0">实值函数</a>的所有二阶<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E5%81%8F%E5%AF%BC%E6%95%B0">偏导数</a>组成的<a href="https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%E6%96%B9%E5%9D%97%E7%9F%A9%E9%98%B5">方块矩阵</a>。</p><p>使用下标记号表示为：</p><script type="math/tex; mode=display">H_{ij} = \frac{\partial^2f}{\partial_{xi}\partial_{xj}}</script><p><strong>泰勒展开与海塞矩阵：</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/78808590-3d913c00-79f8-11ea-955a-1a0e95dee6da.png" data-fancybox="group" data-caption="1586361820642" class="fancybox"><img alt="1586361820642" title="1586361820642" data-src="https://user-images.githubusercontent.com/60562661/78808590-3d913c00-79f8-11ea-955a-1a0e95dee6da.png" class="lazyload"></a></p><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p><a href="https://user-images.githubusercontent.com/60562661/78909188-b5716c00-7ab5-11ea-9b98-5c6f675253e8.png" data-fancybox="group" data-caption="1586361971718" class="fancybox"><img alt="1586361971718" title="1586361971718" data-src="https://user-images.githubusercontent.com/60562661/78909188-b5716c00-7ab5-11ea-9b98-5c6f675253e8.png" class="lazyload"></a></p><p>这就是牛顿法更新的公式。此时下降最快的方向就是 <code>海塞矩阵逆矩阵 * 梯度</code></p><h2 id="思考梯度下降法与牛顿法"><a href="#思考梯度下降法与牛顿法" class="headerlink" title="思考梯度下降法与牛顿法"></a>思考梯度下降法与牛顿法</h2><h3 id="收敛情况"><a href="#收敛情况" class="headerlink" title="收敛情况"></a>收敛情况</h3><p>由求根迭代可以看出，牛顿法显然收敛速度比较快。</p><ul><li>牛顿法采用二阶海塞矩阵逆矩阵求解</li><li>梯度下降法采用梯度求解</li></ul><p>通俗来说，二阶比一阶收敛更快，因为采用二阶逆矩阵求解，不仅考虑了梯度，也考虑了下一步的梯度，看得更远，所以收敛更快。<strong>（来自知乎）</strong></p><h3 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h3><p>牛顿法既然比梯度下降法收敛快，那么为什么在深度学习中并未广泛应用，而是梯度下降法用的更多？原因大致如下：</p><ul><li>海塞矩阵难以求解，矩阵运算复杂度过高；</li><li>我认为比较可接受得原因是<strong>牛顿法不具有普适性</strong>， 用自己得应用范围，而深度学习数据量大且复杂，则更应该采用普适性方法</li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> Math </category>
          
          <category> 优化算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 牛顿法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>再探梯度下降法之梯度</title>
      <link href="/2020/04/08/%E5%86%8D%E6%8E%A2%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E4%B9%8B%E6%A2%AF%E5%BA%A6/"/>
      <url>/2020/04/08/%E5%86%8D%E6%8E%A2%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E4%B9%8B%E6%A2%AF%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>这里记录一下刚开始学比较疑惑的一个问题，就是梯度。</p><p>推荐一篇文章：<a href="https://www.zhihu.com/question/36301367/answer/142096153" target="_blank" rel="noopener">https://www.zhihu.com/question/36301367/answer/142096153</a></p><h2 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h2><p>梯度的来由这里就不细讲了，可以参考上述知乎文章。</p><p>首先梯度是一个向量，那么就有方向。梯度的方向就是函数变化最快的方向。</p><p><strong>函数变化最快的方向</strong>直观理解，就是随着自变量变化函数值变化最快。即：</p><p><code>w_new = w + w_deta</code>，此时函数值可能变大也可能变小，但是变化值不确定，当 w = w+g时，函数值就会增加最快。</p><hr><p><a href="https://user-images.githubusercontent.com/60562661/78803330-737ef200-79f1-11ea-89eb-41b51f5677d6.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/78803330-737ef200-79f1-11ea-89eb-41b51f5677d6.jpg" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 梯度下降法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最小二乘法和梯度下降法</title>
      <link href="/2020/04/07/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"/>
      <url>/2020/04/07/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>最小二乘法是用来最小化误差函数从而寻找函数得最佳参数的一种方法，学习的时候最小二乘法与梯度下降法我感觉很类似，所以来记录一下。</p><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/109986821" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/109986821</a></p><h2 id="最小二乘法推导"><a href="#最小二乘法推导" class="headerlink" title="最小二乘法推导"></a>最小二乘法推导</h2><p>首先，<strong>一元线性回归</strong>的最小二乘法推导比较容易，上述知乎也有链接，这里直接给出结论：</p><script type="math/tex; mode=display">y = Wx + b \\x = {x_1,x_2,...x_m}\\</script><script type="math/tex; mode=display">\begin{cases}W = \frac{\sum_{i=1}^m(x_i-\bar{x})*(y_i-\bar{y})}{\sum_{i=1}^m(x_i-\bar{x})^2}\\b = \bar{y} - W*\bar{x}\end{cases}</script><p>对于<strong>多元线性回归</strong>，采用矩阵求解：同样直接给出结论</p><script type="math/tex; mode=display">y = X\beta</script><p><a href="https://user-images.githubusercontent.com/60562661/78578352-fbce8d00-7861-11ea-81fd-9b29e9efec21.png" data-fancybox="group" data-caption="1586188084270" class="fancybox"><img alt="1586188084270" style="zoom:80%;" title="1586188084270" data-src="https://user-images.githubusercontent.com/60562661/78578352-fbce8d00-7861-11ea-81fd-9b29e9efec21.png" class="lazyload"></a></p><p><a href="https://user-images.githubusercontent.com/60562661/78578356-fd985080-7861-11ea-84e4-d520251026bb.png" data-fancybox="group" data-caption="1586188119187" class="fancybox"><img alt="1586188119187" style="zoom:80%;" title="1586188119187" data-src="https://user-images.githubusercontent.com/60562661/78578356-fd985080-7861-11ea-84e4-d520251026bb.png" class="lazyload"></a></p><h2 id="两者区别"><a href="#两者区别" class="headerlink" title="两者区别"></a>两者区别</h2><p>由上述可以看出，</p><p><strong>最小二乘法：</strong></p><ul><li>可以直接求矩阵求逆一步到位，求出所的参数，求的全局最优解，无需迭代；</li><li>但是只适用于线性回归模型，并且矩阵复杂时计算复杂度高</li></ul><p><strong>梯度下降法：</strong></p><ul><li>需要设置学习率进行一步步迭代，求得局部最小值或者是全局最优解</li><li>适用于任何模型，只要是凸函数均可以求求出解</li></ul><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>这里想起来分类为什么不适合用最小二乘法求解？</p><p><strong>最小二乘法<code>(y-y-true)^2</code>有个平方，对于离群的点平方会很致命，尽量拟合数据就会导致错误出现</strong></p><hr><p><strong>2020年4月7日夜</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 最小二乘法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学中的李群初探</title>
      <link href="/2020/04/01/%E6%95%B0%E5%AD%A6%E4%B8%AD%E7%9A%84%E6%9D%8E%E7%BE%A4%E5%88%9D%E6%8E%A2/"/>
      <url>/2020/04/01/%E6%95%B0%E5%AD%A6%E4%B8%AD%E7%9A%84%E6%9D%8E%E7%BE%A4%E5%88%9D%E6%8E%A2/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="李群"><a href="#李群" class="headerlink" title="李群"></a>李群</h2><h2 id="直观上的李群"><a href="#直观上的李群" class="headerlink" title="直观上的李群"></a>直观上的李群</h2><p>首先李群也是一种降维算法。降维就是为了使用更低维的向量最大程度上表示原来的特征，在机器学习中有很多降维算法：</p><ul><li><code>PCA(Principal Component Analysis)</code>算法处理的是<strong>方形</strong>的矩阵(n*n)；</li><li><p><code>奇异值分解 SVD (Singular Value Decomposition)</code> 可以处理<strong>任意</strong>形状的矩阵(m*n)；</p></li><li><p>李群则可以处理<code>张量</code>,对张量进行降维度。</p></li></ul><h2 id="李群计算流程"><a href="#李群计算流程" class="headerlink" title="李群计算流程"></a>李群计算流程</h2><h3 id="李群运动链模型"><a href="#李群运动链模型" class="headerlink" title="李群运动链模型"></a>李群运动链模型</h3><p><a href="https://user-images.githubusercontent.com/60562661/78159789-a0715900-7475-11ea-8170-80db8de764e7.png" data-fancybox="group" data-caption="1585753553835" class="fancybox"><img alt="1585753553835" title="1585753553835" data-src="https://user-images.githubusercontent.com/60562661/78159789-a0715900-7475-11ea-8170-80db8de764e7.png" class="lazyload"></a></p><p>这里以老鼠为例，假设老鼠身上一共有四根骨头，<strong>AB</strong>,<strong>BC</strong>,<strong>CD</strong>,<strong>DE</strong>，以每根骨头自身作为x轴，单独建立每根骨头的三维坐标系，<strong>C</strong>的坐标在AB骨头坐标系表示就是：</p><script type="math/tex; mode=display">\begin{bmatrix}{q^*}\\{1}\end{bmatrix}\begin{bmatrix}{R}&{t}\\{0}&{1}\\\end{bmatrix} *\begin{bmatrix}{q}\\{1}\end{bmatrix}</script><p>其中，</p><ul><li>q是<strong>C</strong>在BC三维坐标系下的，形状是<strong><code>3*1</code></strong>；</li><li>q^是<strong>C</strong>在AB坐标系下的坐标，形状是   <code>3*1</code></li><li>R是旋转矩阵，形状是   <code>3*3</code></li><li><p>t是平移矩阵，形状是<code>3*1</code></p></li><li><p>中间的矩阵就是变换矩阵，整体维度就是 <code>4*1 = 4*4 * 4*1</code></p></li></ul><p><strong>更一般的坐标变换：</strong></p><script type="math/tex; mode=display">\begin{bmatrix}{x^*}\\{1}\end{bmatrix}\begin{bmatrix}{R_i}&{t_i}\\{0}&{1}\\\end{bmatrix} *\begin{bmatrix}{x}\\{1}\end{bmatrix}=g_i*\begin{bmatrix}{x}\\{1}\end{bmatrix}</script><p><code>gi</code>就是上图中的<strong>g1，g2…….</strong></p><p><strong>姿态变换：</strong></p><p>两种姿态都可以映射到全局的一个三维坐标系上，就可以进行转换。</p><h3 id="旋转矩阵-R"><a href="#旋转矩阵-R" class="headerlink" title="旋转矩阵 R"></a>旋转矩阵 R</h3><p>首先R属于三阶特殊正交阵，(满足6个条件) 而本来的维度是9，所以自由度为3，所以可以用一个向量来代替这个旋转矩阵。</p><p><strong>所以下面的工作就变成了R的降维工作。</strong></p><hr><h4 id="预备知识1：斜对角矩阵"><a href="#预备知识1：斜对角矩阵" class="headerlink" title="预备知识1：斜对角矩阵"></a>预备知识1：斜对角矩阵</h4><script type="math/tex; mode=display">\mu = (\mu_1,\mu_2,\mu_3)</script><script type="math/tex; mode=display">\hat{\mu} =\begin{bmatrix} {0}&{-\mu_3}&{\mu_2}\\{\mu_3}&{0}&{-\mu_1}\\{-\mu_2}&{\mu_1}&{0}\end{bmatrix}</script><ul><li>对于任意的μ，总有唯一的μ^与之对应；</li><li>对于任意的μ^ ，总有唯一的μ与之对应；</li></ul><h4 id="预备知识2-：e-w"><a href="#预备知识2-：e-w" class="headerlink" title="预备知识2 ：e^w ^"></a>预备知识2 ：e^w ^</h4><p><a href="https://user-images.githubusercontent.com/60562661/78159799-a49d7680-7475-11ea-8c58-f2e028894ab3.png" data-fancybox="group" data-caption="1585756692993" class="fancybox"><img alt="1585756692993" title="1585756692993" data-src="https://user-images.githubusercontent.com/60562661/78159799-a49d7680-7475-11ea-8c58-f2e028894ab3.png" class="lazyload"></a></p><hr><h4 id="R的变换"><a href="#R的变换" class="headerlink" title="R的变换"></a>R的变换</h4><p><a href="https://user-images.githubusercontent.com/60562661/78159795-a2d3b300-7475-11ea-9e3b-f06e8f8cae00.png" data-fancybox="group" data-caption="1585755919172" class="fancybox"><img alt="1585755919172" title="1585755919172" data-src="https://user-images.githubusercontent.com/60562661/78159795-a2d3b300-7475-11ea-9e3b-f06e8f8cae00.png" class="lazyload"></a></p><h2 id="直觉理解"><a href="#直觉理解" class="headerlink" title="直觉理解"></a>直觉理解</h2><p><a href="https://user-images.githubusercontent.com/60562661/78159804-a6ffd080-7475-11ea-9526-4575bcd8efae.png" data-fancybox="group" data-caption="1585756802921" class="fancybox"><img alt="1585756802921" title="1585756802921" data-src="https://user-images.githubusercontent.com/60562661/78159804-a6ffd080-7475-11ea-9526-4575bcd8efae.png" class="lazyload"></a></p><ul><li>首先因为旋转矩阵的自由度为3，所以是在3维流形中；</li><li>在流形曲面处的一个很小的切平面就是<strong>李代数</strong>，可以用三维坐标表示处原来在流形中的坐标；</li><li>整个三维流形就是李群</li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> Math </category>
          
          <category> 李群 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 李群 </tag>
            
            <tag> 李代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一套针对人体姿态估计的在视频上的关键点标注流程</title>
      <link href="/2020/04/01/%E4%B8%80%E5%A5%97%E7%94%A8%E4%BA%8E%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A0%87%E6%B3%A8%E6%B5%81%E7%A8%8B/"/>
      <url>/2020/04/01/%E4%B8%80%E5%A5%97%E7%94%A8%E4%BA%8E%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1%E7%9A%84%E5%85%B3%E9%94%AE%E7%82%B9%E6%A0%87%E6%B3%A8%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>最近我们项目需要标注数据集的人体关键点，所以我们开发了一套标注的简单流程，这里来记录一下。</p><p>因为关键点太多，不可能人为的取标注很多东西，所以我们要借助现有的算法先计算在矫正。</p><h2 id="一、标注工具-LabelMe"><a href="#一、标注工具-LabelMe" class="headerlink" title="一、标注工具 LabelMe"></a>一、标注工具 LabelMe</h2><p><strong>项目地址：</strong> <a href="https://github.com/wkentaro/labelme" target="_blank" rel="noopener">https://github.com/wkentaro/labelme</a></p><p><strong>安装：</strong><br>建议在python3环境下运行，python2我自己未测试过，具体的相关的可以到项目地址看</p><p>1.创建虚拟环境</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n labelme python=<span class="number">3.6</span></span><br></pre></td></tr></tbody></table></figure></div><p>2.激活环境</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source activate labelme</span><br></pre></td></tr></tbody></table></figure></div><p>3.安装<code>labelme</code></p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install labelme</span><br></pre></td></tr></tbody></table></figure></div><p><strong>使用：</strong></p><p>激活虚拟环境后，输入 <code>labelme</code> 打开软件</p><h2 id="二、数据标注流程"><a href="#二、数据标注流程" class="headerlink" title="二、数据标注流程"></a>二、数据标注流程</h2><ol><li>用写好的脚本把视频拆分成单帧；</li><li>用<code>OpenPose</code>跑出所有的图片的帧（或者<code>OpenPose</code>直接在视频上跑）,会得到每张图片的关键点的一个<code>json</code>文件；</li><li>用我们写好的脚本把<code>json</code>转换成<code>labelme</code>格式的<code>json</code>;</li><li>用<code>labelme</code>打开文件夹进行标注</li><li>标注完成后在执行一次脚本，补充删除得点</li></ol><p>labelme之后的标注流程可以参考下面的gif动图：</p><p><a href="https://user-images.githubusercontent.com/60562661/78142369-6f862980-745f-11ea-80b3-4118d153b0b7.gif" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/78142369-6f862980-745f-11ea-80b3-4118d153b0b7.gif" class="lazyload"></a></p><hr><p><strong>目前这套流程已经开始测试使用。</strong></p><p>关于上面提到的脚本，我回头都会集成到实用工具类下面，或者会单独一个labelme的项目上传到github。</p><h2 id="三、标注指南"><a href="#三、标注指南" class="headerlink" title="三、标注指南"></a>三、标注指南</h2><h3 id="一、关键点位置"><a href="#一、关键点位置" class="headerlink" title="一、关键点位置"></a>一、关键点位置</h3><p><strong>Body</strong><br><a href="https://user-images.githubusercontent.com/60562661/78350627-14c40d80-75d8-11ea-8169-acee4a9de517.jpg" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:50%;" data-src="https://user-images.githubusercontent.com/60562661/78350627-14c40d80-75d8-11ea-8169-acee4a9de517.jpg" class="lazyload"></a><a href="https://user-images.githubusercontent.com/60562661/78350612-0fff5980-75d8-11ea-87dd-e15f7b781405.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:60%;" data-src="https://user-images.githubusercontent.com/60562661/78350612-0fff5980-75d8-11ea-87dd-e15f7b781405.png" class="lazyload"></a></p><p><strong>Hand</strong></p><ul><li>手的坐标用原来的21个太多了，人也看不清楚，所以选取了其中11个点(手指尖、手心)，需要矫正</li></ul><p><a href="https://user-images.githubusercontent.com/60562661/78350619-1261b380-75d8-11ea-985b-11b8574edfca.png" data-fancybox="group" data-caption="1585815119386" class="fancybox"><img alt="1585815119386" style="zoom:67%;" title="1585815119386" data-src="https://user-images.githubusercontent.com/60562661/78350619-1261b380-75d8-11ea-985b-11b8574edfca.png" class="lazyload"></a><a href="https://user-images.githubusercontent.com/60562661/78350632-155ca400-75d8-11ea-901a-7dae752cb30e.png" data-fancybox="group" data-caption="1585815119386" class="fancybox"><img alt="1585815119386" style="zoom:97%;" title="1585815119386" data-src="https://user-images.githubusercontent.com/60562661/78350632-155ca400-75d8-11ea-901a-7dae752cb30e.png" class="lazyload"></a></p><hr><h3 id="二、标注相关"><a href="#二、标注相关" class="headerlink" title="二、标注相关"></a>二、标注相关</h3><h4 id="1-遮挡问题"><a href="#1-遮挡问题" class="headerlink" title="1.遮挡问题"></a>1.遮挡问题</h4><p>如果遇到手部遮挡，这样人也是标不出来的，所以直接删除被遮挡的手关节，(<strong>身体关节被遮挡、出画后如果检测不出来是(0,0)坐标，这时候就不用管</strong>)一个个点的删除比较慢，如果需要批量删除，可以如下操作：（动图pdf无法播放，会在群里再发一份）</p><p><strong>注意：</strong> 所有的左上角(0,0)的点都可以不用去考虑</p><p><a href="https://user-images.githubusercontent.com/60562661/78350639-17266780-75d8-11ea-96bf-0073a266210f.gif" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom: 67%;" data-src="https://user-images.githubusercontent.com/60562661/78350639-17266780-75d8-11ea-96bf-0073a266210f.gif" class="lazyload"></a></p><p><strong>其中，如果没有polygon Labels这个控制框，可以再view菜单下调出来</strong></p><h4 id="2-运动模糊问题"><a href="#2-运动模糊问题" class="headerlink" title="2.运动模糊问题"></a>2.运动模糊问题</h4><p>碰到人的手很模糊，人眼都很难分辨出来，手的点又比较大，这时候标注会很困难，碰到这种情况，直接保留手的关键点，可以跳过手的标注。例如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/78350623-1392e080-75d8-11ea-806f-060a60f12540.png" data-fancybox="group" data-caption="1585816044541" class="fancybox"><img alt="1585816044541" title="1585816044541" data-src="https://user-images.githubusercontent.com/60562661/78350623-1392e080-75d8-11ea-806f-060a60f12540.png" class="lazyload"></a></p><h2 id="附加：服务器与本地同步"><a href="#附加：服务器与本地同步" class="headerlink" title="附加：服务器与本地同步"></a>附加：服务器与本地同步</h2><p>关于服务器与本地同步很慢的问题，可以在服务器使用百度云即可。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> labelme </tag>
            
            <tag> 关键点标注 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>X-Particles 4.0 粒子</title>
      <link href="/2020/03/29/X-Particles-4-0-%E7%B2%92%E5%AD%90/"/>
      <url>/2020/03/29/X-Particles-4-0-%E7%B2%92%E5%AD%90/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>最近学习到了X-Particles4.0粒子的破解流程，虽然我目前已经不做<code>CG</code>了，但是还是值得记录一下，因为这个插件是我当时<code>交智商税交的最多的</code>一个东西,我记得是花了我<strong>380还是390大洋</strong>!</p><p>这里只是记录一下流程，至于具体的破解文件、工具之类的就不放出来了。</p><p><a href="https://user-images.githubusercontent.com/60562661/77853841-f1384600-7218-11ea-9467-d7e1a97a7191.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/77853841-f1384600-7218-11ea-9467-d7e1a97a7191.png" class="lazyload"></a></p><h2 id="下载插件"><a href="#下载插件" class="headerlink" title="下载插件"></a>下载插件</h2><p>首先下载好<code>X-Particles 7.3.2</code>,放入C4D的插件目录</p><h2 id="破解"><a href="#破解" class="headerlink" title="破解"></a>破解</h2><p>输入用户名、邮箱、<code>序列号</code>， 关于序列号这里照常不写出来。</p><h2 id="定时"><a href="#定时" class="headerlink" title="定时"></a>定时</h2><p>修改C4D时间，定时到某一个点，即可破解成功</p><h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><p>打开C4D 一定要断网打开，然后再联网即可正常使用。</p><h2 id><a href="#" class="headerlink" title></a><a href="https://user-images.githubusercontent.com/60562661/77853838-eda4bf00-7218-11ea-95a6-9bf0f36c459a.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/77853838-eda4bf00-7218-11ea-95a6-9bf0f36c459a.jpg" class="lazyload"></a></h2></body></html>]]></content>
      
      
      <categories>
          
          <category> VFX-视效 </category>
          
          <category> 插件 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> X-Particle4.0 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>直方图均衡化算法</title>
      <link href="/2020/03/26/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96%E7%AE%97%E6%B3%95/"/>
      <url>/2020/03/26/%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>首先推荐一个知乎回答：<a href="https://www.zhihu.com/question/37204742/answer/221844779" target="_blank" rel="noopener">https://www.zhihu.com/question/37204742/answer/221844779</a> 我觉得讲得很不错。</p><h2 id="为什么需要均衡化？"><a href="#为什么需要均衡化？" class="headerlink" title="为什么需要均衡化？"></a>为什么需要均衡化？</h2><p>直方图均衡化就是因为本来的图像可能像素都集中在某一个或者一两个灰度级，其他灰度级都没有像素信息，此时图片呈现就不会很正常。<strong>（这里不能说不好，直方图没有好坏，它只是反映了图像的信息，而图像则可能是艺术效果而故意为之。）</strong>此时就需要所有的灰度级都有信息，因此均衡化就可以认为是把原来的直方图做了拉伸。</p><p><a href="https://user-images.githubusercontent.com/60562661/77671749-fef48e00-6fc2-11ea-9a7d-1004370c0b2d.png" data-fancybox="group" data-caption="1585238407011" class="fancybox"><img alt="1585238407011" title="1585238407011" data-src="https://user-images.githubusercontent.com/60562661/77671749-fef48e00-6fc2-11ea-9a7d-1004370c0b2d.png" class="lazyload"></a></p><p>而直方图均衡化，也就是增强了图像的对比度，简单的理解就是<strong>偏黑的更黑，偏白的更白</strong>。</p><h2 id="均衡化算法"><a href="#均衡化算法" class="headerlink" title="均衡化算法"></a>均衡化算法</h2><p>下面讨论单色(黑白)图直方图均衡化的一般性算法。</p><p>假设图像 <strong>f,g</strong>,输入图像为<code>f(x,y)</code>,输出为<code>g(x,y)</code>,目标就是用<code>T函数</code>把f变换到g，即<strong>s = T(r)</strong>,g比f更分散。</p><p>设任意灰度值 <strong>t</strong> 在 <strong>f</strong> 中出现的概率为函数    $p_f(t)$,在<strong>g</strong>中概率为：$p_g(t)$, 这两个函数很容易计算，然后定义：</p><script type="math/tex; mode=display">S_f(n) = \int_1^np_f(t)dt</script><p>表示图像f中灰度值小于n的概率</p><script type="math/tex; mode=display">S_g(n) = \int_1^np_g(t)dt</script><p>表示图像g中灰度值小于n的概率</p><p>则可以得出：</p><script type="math/tex; mode=display">S_f(r) = S_g(T(r)) \implies S_f(r) = S_g(s)     \tag{1}</script><p>这个公式很容易理解，举个例子：图像f灰度值小于0.5的像素出现的概率是0.7，映射到图像g后灰度值小于0.7的概率也是0.7。</p><p>对这一式子求微分：</p><script type="math/tex; mode=display">p_f(r)*dr = p_g(s)*ds \tag{2}</script><p>接下来令    $T(r) = L<em>S_f(r)$，即变换公式T = 灰度级数量L </em> f图像中小于灰度r的概率，则：</p><script type="math/tex; mode=display">s = T(r) = L*S_f(r) = L*\int_1^rp_f(t)dt \\\implies \frac{ds}{dr} = L*p_f(r) \tag{3}</script><p>由公式2、3(最后两个公式)得：</p><script type="math/tex; mode=display">p_f(r) = p_g(s)*L*p_f(r) \\\implies p_g(s) = 1/L</script><p>这时候神奇得事情出现了：图像g中各个灰度级像素出现的概率相等，为1/L，也就是说所有灰度级都会出现，有信息。</p><p>总结一下流程：</p><ol><li>计算各个灰度值出现的概率pf；</li><li>用映射函数  $s = T(r) = L<em>S_f(r) = L</em>\int_1^rp_f(t)dt$, 计算所有灰度值的输出即可。</li></ol><p>现实中数字图像的灰度值是<strong>离散</strong>的，同理，把积分改为求和即可。即：</p><script type="math/tex; mode=display">S_f(n) = \sum_{i=1}^np_f(i)</script><script type="math/tex; mode=display">S_g(n) = \sum_{i=1}^np_g(i)</script><script type="math/tex; mode=display">T(r) = L*\sum_{i=1}^rp_f(i)</script><p>现实中由于是离散的，r、s均为整数，所以各个灰度级出现的概率不一定相等。但确定的是，<code>g灰度级信息一定比f更分散了。</code></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数字图像 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像直方图理解</title>
      <link href="/2020/03/26/%E5%9B%BE%E5%83%8F%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%8F%8A%E5%85%B6%E5%9D%87%E8%A1%A1%E5%8C%96%E7%AE%97%E6%B3%95/"/>
      <url>/2020/03/26/%E5%9B%BE%E5%83%8F%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%8F%8A%E5%85%B6%E5%9D%87%E8%A1%A1%E5%8C%96%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="直方图-Histogram"><a href="#直方图-Histogram" class="headerlink" title="直方图 Histogram"></a>直方图 Histogram</h2><p>每张图像都对应有直方图，再Ps软件、单反都可以轻易地看到图像的直方图，直方图则可以看出图像得风格。</p><p><code>图像的直方图代表了不同灰度级的像素出现的次数</code>。</p><p><strong>首先看一下灰度级是什么意思？</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/77671759-0156e800-6fc3-11ea-94f9-ec57ac5df32e.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/77671759-0156e800-6fc3-11ea-94f9-ec57ac5df32e.jpg" class="lazyload"></a></p><p>这张图就清晰的展示了灰度级的概念，通俗来说就是明亮的程度。</p><p><strong>然后看一下直方图长什么样子？</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/77671737-fbf99d80-6fc2-11ea-9930-b89e1408c2a6.png" data-fancybox="group" data-caption="1585236990450" class="fancybox"><img alt="1585236990450" title="1585236990450" data-src="https://user-images.githubusercontent.com/60562661/77671737-fbf99d80-6fc2-11ea-9930-b89e1408c2a6.png" class="lazyload"></a></p><p>这里面对三个颜色通道都画了出来，横坐标代表灰度级，纵坐标代表数量，数量越多图像就越高。</p><p><strong>如何理解直方图？</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/77671928-3bc08500-6fc3-11ea-809f-4313c587180d.png" data-fancybox="group" data-caption="1585237212885" class="fancybox"><img alt="1585237212885" style="zoom:67%;" title="1585237212885" data-src="https://user-images.githubusercontent.com/60562661/77671928-3bc08500-6fc3-11ea-809f-4313c587180d.png" class="lazyload"></a></p><p>如图，再PS的Camera滤镜中把灰度级做了一个区分，它把直方图分成了五类，从左到右分别是：<strong>黑色、阴影、曝光、高光、白色</strong>，鼠标悬浮即会高亮一片区域并且显示该区域代表图像的什么部分。所以就很容易理解直方图了，调整图像的高光等都会使得直方图放生改变。</p><p><code>这里有一个误区是不能把从左到右直方图对应到图像从左到右。</code>因为图像的阴影、高光区域位置都很不规则。如果图像真的从左到右与直方图对应，那么就变成了第一张图这种形式了。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># # @Time    : 2020/3/22 上午 11:33</span></span><br><span class="line"><span class="comment"># # @Author  : fry</span></span><br><span class="line"><span class="comment"># @FileName: 6_2_his_rgb.py</span></span><br><span class="line"><span class="comment"># ------------------------------------------------------------------------------</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image_hist</span><span class="params">(image)</span>:</span> <span class="comment">#画三通道图像的直方图</span></span><br><span class="line">   color = (<span class="string">"blue"</span>, <span class="string">"green"</span>, <span class="string">"red"</span>)<span class="comment">#画笔颜色的值可以为大写或小写或只写首字母或大小写混合</span></span><br><span class="line">   <span class="keyword">for</span> i, color <span class="keyword">in</span> enumerate(color):</span><br><span class="line">       <span class="comment"># 计算直方图函数</span></span><br><span class="line">       <span class="comment"># src-img; channel; mask; bin多少个灰度级; range[0-256],像素值范围； 除了mask其他都要 []</span></span><br><span class="line">       hist = cv2.calcHist([image], [i], <span class="literal">None</span>, [<span class="number">256</span>], [<span class="number">0</span>, <span class="number">256</span>])</span><br><span class="line">       plt.plot(hist, color=color)</span><br><span class="line">       plt.xlim([<span class="number">0</span>, <span class="number">256</span>])</span><br><span class="line">   plt.show()</span><br><span class="line"></span><br><span class="line">image = cv2.imread(<span class="string">'./img/5.jpg'</span>, <span class="number">1</span>)</span><br><span class="line">cv2.namedWindow(<span class="string">"img"</span>,cv2.WINDOW_NORMAL)</span><br><span class="line">cv2.imshow(<span class="string">'img'</span>, image)</span><br><span class="line">image_hist(image)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure></div><p>三通道直方图：</p><p><a href="https://user-images.githubusercontent.com/60562661/77671744-fe5bf780-6fc2-11ea-864f-23d315de5e4c.png" data-fancybox="group" data-caption="1585237602654" class="fancybox"><img alt="1585237602654" style="zoom:67%;" title="1585237602654" data-src="https://user-images.githubusercontent.com/60562661/77671744-fe5bf780-6fc2-11ea-864f-23d315de5e4c.png" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数字图像 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高斯混合模型</title>
      <link href="/2020/03/24/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/"/>
      <url>/2020/03/24/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="Gaussion-Distribution"><a href="#Gaussion-Distribution" class="headerlink" title="Gaussion Distribution"></a>Gaussion Distribution</h2><p>首先复习一下高斯分布：</p><script type="math/tex; mode=display">N(x,\mu,\sigma) = \frac{1}{\sqrt {2\pi\sigma}}\exp(-\frac{(x-\mu)^2}{2\sigma^2})</script><p>用高斯分布估计数据分布是有很大的局限的，它一定对称，只有一个峰值，这些使得它再估计真实数据时表现很乏力，因此引入了高斯混合模型。</p><h2 id="Gaussion-Mixture-Model"><a href="#Gaussion-Mixture-Model" class="headerlink" title="Gaussion Mixture Model"></a>Gaussion Mixture Model</h2><p>顾名思义，高斯混合模型就是很多个高斯模型混合在一起</p><p><a href="https://user-images.githubusercontent.com/60562661/77444240-92dd2300-6e26-11ea-8243-7ddad8a9cd80.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/77444240-92dd2300-6e26-11ea-8243-7ddad8a9cd80.png" class="lazyload"></a></p><p>如图，黑色的分布就是下面彩色高斯分布混合而成。实际上就是很多高斯的加权和：</p><script type="math/tex; mode=display">p(x) = \sum_{k=1}^K W_k*g_k(x|\mu_k,\sigma_k)</script><p>其中，</p><ul><li><p>g_k表示第k个高斯分布，具体公式就是上面列的高斯分布公式；</p></li><li><p>w_k是它的权重系数，因此满足：</p></li></ul><script type="math/tex; mode=display">W_k>0 \space\space\space\space\space\space\space\space\space\space\sum_{k=1}^K W_k = 1</script></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Gaussion Mixture Model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>softmax函数详解</title>
      <link href="/2020/03/23/softmax%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/"/>
      <url>/2020/03/23/softmax%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="softmax-作用"><a href="#softmax-作用" class="headerlink" title="softmax 作用"></a>softmax 作用</h2><p><code>softmax</code> 函数一般用在分类场景，尤其是多分类，它接受一组任意值作为输入，然后输出对应的一组<strong>0-1</strong>之间的数值，并且加起来为1，也就是做了归一化，实际上输出的值代表概率值。</p><h2 id="softmax-计算方式"><a href="#softmax-计算方式" class="headerlink" title="softmax 计算方式"></a>softmax 计算方式</h2><p><a href="https://user-images.githubusercontent.com/60562661/77334644-0e27d180-6d60-11ea-9844-be15700132e4.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/77334644-0e27d180-6d60-11ea-9844-be15700132e4.png" class="lazyload"></a></p><p>如上图，z1,z2,z3 作为输入，分别取<code>exp</code>, 然后加起来作为分母，分别的除以分母就得到对应的值。即zi的softmax值为：</p><script type="math/tex; mode=display">S_{z_i} = \frac{\exp(z_i)}{\sum_{j=1}^3 \exp(z_j)  }</script><p>推广一下：</p><script type="math/tex; mode=display">S_{z_i} = \frac{ \exp(z_i)}{\sum_{j} \exp(z_j)  }</script><p>其中，$\sum<em>{i} S</em>{z_i} = 1$</p><h2 id="再看交叉熵"><a href="#再看交叉熵" class="headerlink" title="再看交叉熵"></a>再看交叉熵</h2><p>对于二分类问题，交叉熵函数为：</p><script type="math/tex; mode=display">L = \hat{y_i}*\ln {y_i} + (1-\hat{y_i})*\ln{(1- y_i)}</script><p>对于多分类问题：</p><script type="math/tex; mode=display">L = -\hat{y}*\ln(y)</script><p>其中，$\hat{y}$表示label，y表示预测的值，由softmax函数计算得到</p><h2 id="softmax求导"><a href="#softmax求导" class="headerlink" title="softmax求导"></a>softmax求导</h2><p>上面已经知道多分类时交叉熵的公式，预测第i个时，认为它的label是1，则：</p><script type="math/tex; mode=display">L_i = -\ln(y_i)</script><p>则：</p><script type="math/tex; mode=display">\frac{\partial L_i}{\partial i} = -\frac{\partial{\ln(y_i)}}{\partial i} = \frac{\partial \ln (\frac{e^i}{\sum_j e^j})}{\partial i} =··· ···=y_i-1</script><p>具体推导如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/77334662-14b64900-6d60-11ea-9b3d-beca92583e36.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:70%;" data-src="https://user-images.githubusercontent.com/60562661/77334662-14b64900-6d60-11ea-9b3d-beca92583e36.png" class="lazyload"></a></p><hr><p>以上便是softmax函数的相关知识了。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> softmax </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch常用的几个操作</title>
      <link href="/2020/03/21/pytorch%E5%B8%B8%E7%94%A8%E7%9A%84%E5%87%A0%E4%B8%AA%E5%87%BD%E6%95%B0/"/>
      <url>/2020/03/21/pytorch%E5%B8%B8%E7%94%A8%E7%9A%84%E5%87%A0%E4%B8%AA%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>目前写的工程遇到这些比较多，后面再丰富。</p><h2 id="Pytorch保存模型、载入模型"><a href="#Pytorch保存模型、载入模型" class="headerlink" title="Pytorch保存模型、载入模型"></a>Pytorch保存模型、载入模型</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save model</span></span><br><span class="line">model = LSTM_Cycle().cuda()</span><br><span class="line">path = <span class="string">'./models_new/lstm_'</span> + <span class="string">'epoch_'</span> + str(epoch) +<span class="string">'Iteration_'</span> + str(i)+<span class="string">'_.pth'</span></span><br><span class="line">torch.save(model.state_dict(), path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load model</span></span><br><span class="line">model = test.demo_model.LSTM_Cycle_Test()</span><br><span class="line">pre_train = torch.load(<span class="string">'./models_correct/lstm_epoch_9Iteration_240_.pth'</span>)</span><br><span class="line">model.load_state_dict(pre_train)</span><br><span class="line">model.cuda()</span><br></pre></td></tr></tbody></table></figure></div><h2 id="查看pytorch张量"><a href="#查看pytorch张量" class="headerlink" title="查看pytorch张量"></a>查看pytorch张量</h2><p>tensor不能直接看，一般会转换为numpy看：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(tensor.cpu().detach().numpy())</span><br></pre></td></tr></tbody></table></figure></div><p>这里有一个技巧就是一般tensor输出中间是省略号，若想看全部的tensor数据，则如下操作：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.set_printoptions(threshold=np.inf)</span><br><span class="line">print(tensor.cpu().detach().numpy())</span><br></pre></td></tr></tbody></table></figure></div><h2 id="忽略pytorch警告"><a href="#忽略pytorch警告" class="headerlink" title="忽略pytorch警告"></a>忽略pytorch警告</h2><p>pytorch版本不同会有很多警告，会比较烦，在代码开头加上如下代码即可忽略：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br></pre></td></tr></tbody></table></figure></div><h2 id="pytorch-指定多GPU-训练"><a href="#pytorch-指定多GPU-训练" class="headerlink" title="pytorch 指定多GPU 训练"></a>pytorch 指定多GPU 训练</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多个gpu</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0,1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 单 gpu</span></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>] = <span class="string">"0"</span></span><br></pre></td></tr></tbody></table></figure></div><h2 id="Pytorch数据标准化"><a href="#Pytorch数据标准化" class="headerlink" title="Pytorch数据标准化"></a>Pytorch数据标准化</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                             std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]),</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">input = transform(input).unsqueeze(<span class="number">0</span>).permute(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>) <span class="comment"># 维度变换</span></span><br></pre></td></tr></tbody></table></figure></div></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像仿射变换后的坐标求解</title>
      <link href="/2020/03/21/%E5%9B%BE%E5%83%8F%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E5%90%8E%E7%9A%84%E5%9D%90%E6%A0%87%E6%B1%82%E8%A7%A3/"/>
      <url>/2020/03/21/%E5%9B%BE%E5%83%8F%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%E5%90%8E%E7%9A%84%E5%9D%90%E6%A0%87%E6%B1%82%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>我是做人体姿态估计的方向，在姿态估计中有必不可少的 一步，就是坐标变换。最直观的从原始不规则图像变换到45<em>45（或者其他）尺寸的<code>heatmap</code>，原始图像的人体关键点的坐标就需要做相应的变换才可以生成正确的新的坐标，从而生成正确的<em>*heatmap</em></em>。</p><p>而这一问题看似很容易，却是在一直困扰着我，我在做的项目里这一问题一直没彻底解决，而今天经过反复测试找到一个比较合适的实现方案，实现由原图变换到heatmap，heatmap解出原坐标。下面进入正文。</p><h2 id="简单的坐标变换"><a href="#简单的坐标变换" class="headerlink" title="简单的坐标变换"></a>简单的坐标变换</h2><p>首先，图像直接resize变换，是可以直接用坐标除以比例得到新的坐标。注意直接用<code>cv2.resize()</code>图像可能会发生形变，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/77230391-cf164680-6bce-11ea-94da-3979ac3883c7.jpg" data-fancybox="group" data-caption="d" class="fancybox"><img alt="d" title="d" data-src="https://user-images.githubusercontent.com/60562661/77230391-cf164680-6bce-11ea-94da-3979ac3883c7.jpg" class="lazyload"></a></p><p>这是我1280<em>720图像直接resize到 368\</em>368的结果，红点是在原始图像画上去的，resize后到了上图中，上图中绿色的圈圈(下面的)则是经过简单的坐标计算：</p><script type="math/tex; mode=display">ratio_{width} = width / 368\\ratio_{height} = height / 368\\x' = x / ratio_{width} \\y' = y / ratio_{height}</script><h2 id="仿射变换"><a href="#仿射变换" class="headerlink" title="仿射变换"></a>仿射变换</h2><p>仿射变换（Affine Transformation或 Affine Map）是二维情况下坐标之间的变换。它保持了二维图形的“平直性”（即：直线经过变换之后依然是直线）和“平行性”（即：二维图形之间的相对位置关系保持不变，平行线依然是平行线，且直线上点的位置顺序不变）。</p><p>数学上，仿射变换就是用2*3的变换矩阵来计算。</p><p>仿射变换可以写为如下的形式：<code>(x,y)为原始坐标 （x',y'）为变换后的坐标</code></p><script type="math/tex; mode=display">\left\{\begin{aligned}x'& = ax + by + m\\y'& = cx + dy + n\end{aligned}\right.</script><p>矩阵计算比较常用，可以写为：</p><script type="math/tex; mode=display">\begin{bmatrix}{u}\\{v}\\\end{bmatrix} = \begin{bmatrix}{a_2}&{a_1}&{a_0}\\{b_2}&{b_1}&{b_0}\\\end{bmatrix} *\begin{bmatrix}{x}\\{y}\\{1}\end{bmatrix}</script><p>也就是简单的新的坐标 = 变换矩阵 * 原始坐标。而在姿态估计中，GT坐标都是三维，第三维为0胡总和1.1表示坐标正确，0表示不可信。因此很容易用仿射变换来求解变换后的坐标。</p><p>仿射变换是一系列原子操作组合而成，平移、缩放、旋转、反转、错切 。这些操作组合可以完成各种变换。</p><h2 id="姿态估计中的坐标变换"><a href="#姿态估计中的坐标变换" class="headerlink" title="姿态估计中的坐标变换"></a>姿态估计中的坐标变换</h2><p>姿态估计中自上而下的方法，需要先从原图把每个人切出来，然后切出来的图片一般要变换成正方形，然后再缩放到对应的<code>heatmap</code>尺寸，一般是(64*64)。所以上面提到的直接求比例是行不通的。</p><p>如果用常规方法去做，会非常麻烦，因为不只是需要<code>heatmap</code>的时候进行变换，再最后预测出heatmap后，依然需要求出坐标而且变换到原图。我之前都是直接做，所以一直出错。而如果借助仿射变换，这件事就很容易了。我借助的别人一个函数，来求仿射矩阵，变换图像；同时由heatmap反求坐标变换到原始图像。代码在我的工具类可以看到。下面看一下我的测试结果：</p><hr><p><strong><1> 原始图像</strong>（忽略上面的蓝色点，是我刚开始求出的错误的）：</p><p><a href="https://user-images.githubusercontent.com/60562661/77230393-d178a080-6bce-11ea-81fe-dec05a0c4d61.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/77230393-d178a080-6bce-11ea-81fe-dec05a0c4d61.jpg" class="lazyload"></a></p><p><strong><2> 裁剪后的图像以及坐标变换</strong>：</p><p><a href="https://user-images.githubusercontent.com/60562661/77230395-d2113700-6bce-11ea-80f6-a92bd30fe478.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/77230395-d2113700-6bce-11ea-80f6-a92bd30fe478.jpg" class="lazyload"></a></p><p>可以看出计算出的坐标是正确的。</p><p><strong><3> 裁剪后的图像关键点生成heatmap再反变换到原始图像：</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/77230396-d2a9cd80-6bce-11ea-869f-9fcc65abbb34.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/77230396-d2a9cd80-6bce-11ea-869f-9fcc65abbb34.jpg" class="lazyload"></a></p><p><strong>可以看出是正确的坐标了。</strong></p><h2 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h2><p>在上面<2>中，手动求坐标就是一开始提到公式，这里有两个坑：</p><ul><li><code>新的坐标 = 变换矩阵 * 原始坐标</code>  , 我之前这一步错在用原始坐标 去乘 变换矩阵，不懂原理嘛。</li><li>这里的乘法是正式的矩阵乘法，用numpy实现就是：<code>np.dot(M,a)</code> , 这样算出来的就是二维坐标，即 (x,y),我之前这里也错了，如果直接用 M <em> a就是对应位置相乘，得到的是 2</em>3形状的矩阵。</li><li>还有就是以前求不对变换矩阵，后面需要就直接调用即可</li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_coordinates = np.dot(trans_final, coordinates)</span><br></pre></td></tr></tbody></table></figure></div><p><strong>有个心得，就是在做姿态估计准备训练模型前，先试几组坐标变换看对不对在做后面的事。</strong></p><p>至此，困扰很久的坐标问题终于得到解决！</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 仿射变换 </tag>
            
            <tag> 坐标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention方法</title>
      <link href="/2020/03/19/Attention%E6%96%B9%E6%B3%95/"/>
      <url>/2020/03/19/Attention%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="Attention背景"><a href="#Attention背景" class="headerlink" title="Attention背景"></a>Attention背景</h2><p>RNN循环神经网络，具有一定的记忆性，但是有一个固有的缺点就是不能并行运算，要算后一个必须就要先算出前一个。显示提出了CNN假设，用CNN替代RNN，此时已经可以并行计算，但是CNN必要有很多层才可以看更长时间的东西。</p><p>因此出现了Attention这种方法，成为RNN的代替方案。</p><p><a href="https://user-images.githubusercontent.com/60562661/77137835-82394f80-6aaa-11ea-8142-cb7137e2d776.png" data-fancybox="group" data-caption="1584675106927" class="fancybox"><img alt="1584675106927" style="zoom:67%;" title="1584675106927" data-src="https://user-images.githubusercontent.com/60562661/77137835-82394f80-6aaa-11ea-8142-cb7137e2d776.png" class="lazyload"></a></p><h2 id="计算流程"><a href="#计算流程" class="headerlink" title="计算流程"></a>计算流程</h2><h3 id="计算出-Q-K-V"><a href="#计算出-Q-K-V" class="headerlink" title="计算出 Q,K,V"></a>计算出 Q,K,V</h3><p>首先输入x计算出a，a分别乘上三个矩阵得到q，k，v。</p><p><a href="https://user-images.githubusercontent.com/60562661/77137839-85344000-6aaa-11ea-9891-181565f8ac57.png" data-fancybox="group" data-caption="1584675959648" class="fancybox"><img alt="1584675959648" title="1584675959648" data-src="https://user-images.githubusercontent.com/60562661/77137839-85344000-6aaa-11ea-9891-181565f8ac57.png" class="lazyload"></a></p><script type="math/tex; mode=display">q_i = W_q *a_i,\space \space k_i = w_k*a_i,\space \space v_i = w_v*a_i</script><ul><li>其中q表示query，查询，也就是用来做匹配；</li><li>k表示key，是被匹配的对象；</li><li>v表示value，是要被抽取出来的信息</li></ul><h3 id="用每个q对k做Attention"><a href="#用每个q对k做Attention" class="headerlink" title="用每个q对k做Attention"></a>用每个q对k做Attention</h3><p><a href="https://user-images.githubusercontent.com/60562661/77137840-85ccd680-6aaa-11ea-8081-dd2b4e99974d.png" data-fancybox="group" data-caption="1584676364224" class="fancybox"><img alt="1584676364224" style="zoom: 50%;" title="1584676364224" data-src="https://user-images.githubusercontent.com/60562661/77137840-85ccd680-6aaa-11ea-8081-dd2b4e99974d.png" class="lazyload"></a></p><script type="math/tex; mode=display">\alpha_{1,i} = q_1 \cdot k_i / \sqrt d</script><p>其中，q和k做内积除以它们的维度。d是q和k的维度。这个注意力计算公式也可以是其他的。</p><p><a href="https://user-images.githubusercontent.com/60562661/77137842-86656d00-6aaa-11ea-9d05-84d9f9eff4ab.png" data-fancybox="group" data-caption="1584676778515" class="fancybox"><img alt="1584676778515" style="zoom:67%;" title="1584676778515" data-src="https://user-images.githubusercontent.com/60562661/77137842-86656d00-6aaa-11ea-9d05-84d9f9eff4ab.png" class="lazyload"></a></p><p>其中<code>α</code>经过<code>softmax</code>层得到<code>α帽</code></p><p><a href="https://user-images.githubusercontent.com/60562661/77137843-87969a00-6aaa-11ea-969f-6ce4068b99d4.png" data-fancybox="group" data-caption="1584676981736" class="fancybox"><img alt="1584676981736" style="zoom:67%;" title="1584676981736" data-src="https://user-images.githubusercontent.com/60562661/77137843-87969a00-6aaa-11ea-969f-6ce4068b99d4.png" class="lazyload"></a></p><p>α帽和相应的v做一个加权和得到b，即：</p><script type="math/tex; mode=display">b_1 =\sum_i \hat a_{1,i} * v_i</script><p>此时，b1就已经考虑了整个的次序。</p><p>b2,b3,b4流程和b1一模一样，即：</p><script type="math/tex; mode=display">b_j =\sum_i \hat a_{j,i} * v_i</script><p>并且是可以并行计算的。</p><h3 id="并行运算细节"><a href="#并行运算细节" class="headerlink" title="并行运算细节"></a>并行运算细节</h3><p><a href="https://user-images.githubusercontent.com/60562661/77137844-882f3080-6aaa-11ea-8749-5761729c56ba.png" data-fancybox="group" data-caption="1584677516392" class="fancybox"><img alt="1584677516392" title="1584677516392" data-src="https://user-images.githubusercontent.com/60562661/77137844-882f3080-6aaa-11ea-8749-5761729c56ba.png" class="lazyload"></a></p><p>首先是Q，K，V三个矩阵，把a堆起来乘以w矩阵得到Q矩阵，K,V类似，这就可以同时计算。</p><p><a href="https://user-images.githubusercontent.com/60562661/77137847-88c7c700-6aaa-11ea-9972-ed2c8c284444.png" data-fancybox="group" data-caption="1584677805635" class="fancybox"><img alt="1584677805635" title="1584677805635" data-src="https://user-images.githubusercontent.com/60562661/77137847-88c7c700-6aaa-11ea-9972-ed2c8c284444.png" class="lazyload"></a></p><p>把k堆起来乘以q可以得到α，经过softmax得到α帽。</p><p><a href="https://user-images.githubusercontent.com/60562661/77137849-89605d80-6aaa-11ea-8687-b6b9d8dbb332.png" data-fancybox="group" data-caption="1584677978747" class="fancybox"><img alt="1584677978747" title="1584677978747" data-src="https://user-images.githubusercontent.com/60562661/77137849-89605d80-6aaa-11ea-8687-b6b9d8dbb332.png" class="lazyload"></a></p><p>此时把v堆起来，就可以计算得出b1，b2，b3，b4.堆起来就是最后的输出。</p><h3 id="位置信息"><a href="#位置信息" class="headerlink" title="位置信息"></a>位置信息</h3><p>此时Attention其实是没有考虑时间次序的，因为q对每一个k都会做Attention，不管远近都会做，所以时序信息是没有考虑进去的。</p><p><a href="https://user-images.githubusercontent.com/60562661/77137851-89f8f400-6aaa-11ea-98b8-fb9eeb63f627.png" data-fancybox="group" data-caption="1584678717485" class="fancybox"><img alt="1584678717485" title="1584678717485" data-src="https://user-images.githubusercontent.com/60562661/77137851-89f8f400-6aaa-11ea-98b8-fb9eeb63f627.png" class="lazyload"></a></p><p>此时只要给ai加上一个ei矩阵即可。</p><p><a href="https://user-images.githubusercontent.com/60562661/77137852-8a918a80-6aaa-11ea-9ae6-ab692a30e297.png" data-fancybox="group" data-caption="1584678784655" class="fancybox"><img alt="1584678784655" title="1584678784655" data-src="https://user-images.githubusercontent.com/60562661/77137852-8a918a80-6aaa-11ea-9ae6-ab692a30e297.png" class="lazyload"></a></p><p>此时可以解释为给输入xi拼上了一个onehot向量，用来代表是哪个输入，然后得到上面的式子。</p><p>这只是大概讲了一下 Attention怎么计算，具体的NLP中怎么应用可以深究一下，但我不是做NLP的，所以大概了解就可以。</p><p><strong>不求甚解如我。关于封面，Attention顾名思义，注意力，所以封面更能体现Attention。</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>均值与期望</title>
      <link href="/2020/03/19/%E5%9D%87%E5%80%BC%E4%B8%8E%E6%9C%9F%E6%9C%9B/"/>
      <url>/2020/03/19/%E5%9D%87%E5%80%BC%E4%B8%8E%E6%9C%9F%E6%9C%9B/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>在深度学习经常会听到期望这个词，而且总觉得与均值差不多，在这里记录一下做一个区分。</p><ul><li>平均数是一个统计学概念</li><li>期望则是一个概率论概念</li></ul><p>平均数是根据实验结果统计的样本的平均值，期望则是实验前根据样本概率分布预测样本的平均值。</p><p>之所以说“预测”是因为在实验前能得到的期望与实际实验得到的样本的平均数总会不可避免地存在偏差，毕竟随机实验的结果永远充满着不确定性。如果我们能进行无穷次随机实验并计算出其样本的平均数的话，那么这个平均数其实就是期望。当然实际上根本不可能进行无穷次实验，但是实验样本的平均数会随着实验样本的增多越来越接近期望，就像频率随着实验样本的增多会越来越接近概率一样。</p><p><code>如果说概率是频率随样本趋于无穷的极限，那么期望就是平均数随样本趋于无穷的极限。</code></p></body></html>]]></content>
      
      
      <categories>
          
          <category> Math </category>
          
          <category> 概率论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学期望 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>似然估计-Likelyhood</title>
      <link href="/2020/03/19/%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1-Likelyhood/"/>
      <url>/2020/03/19/%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1-Likelyhood/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>参考：<a href="https://zhuanlan.zhihu.com/p/36824006" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36824006</a></p><h2 id="概率-Probability-amp-似然-Likelyhood"><a href="#概率-Probability-amp-似然-Likelyhood" class="headerlink" title="概率 (Probability) & 似然 (Likelyhood)"></a>概率 (Probability) & 似然 (Likelyhood)</h2><p>概率就是随机事件发生的可能性的度量，根据实验或者已知的模型来计算某种可能性，可以称之为<strong>概率。</strong></p><p>似然则是已经知道数据结果分布，由结果来推测模型的参数，这个过程就是<strong>似然。</strong></p><p>所以这两个过程大概上是相反的。似然更通俗的说就是给定样本$X = x$下参数 $\theta = \theta_1$相对于参数取另外的值$\theta = \theta_2$为真实值的可能性。</p><h2 id="似然函数"><a href="#似然函数" class="headerlink" title="似然函数"></a>似然函数</h2><p>设模型参数为θ，则：</p><script type="math/tex; mode=display">L(\theta|x) = P(X=x|\theta)</script><p>也就是说估计θ值使得实验结果X=x。</p><p>对于某一实验，我们可能包含多种情况，其中每个实验结果的概率可记为一个集合 ：</p><script type="math/tex; mode=display">P = \{p_1,p_2,p_3,...p_n\} \  (\sum_{i=1}^np_i = 1)</script><p>假设做了m次实验，则实验结果出现概率为：</p><script type="math/tex; mode=display">E = \prod_{l=2}^mp_k,\space p_k \in P</script><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>似然函数L最大化时就是极大似然估计。极大似然估计因为是连乘，所以一般是取log之后变为求和，然后再去求最大值，具体可以参考文章开始提到的链接中的抓豆子实验，很形象。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>在深度学习中，用神经网络作分类问题实际上也就是建立了一个模型，这个模型的参数(w)就相当于似然估计中的参数θ，用大量的数据去学习，去调整w，也就是已经知道结果，由结果去估计参数(w),使得取参数w后可以更准确的估计真实数据概率。也就是在做最大似然估计，而最大似然估计已经证明就是最小化交叉熵。</p><p><strong>所以极大似然估计就是根据经验来推断规律。</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> Math </category>
          
          <category> 概率论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> likelyhood </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux下常用命令</title>
      <link href="/2020/03/19/Hello,World/"/>
      <url>/2020/03/19/Hello,World/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="Linux-Ubuntu-操作指令"><a href="#Linux-Ubuntu-操作指令" class="headerlink" title="Linux(Ubuntu) 操作指令"></a>Linux(Ubuntu) 操作指令</h2><p>记录一下我自己在Linux下常用的操作指令，会一直更新。</p><h3 id="Python环境、库安装"><a href="#Python环境、库安装" class="headerlink" title="Python环境、库安装"></a>Python环境、库安装</h3><p><strong>source activate HPEG_PT</strong>      激活虚拟环境</p><p> <strong>mkvirtualenv</strong>     创建虚拟环境</p><p><strong>conda create -n NEW_Nmae python=3.6</strong>      conda创建虚拟环境</p><p><strong>conda install pytorch=0.4.0 torchvision cudatoolkit=9.0 -c pytorch</strong>       安装pytorch</p><p><strong>pip install tensorflow-gpu==1.12</strong>  安装任意版本tensorflow</p><p><strong>pip install -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a> numpy(指定的库)</strong>    pip使用清华镜像安装库，不能翻墙可提高下载速度</p><hr><h3 id="程序监控、进程"><a href="#程序监控、进程" class="headerlink" title="程序监控、进程"></a>程序监控、进程</h3><p><strong>htop F4 f9 9 Enter</strong>   杀死某个指定进程</p><ul><li>htop 打开任务监视器</li></ul><p><strong>watch -n 1 nvidia-smi</strong>          查看显卡使用情况，每隔一秒刷新一次情况</p><p> <strong>nohup python -u train.py &>nohup.out&</strong>  命令调到后台执行，并且输出信息写到<code>nohup.out</code>文件中，这个很有用，操作服务器时本地窗口可以关掉不影响服务器运行</p><h3 id="Net-网络相关"><a href="#Net-网络相关" class="headerlink" title="Net 网络相关"></a>Net 网络相关</h3><hr><p><strong>ifconfig -a</strong>          查看ip地址、端口等网络相关信息</p><p><strong>ssh jion@1.tcp.cpolar.cn -p 20279</strong>  ssh远程连接 ssh username@ip -p(端口)</p><p><strong>服务器文件与本地同步</strong></p><p>复制到本地：   <strong>scp -r -P 端口号  用户名@ip:服务器文件夹地址/ /本地文件夹地址/</strong></p><p>复制到服务器    <strong>scp  -P 端口号 文件  用户名@ip:服务器文件夹地址/</strong></p><p>eg: </p><p><strong>scp -r -P 20279 jion@1.tcp.cpolar.cn:/media/jion/D/chenhaoming/DataSet/DouYin/images/dance.zip D:/Server</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch、numpy中的shape和size</title>
      <link href="/2020/03/18/pytorch-numpt%E4%B8%AD%E7%9A%84shape%E5%92%8Csize/"/>
      <url>/2020/03/18/pytorch-numpt%E4%B8%AD%E7%9A%84shape%E5%92%8Csize/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>每次在获得图片尺寸时我都会写错，因此来记录一下shape、size。</p><h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h2><p><code>.size()</code>是方法(function),可以传参数，例如：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> c = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># c</span></span><br><span class="line"><span class="comment">#tensor([[ 1.4753, -0.5479, -0.4448],</span></span><br><span class="line"><span class="comment">#        [ 0.1452,  0.9948,  0.1481]])</span></span><br><span class="line">c.size(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#2</span></span><br><span class="line">c.size(<span class="number">1</span>)</span><br><span class="line"><span class="comment">#3</span></span><br><span class="line">c.size[<span class="number">0</span>]</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"<stdin>"</stdin></span>, line <span class="number">1</span>, <span class="keyword">in</span> <module></module></span><br><span class="line">TypeError: <span class="string">'builtin_function_or_method'</span> object <span class="keyword">is</span> <span class="keyword">not</span> subscriptable</span><br><span class="line">c.size[<span class="number">1</span>]</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"<stdin>"</stdin></span>, line <span class="number">1</span>, <span class="keyword">in</span> <module></module></span><br><span class="line">TypeError: <span class="string">'builtin_function_or_method'</span> object <span class="keyword">is</span> <span class="keyword">not</span> subscriptable</span><br></pre></td></tr></tbody></table></figure></div><p><code>.shape则是属性</code></p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">c.shape</span><br><span class="line"><span class="comment"># torch.Size([2, 3])</span></span><br><span class="line">c.shape(<span class="number">0</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"<stdin>"</stdin></span>, line <span class="number">1</span>, <span class="keyword">in</span> <module></module></span><br><span class="line">TypeError: <span class="string">'torch.Size'</span> object <span class="keyword">is</span> <span class="keyword">not</span> callable</span><br><span class="line">c.shape[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line">c.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line">type(c.shape)</span><br><span class="line"><<span class="class"><span class="keyword">class</span> '<span class="title">torch</span>.<span class="title">Size</span>'></span></span><br></pre></td></tr></tbody></table></figure></div><p>故如果是torch的张量，在需要调用图像尺寸时，应该写：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c.size(<span class="number">0</span>)</span><br><span class="line">c.size(<span class="number">1</span>)</span><br><span class="line"><span class="comment">#或者</span></span><br><span class="line">c.shape[<span class="number">0</span>]</span><br><span class="line">c.shape[<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure></div><h2 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h2><p>numpy中，<code>.size()</code>是属性，用来输出元素个数 </p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a</span><br><span class="line"><span class="comment">#array([2, 3])</span></span><br><span class="line">a.size</span><br><span class="line"><span class="comment">#2</span></span><br><span class="line">a.size()</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"<stdin>"</stdin></span>, line <span class="number">1</span>, <span class="keyword">in</span> <module></module></span><br><span class="line">TypeError: <span class="string">'int'</span> object <span class="keyword">is</span> <span class="keyword">not</span> callable</span><br><span class="line"><span class="meta">>>> </span>a.size(<span class="number">0</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"<stdin>"</stdin></span>, line <span class="number">1</span>, <span class="keyword">in</span> <module></module></span><br><span class="line">TypeError: <span class="string">'int'</span> object <span class="keyword">is</span> <span class="keyword">not</span> callable</span><br></pre></td></tr></tbody></table></figure></div><p><code>.shape</code>也是属性</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a.shape(<span class="number">0</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"<stdin>"</stdin></span>, line <span class="number">1</span>, <span class="keyword">in</span> <module></module></span><br><span class="line">TypeError: <span class="string">'tuple'</span> object <span class="keyword">is</span> <span class="keyword">not</span> callable</span><br><span class="line"><span class="meta">>>> </span>a.shape</span><br><span class="line">(<span class="number">2</span>,)</span><br><span class="line"><span class="meta">>>> </span>a.shape[<span class="number">0</span>]</span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">>>> </span>a.shape[<span class="number">1</span>]</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"<stdin>"</stdin></span>, line <span class="number">1</span>, <span class="keyword">in</span> <module></module></span><br><span class="line">IndexError: tuple index out of range</span><br></pre></td></tr></tbody></table></figure></div><p>故如果是numpy类型的数组，获取图片尺寸应用：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a.shape[<span class="number">0</span>]</span><br><span class="line">a.shape[<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure></div><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li><p><strong>torch</strong>的<strong>Tensor</strong>形状可以用<code>size</code>，也可以用<code>shape</code>；</p></li><li><p><strong>numpy</strong>则只用<code>shape</code></p></li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习实践</title>
      <link href="/2020/03/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/"/>
      <url>/2020/03/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>这里是自己实现过的深度学习代码，放上github地址：</p><p><a href="https://github.com/tzwx/DeepLearning" target="_blank" rel="noopener">https://github.com/tzwx/DeepLearning</a></p><p><strong>会不断更新代码。</strong></p><h4 id="1-Cartoon-GAN"><a href="#1-Cartoon-GAN" class="headerlink" title="1. Cartoon_GAN"></a><strong>1. Cartoon_GAN</strong></h4><p>实现简单的GAN网络，生成动漫头像，这是DC-GAN的原型。</p><p><a href="https://user-images.githubusercontent.com/60562661/75624471-56cfed00-5bef-11ea-9518-2a8b9e6fc747.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/75624471-56cfed00-5bef-11ea-9518-2a8b9e6fc747.png" class="lazyload"></a></p><p><strong>网络结构：</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/78143357-bc1e3480-7460-11ea-9e86-843fb4f922bb.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/78143357-bc1e3480-7460-11ea-9e86-843fb4f922bb.png" class="lazyload"></a></p><p><strong>遗留的问题：</strong></p><ul><li>关于反卷积，padding等参数设置现在依然未搞清楚</li></ul><h4 id="2-Regression"><a href="#2-Regression" class="headerlink" title="2. Regression"></a><strong>2. Regression</strong></h4><p>一个简单的回归案例，用到了<code>Adagrad</code>算法。原理可以看<a href="https://huaqi.blue/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/#%E4%BB%A3%E7%A0%81%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82" target="_blank" rel="noopener">梯度下降法2</a> 这篇文章。</p><p><strong>3. Back Propagation</strong></p><p>手动实现一个简单的反向传播BP算法，理清楚原理之后发现就是矩阵的连乘。具体原理可以参考<a href="https://huaqi.blue/2020/02/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%974-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADBP%E7%AE%97%E6%B3%95/" target="_blank" rel="noopener">反向传播bp算法</a>这篇文章。</p><h4 id="4-My-Tools"><a href="#4-My-Tools" class="headerlink" title="4. My Tools"></a><strong>4. My Tools</strong></h4><p>实用工具类，我自己最常用的工具，包括图像、视频、姿态估计、可视化相关。</p><p><strong>已实现功能：</strong></p><ul><li><p>[x] 视频分解成图片</p></li><li><p>[x] 图片合成视频</p></li><li><p>[x] 图片裁剪</p></li><li><p>[x] 热图得到坐标</p></li><li><p>[x] 坐标生成热图</p></li><li><p>[x] 2D关键点可视化</p></li><li><p>[x] Json文件读写</p></li></ul><p><strong>5. LSTM Cycle网络</strong></p><p>利用CycleLoss进行人体姿态估计。以下是大致的网络结构。</p><p><a href="https://user-images.githubusercontent.com/60562661/78142337-62693a80-745f-11ea-95c6-f06f129805fb.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/78142337-62693a80-745f-11ea-95c6-f06f129805fb.jpg" class="lazyload"></a></p><p><a href="https://user-images.githubusercontent.com/60562661/78142373-701ec000-745f-11ea-96f6-394ceabc5730.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/78142373-701ec000-745f-11ea-96f6-394ceabc5730.png" class="lazyload"></a></p><p>LSTM_Cycle <strong>项目情况</strong></p><ul><li><p>项目初衷是做弱监督下的人体姿态估计，但是事实上并不是弱监督，事实上训练弱监督效果太差，一方面是idea本身比较弱，另一方面则可能是代码的问题，我也不知道是哪方面原因。</p></li><li><p>这个项目是我做的第一个比较完整的项目，从0开始手撸，未用到现在任何的经典卷积网络，都是自己搭建起来的。</p></li><li><p>但是现在由于种种原因可能要放弃了，所以在这里来记录一下原理，防止遗忘。</p></li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAN_4 f-GAN推导</title>
      <link href="/2020/03/15/GAN-4-f-GAN%E6%8E%A8%E5%AF%BC/"/>
      <url>/2020/03/15/GAN-4-f-GAN%E6%8E%A8%E5%AF%BC/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="为什么要引入f-GAN？"><a href="#为什么要引入f-GAN？" class="headerlink" title="为什么要引入f-GAN？"></a>为什么要引入f-GAN？</h2><p>之前谈到GAN基础理论，可以知道GAN的生成器其实就是在最小化 <strong>P_data</strong> 和 <strong>P_G</strong> 的 JS散度。而事实上不一定要用<code>JS-Divergence</code> 来测量，<strong>KL-Divergence、JS-Divergence</strong>等都是属于<strong>f-Divergence</strong>，故f-GAN其实就是想要换掉GAN中的分布之间的距离测度函数。</p><h2 id="f-Divergence"><a href="#f-Divergence" class="headerlink" title="f-Divergence"></a>f-Divergence</h2><script type="math/tex; mode=display">D_f(P||Q) = \int_x q(x)*f(\frac{p(x)}{q(x)})dx</script><p>其中，f是<code>convex凸函数</code>, f(1) = 0，故：<br><a href="https://user-images.githubusercontent.com/60562661/76704169-fb831c00-6711-11ea-94ea-6a4b4879e385.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/76704169-fb831c00-6711-11ea-94ea-6a4b4879e385.png" class="lazyload"></a></p><p>其中<code>>=</code>用到了詹森不等式.</p><ul><li>若p(x),q(x)相等，则值为0；</li><li>若其不相等，则<strong>Df>=0</strong>  因此 Df可以用来测量分布之间的距离</li></ul><p><a href="https://user-images.githubusercontent.com/60562661/76704155-ee662d00-6711-11ea-8e2e-17e4da092990.png" data-fancybox="group" data-caption="1584260831145" class="fancybox"><img alt="1584260831145" style="zoom:67%;" title="1584260831145" data-src="https://user-images.githubusercontent.com/60562661/76704155-ee662d00-6711-11ea-8e2e-17e4da092990.png" class="lazyload"></a></p><p>此时f(x)取不同的表达式，得到不同的散度。</p><h2 id="共轭函数"><a href="#共轭函数" class="headerlink" title="共轭函数"></a>共轭函数</h2><p><a href="https://user-images.githubusercontent.com/60562661/76704159-f1f9b400-6711-11ea-9639-528670025d24.png" data-fancybox="group" data-caption="1584260962776" class="fancybox"><img alt="1584260962776" style="zoom: 80%;" title="1584260962776" data-src="https://user-images.githubusercontent.com/60562661/76704159-f1f9b400-6711-11ea-9639-528670025d24.png" class="lazyload"></a></p><p>每一个凸函数<code>f</code>都有一个共轭函数<code>f*</code></p><p>对于函数f(x),</p><ul><li>首先t取t1，x取遍定义域内所有的值x1…xn,找出最大的$f^*(t1) = x_1t_1-f(x)$</li><li>对于每一个t都用相同的方法计算，就可以知道最终的f*</li></ul><p>这样计算太麻烦，于是：</p><p><a href="https://user-images.githubusercontent.com/60562661/76704161-f32ae100-6711-11ea-8892-daa86908a963.png" data-fancybox="group" data-caption="1584261338486" class="fancybox"><img alt="1584261338486" title="1584261338486" data-src="https://user-images.githubusercontent.com/60562661/76704161-f32ae100-6711-11ea-8892-daa86908a963.png" class="lazyload"></a></p><p>如上图，画出所有的函数图像，取得图像上界即可。</p><p><a href="https://user-images.githubusercontent.com/60562661/76704163-f4f4a480-6711-11ea-840f-774ec211ec48.png" data-fancybox="group" data-caption="1584280774347" class="fancybox"><img alt="1584280774347" title="1584280774347" data-src="https://user-images.githubusercontent.com/60562661/76704163-f4f4a480-6711-11ea-840f-774ec211ec48.png" class="lazyload"></a></p><p>当$f(x) = x\log(x)$时，求的其共轭函数为：$f^*(t) = exp(t-1)$</p><h2 id="Connection-with-GAN"><a href="#Connection-with-GAN" class="headerlink" title="Connection with GAN"></a>Connection with GAN</h2><p><a href="https://user-images.githubusercontent.com/60562661/76704164-f58d3b00-6711-11ea-9e3f-aa670f1f3445.png" data-fancybox="group" data-caption="1584281621663" class="fancybox"><img alt="1584281621663" title="1584281621663" data-src="https://user-images.githubusercontent.com/60562661/76704164-f58d3b00-6711-11ea-9e3f-aa670f1f3445.png" class="lazyload"></a></p><p>这块的逻辑是这样：</p><ul><li>首先 f(x) 于 f* 互为共轭函数，故可以有第一行的公式，然后把它代入D中，得到上图中得第三行的公式，目前问题变为：找一个t，使得这个积分项最大；</li><li>直接找t不好找，所以训练一个判别器D，输入是x，输出是t，D(x) = t,便得到上图下部分得公式，所以问题又转化为寻找一个D，可以最大两个积分项做减法得值</li><li>简单来说，就是把寻找t变为寻找D(x)</li></ul><p><a href="https://user-images.githubusercontent.com/60562661/76704165-f756fe80-6711-11ea-8701-537bbba1819e.png" data-fancybox="group" data-caption="1584282526678" class="fancybox"><img alt="1584282526678" title="1584282526678" data-src="https://user-images.githubusercontent.com/60562661/76704165-f756fe80-6711-11ea-8701-537bbba1819e.png" class="lazyload"></a></p><p>这里就很顺理成章了，P_data 和 P_G 之间的f-divergence就可以求出来了;<strong>而D(x)、f*()取不同的表达式 ，得到的就是不同的f-divergence。</strong></p><p>对于生成器来说，就是最小化真实数据和生成数据之间的<code>f-divergence</code></p><p><a href="https://user-images.githubusercontent.com/60562661/76704167-f9b95880-6711-11ea-9b66-0b6a6fb6e0ca.png" data-fancybox="group" data-caption="1584284110315" class="fancybox"><img alt="1584284110315" title="1584284110315" data-src="https://user-images.githubusercontent.com/60562661/76704167-f9b95880-6711-11ea-9b66-0b6a6fb6e0ca.png" class="lazyload"></a></p><p>其中，Generator f(u) 就是上面公式中的D(x)表达式；f<em>(t)就是上面公式的 f\</em>.</p><p>以上就是f-GAN得数学推导过程。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Auto Encoder in DeepLearning</title>
      <link href="/2020/03/06/Auto-Encoder-in-DeepLearning/"/>
      <url>/2020/03/06/Auto-Encoder-in-DeepLearning/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>VAE部分公式推导省略的可以观看李宏毅老师的课：<a href="https://www.bilibili.com/video/av9770190?p=18" target="_blank" rel="noopener">https://www.bilibili.com/video/av9770190?p=18</a></p><h2 id="Back-Ground"><a href="#Back-Ground" class="headerlink" title="Back Ground"></a>Back Ground</h2><p>首先自编码器的意义是什么呢？</p><p>以CV举例，在影像处理中，人脸识别一张普通的200<em>200像素的图，就有40000维向量要处理，显然不实际，因此如果可以有一个编码器可以输入一张图，输出一个30维的向量；再将这个30维向量输出成200\</em>200的图，尽量与原图接近。也就是说30维的向量代表了40000维的图，并且尽量保持图片特征、不失真，这就是自编码器的应用。</p><h2 id="Auto-Encoder"><a href="#Auto-Encoder" class="headerlink" title="Auto Encoder"></a>Auto Encoder</h2><p>最初的 Auto Encoder设计结构如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/76104292-15d04200-600e-11ea-8aee-705cbc26107b.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/76104292-15d04200-600e-11ea-8aee-705cbc26107b.jpg" class="lazyload"></a></p><p>只要让输出尽可能的接近输入即可，然后改造成深度自编码器，如下：</p><p><a href="https://user-images.githubusercontent.com/60562661/76104294-1668d880-600e-11ea-825c-53ce6dd24f79.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/76104294-1668d880-600e-11ea-825c-53ce6dd24f79.jpg" class="lazyload"></a></p><p>loss依然是最初的。这便是<code>Auto Encoder</code></p><h2 id="Problem-in-Auto-Encoder"><a href="#Problem-in-Auto-Encoder" class="headerlink" title="Problem in Auto Encoder"></a>Problem in Auto Encoder</h2><p>首先，Auto Encoder存在一个问题，它把所有训练的图片都是对应到了一个高维空间中的一个点，可以这么理解，本来图像是40000维空间的一个点，经过编码变成30维空间的一个点，如果采样正好采了这个点，则可以比较精确的用解码器还原图像；但是如果在30维空间中，采样到了一个从未训练过的点，那么解码器就大概率只会解码出一堆噪音，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/76104295-17016f00-600e-11ea-9845-bc047ffaeb5e.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/76104295-17016f00-600e-11ea-9845-bc047ffaeb5e.jpg" class="lazyload"></a></p><p>如果采样到 code中间未训练过的点，解码出来的图像就不像是一张真实图像，会是一堆乱码，而VAE做的事情就是每张图片不在是对应一个点，而是一个区间，在这个区间内都可以解码出这张图片，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/76104298-179a0580-600e-11ea-8905-cd6403150473.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/76104298-179a0580-600e-11ea-8905-cd6403150473.png" class="lazyload"></a></p><p>这就引入了VAE.</p><h2 id="Variance-Auto-Encoder-VAE"><a href="#Variance-Auto-Encoder-VAE" class="headerlink" title="Variance Auto Encoder(VAE)"></a>Variance Auto Encoder(VAE)</h2><p>先看一下VAE的整体架构：</p><p><a href="https://user-images.githubusercontent.com/60562661/76104300-18329c00-600e-11ea-97d8-8040afde0995.png" data-fancybox="group" data-caption="1583506317352" class="fancybox"><img alt="1583506317352" title="1583506317352" data-src="https://user-images.githubusercontent.com/60562661/76104300-18329c00-600e-11ea-97d8-8040afde0995.png" class="lazyload"></a></p><p>首先是输入经过一个编码器，产生一组m，一组σ，然后一组e是从高斯分布采样的，即：</p><script type="math/tex; mode=display">m_i+\exp(\sigma_i)+e_i = c_i</script><p>也就是原始编码加上噪音，而σ则是控制噪音的方差，即$\exp(\sigma_i)$就是噪音的方差，是自动学习的；同时损失函数在原来的基础上，加上了上图右下角一项。</p><p>直观上，如果不加限制的让机器自己去学习，那么机器肯定会认为噪音对原图像干扰越小越好，于是会给exp(σ)赋值为0或者很接近0的数，但是这也就失去了意义。因此加上这一项，$\exp(\sigma_i)-(1+\sigma_i)$的最小值在σ=0时得到最小值，也就是说σ=0loss最小，此时方差=1,所以机器自己学习就不会让Variance太小；$m_i^2$可以认为是L2正则化。这就是直观上VAE这样设计的原理，下面从数学上理解VAE的原理。</p><h2 id="VAE的原理"><a href="#VAE的原理" class="headerlink" title="VAE的原理"></a>VAE的原理</h2><p>首先，我们的工作任务是可以采样到需要的图像，也就是要估计原始图像的概率分布P(X),如果知道了原始图像的分布，那么我们只要让我们的生成的图像分布尽量接近原始图像也就是计算它们的KL散度即可。所以为题转化为求P(X).</p><p>高斯混合模型认为，任何一个分布都可以由多种高斯分布混合(加权和)而成。则此时；</p><script type="math/tex; mode=display">P(X) =\sum_m P(m)*P(x|m)\\x|m\approx N(\mu_m,\sigma_m)</script><p>这件事更像是对图像做了一个分类，我们所看到的x都来自于某一类，这是不好的，更好的方法应该是用一个向量来表示图像。</p><p>则此时引出了z，z是一个隐向量，是服从高斯分布的；向量的每一个维度对应图像的某一些特征。<strong>注意，这里之所以z取高斯分布，是因为逻辑上说，没有特色的东西占多数，图像每种属性的分布其实大概率是服从高斯分布的，因此z取高斯分布也是比较合理的，但是z可以是任何分布。</strong></p><p>同时，z有无穷多个，是连续的，不再是高斯混合模型那样有固定个z；每一个z对应的均值μ、方差σ都是由神经网络学来的。</p><p>所以此时，</p><script type="math/tex; mode=display">P(X) =\int_z P(z)*P(X|z)dz\\P(z)\approx N(0,1)\\X|z\approx N(\mu(z),\sigma(z))</script><p>真正要求的就是μ(z)、σ(z)，最大化P(X):</p><script type="math/tex; mode=display">L = \sum_X\log P(X)</script><p><a href="https://user-images.githubusercontent.com/60562661/76104281-11a42480-600e-11ea-9576-56fff9f9fc18.png" data-fancybox="group" data-caption="1583509685308" class="fancybox"><img alt="1583509685308" title="1583509685308" data-src="https://user-images.githubusercontent.com/60562661/76104281-11a42480-600e-11ea-9576-56fff9f9fc18.png" class="lazyload"></a></p><p>所以，此时z经过一个伸进和网络输出均值和方差，目的是最大化L；这时候需要引入另外一个分布q(z|x),也就是输入图像，提取它的高斯分布；所以，上图中蓝色的就是<strong>Decoder</strong>，绿色的就是<strong>Encoder</strong>。然后继续用数学推导：</p><p><strong>注意以下公式推导有跳步，具体可以看一下李宏毅老师的视频讲解。</strong></p><script type="math/tex; mode=display">\log P(x) = \int_z q(z|x)\log P(x)dz = \int_zq(z|x)\log\frac{P(z,x)}{q(z|x)}dz+KL(q(z|x)||p(z|x))</script><p>其中，KL散度一项大于等于0，故前面的一项就是$\log P(x)$的下限(lower bound).这个下限记为Lb:</p><script type="math/tex; mode=display">L_b=\int_zq(z|x)\log\frac{P(z,x)}{q(z|x)}dz=\int_zq(z|x)\log\frac{P(z)*P(x|z)}{q(z|x)}dz</script><p>则原式化作:</p><script type="math/tex; mode=display">\log P(x) = L_b + KL(q(z|x)||p(z|x))</script><p><strong>这里就真正的体现了VAE的精妙之处：</strong></p><p>本来是要寻找P(x|z)来最大化L，但是现在需要同时寻找P(x|z)、q(z|x)两项，来最大化Lb，从而最大化L。</p><p>q分布实际上与 <code>log P(x)</code> 是无关的，log P(x)至于P分布有关。所以q无论取什么值，log P(x) 都不变，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/76104286-149f1500-600e-11ea-9125-d9cd58c1fa8f.png" data-fancybox="group" data-caption="1583510884527" class="fancybox"><img alt="1583510884527" title="1583510884527" data-src="https://user-images.githubusercontent.com/60562661/76104286-149f1500-600e-11ea-9125-d9cd58c1fa8f.png" class="lazyload"></a></p><p>所以当P固定，q最大化Lb时，KL会越来越小，最后消失不见，也就是q(z|x)和p(z|x)的分布完全相同，此时在上升下限，Likelyhood也会最大化。<strong>所以也就是说损失函数中实际起作用的就是Lb这一项。</strong></p><p>所以此时，就是寻找P(x|z)、q(z|x)最大化Lb来最大化似然估计，同时顺便会找到q(z|x)相似于p(z|x)。</p><script type="math/tex; mode=display">L_b=\int_zq(z|x)\log\frac{P(z,x)}{q(z|x)}dz = -KL(q(z|x)||P(z))+\int_zq(z|x)\log P(x|z)dz</script><p>此时最大化Lb，也就是要最小化q(z|x)和P(z)的相似度，其中q是一个神经网络，用来提取输入x所服从的高斯分布，所以这里最小化的散度就是要调节q所对应的神经网络，让它产生的高斯分布与z这个高斯分布越接近越好，最小化散度这一项可以推导为：</p><script type="math/tex; mode=display">KL(q(z|x)||P(z)) = \sum_{i=1}^3(\exp(\sigma_i)-(1+\sigma_i)+(m_i)^2)</script><p>这就是文章一开始提到的VAE架构中新增的需要最小化的损失函数。</p><p>而Lb另外一项积分也要最大化，即：</p><script type="math/tex; mode=display">\max \space \int_zq(z|x)\log P(x|z)dz</script><p>直观上就是从q中采样一个z分布，使得在在z分布下采样到的x的概率越大越好。这实际上就是Auto Encoder在做的事情。</p><p><a href="https://user-images.githubusercontent.com/60562661/76104288-1537ab80-600e-11ea-805b-f8f4c316caa4.png" data-fancybox="group" data-caption="1583512456046" class="fancybox"><img alt="1583512456046" title="1583512456046" data-src="https://user-images.githubusercontent.com/60562661/76104288-1537ab80-600e-11ea-805b-f8f4c316caa4.png" class="lazyload"></a></p><p>通俗的描述一下，就是说，首先q会从输入图像x中采样出一个(Normal Distribution )z,然后要最大化z分布产生x的概率，就是会把z作为NN的输入，输出一组高斯分布，使得这个高斯分布产生x的概率最大 。所以现在就是如何让这组高斯产生的x概率最大。</p><p>实际上在训练时，我们不会取考虑方差，只需要让Decoder输出的均值<strong>(mean)μ=x(Input)</strong>即可，因为高斯分布在均值μ处采样的可能性最大，所以只需要让x=μ即可。<strong>也就是说这一部分就是让输入的x与输出尽可能的接近。</strong></p><p><strong>同时，因为Encoder和Decoder的输出3都是一组分布，即现在图片对应的是一个分布，所以也就解决了把图片对应到了一个点上的问题，也就解决了Auto Encoder的存在的问题。</strong></p><p>所以Lb这两项合起来，就是文章一开始提到的VAE的两个损失函数。所以说VAE就精妙在损失函数的设计上面。</p><p><strong>妙哉妙哉！</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> VAE </tag>
            
            <tag> Auto Encoder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>概率论的贝叶斯公式</title>
      <link href="/2020/03/06/%E6%A6%82%E7%8E%87%E8%AE%BA%E7%9A%84%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F/"/>
      <url>/2020/03/06/%E6%A6%82%E7%8E%87%E8%AE%BA%E7%9A%84%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>概率论是深度学习的基础，但是我发现已经忘记了概率论中很多知识，所以复习了一下，做一下记录，也算是想起一些以前疑惑的东西。</p><p>关于贝叶斯与全概率，</p><p>数学上张宇讲的比较透彻，也比较容易理解：<a href="https://www.bilibili.com/video/av9735632?p=5" target="_blank" rel="noopener">https://www.bilibili.com/video/av9735632?p=5</a></p><p>但是可以从另外一个给人启发的角度看待：<a href="https://www.bilibili.com/video/av84799361" target="_blank" rel="noopener">https://www.bilibili.com/video/av84799361</a></p><h2 id="互斥与独立"><a href="#互斥与独立" class="headerlink" title="互斥与独立"></a>互斥与独立</h2><ul><li><code>两个事件互斥 == 两个事件互不相容</code> , 这一点可以从几何关系理解，也就是说两个集合没有交集；</li><li>两个事件独立则是从概率的角度来讲的，举一个简单的公式，若A、B事件互相独立，则：</li></ul><script type="math/tex; mode=display">P(AB) = P(A) * P(B)</script><p>然后由条件概率可以知道，在A发生的条件下，B发生的概率为：</p><script type="math/tex; mode=display">P(B|A) = \frac {P(AB)}{P(A)}\\P(AB) = P(A)*P(B|A)</script><p>若AB事件独立，可以推出</p><script type="math/tex; mode=display">P(B) = P(B|A)</script><p><strong>通俗来说，就是B发生的概率与在A发生的条件下B发生的概率相同，也就是A事件发生并不影响B，这就是AB相互独立。</strong></p><p>这两者其实是没什么关系的，不用刻意去区分这两个概念。</p><h2 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h2><p>数学上理解，贝叶斯公式就是条件概率，执果索因：</p><script type="math/tex; mode=display">P(H|E) = \frac {P(H)P(E|H)}{P(E)} = \frac{P(H)P(E|H)}{P(H)P(E|H)+P(\neg H)P(E|\neg H)}</script><p>其中，</p><ol><li><p>P(H)也叫<strong>先验概率</strong>，H常被视为导致试验结果E发生的”原因“</p></li><li><p>P(E|H)也叫似然概率，likelyhood</p></li><li>P(H|E)这个计算出来的概率则成为<strong>后验概率</strong>，当试验产生了结果A之后，再对各种原因概率的新认识，故称后验概率。</li></ol><p>贝叶斯公式给我们的思考是：</p><ul><li><strong>见到所有的证据从而限制了概率空间之后，在考虑比例，这就是贝叶斯公式的精髓</strong></li><li><strong>证据不应该直接决定了你的看法，而是应该更新你的看法</strong></li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> Math </category>
          
          <category> 概率论 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 贝叶斯 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Recurrent Neural Network(RNN)</title>
      <link href="/2020/03/05/Recurrent-Neural-Network-RNN/"/>
      <url>/2020/03/05/Recurrent-Neural-Network-RNN/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>RNN,循环神经网络，一般用来处理序列化任务。例如一个句子就是一个词序列；一段视频就是一个图像序列；人讲一句话也是一个序列，处理序列就考虑到了时间维的信息。</p><h2 id="SIMPLE-RNN"><a href="#SIMPLE-RNN" class="headerlink" title="SIMPLE RNN"></a>SIMPLE RNN</h2><p>最简单的一个<strong>RNN循环神经网络</strong>就是存储了上一时刻的隐藏状态的值。简单的讨论一下RNN的设计结构。</p><p><a href="https://user-images.githubusercontent.com/60562661/75954684-41103f80-5eef-11ea-9846-59ada03ace43.png" data-fancybox="group" data-caption="1583384720030" class="fancybox"><img alt="1583384720030" style="zoom:80%;" title="1583384720030" data-src="https://user-images.githubusercontent.com/60562661/75954684-41103f80-5eef-11ea-9846-59ada03ace43.png" class="lazyload"></a></p><p>假设只有一个隐藏层，有两个句子：</p><ul><li>leave <strong>Taipei</strong></li><li>arrive <strong>Taipei</strong></li></ul><p>如果采用一般的神经网络，见到<strong>Taipei</strong>这同一个词汇，输出的值都是相同的，与预期任务就不符合了。</p><p>首先leave作为第一时刻的输入，产生一个值；隐藏状态值是a1要存储起来，如上图的蓝色a1，然后下一时刻输入<strong>Taipei</strong>就同时收到<code>x2、a1</code> 的影响。后面的也同理，因此同一个输入就有不同的输出。</p><p>上面是一个简单的单向的RNN，还有双向的RNN，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/75954689-42da0300-5eef-11ea-8ffc-c46500154632.png" data-fancybox="group" data-caption="1583385188547" class="fancybox"><img alt="1583385188547" style="zoom:67%;" title="1583385188547" data-src="https://user-images.githubusercontent.com/60562661/75954689-42da0300-5eef-11ea-8ffc-c46500154632.png" class="lazyload"></a></p><p>训练一个正向的和一个反向的循环神经网络，然后把同一时刻(xt)两个网络的隐藏层拿出来同时计算该时刻的输出(yt).也就是说计算每一时刻的输出都要考虑到整个序列。</p><h2 id="Long-Short-term-Memory（LSTM）"><a href="#Long-Short-term-Memory（LSTM）" class="headerlink" title="Long Short-term Memory（LSTM）"></a>Long Short-term Memory（LSTM）</h2><p>上面的RNN是一个简单的循环神经网络的结构，它的记忆非常非常有限，只能记住前一个时刻的，这对我们来说仍然是不足的，因此需要记住更久的东西，就有了LSTM。（<strong>注意名字，是多个短期记忆，一般叫长短期记忆网络，应该是很长的短期记忆网络，而不是长-短 期记忆网络。</strong>）</p><p>LSTM网络结构如下：</p><p><a href="https://user-images.githubusercontent.com/60562661/75954691-43729980-5eef-11ea-8dce-c0976f87aa09.png" data-fancybox="group" data-caption="1583385772848" class="fancybox"><img alt="1583385772848" style="zoom:67%;" title="1583385772848" data-src="https://user-images.githubusercontent.com/60562661/75954691-43729980-5eef-11ea-8dce-c0976f87aa09.png" class="lazyload"></a></p><p><strong>它的核心在于中间蓝色部分的记忆细胞。</strong></p><p>它的计算过程观察上图，首先有 $z,z_i,z_f,z_o$ 四个gate，这四个门可以理解为四个控制信号，其中</p><ul><li>$z$ 表示的是要输入的信息的信号；</li><li>$z_i$ 表示的是输入门的信号；</li><li>$z_f$ 表示的是遗忘门的信号；</li><li>$z_o$ 表示的是输出门的信号；</li></ul><script type="math/tex; mode=display">Cell = \sigma(z) * \tanh(z_i) + c * \sigma(z_f)\\Out = \tanh(Cell) * \sigma(z_o)</script><p>其中， $z,z_i,z_f,z_o$的值就是输入$x_t$ 乘上四个矩阵得到的，整个流程如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/75954692-44a3c680-5eef-11ea-9e4f-8663edfda312.png" data-fancybox="group" data-caption="1583386846200" class="fancybox"><img alt="1583386846200" style="zoom: 80%;" title="1583386846200" data-src="https://user-images.githubusercontent.com/60562661/75954692-44a3c680-5eef-11ea-9e4f-8663edfda312.png" class="lazyload"></a></p><p>细胞贯穿主线，xt的输入得到输出yt嘛，中间经过一系列的组合计算；<strong>注意上图中z其实也是有激活函数的，z激活函数是tanh()</strong>.</p><p>上图只是展示了LSTM计算过程，实际操作时会照以下操作（单个LSTM）：</p><p><a href="https://user-images.githubusercontent.com/60562661/75954698-453c5d00-5eef-11ea-9b2f-872c5c166771.png" data-fancybox="group" data-caption="1583387314831" class="fancybox"><img alt="1583387314831" style="zoom:78%;" title="1583387314831" data-src="https://user-images.githubusercontent.com/60562661/75954698-453c5d00-5eef-11ea-9b2f-872c5c166771.png" class="lazyload"></a></p><p>会把前一时刻的输出值($h<em>{t-1}$)、记忆单元值($c</em>{t-1}$)、这一时刻的输入值 ($x_T$)堆在一起，一起再去计算，堆在一起就是torch中的级联：<code>torch.cat()</code></p><p>在实际训练网络时单个LSTM肯定是不够的，所以需要计算多个的，此时：</p><p><a href="https://user-images.githubusercontent.com/60562661/75954700-45d4f380-5eef-11ea-9a49-40ac10040c87.png" data-fancybox="group" data-caption="1583387631495" class="fancybox"><img alt="1583387631495" style="zoom:90%;" title="1583387631495" data-src="https://user-images.githubusercontent.com/60562661/75954700-45d4f380-5eef-11ea-9a49-40ac10040c87.png" class="lazyload"></a></p><p>横向的都是同一个LSTM，纵向的是堆起来的不同的LSTM模块。</p><p><strong>下面总结一下LSTM模块：</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/75954701-47062080-5eef-11ea-9419-26acce47dc07.png" data-fancybox="group" data-caption="1583387847413" class="fancybox"><img alt="1583387847413" title="1583387847413" data-src="https://user-images.githubusercontent.com/60562661/75954701-47062080-5eef-11ea-9419-26acce47dc07.png" class="lazyload"></a></p><p>这张图实际上前一时刻的细胞状态是没有和输入叠在一起的。以上就是整个LSTM架构，而它的几个变体都是大同小异。</p><h2 id="Why-Lstm-not-Simple-RNN"><a href="#Why-Lstm-not-Simple-RNN" class="headerlink" title="Why Lstm not Simple-RNN?"></a>Why Lstm not Simple-RNN?</h2><p>RNN基础模型一般不是很容易训练起来，面临梯度弥散、梯度爆炸的问题。</p><p> <a href="https://user-images.githubusercontent.com/60562661/75954702-479eb700-5eef-11ea-9372-eca219db0d1c.png" data-fancybox="group" data-caption="1583388071831" class="fancybox"><img alt="1583388071831" style="zoom: 80%;" title="1583388071831" data-src="https://user-images.githubusercontent.com/60562661/75954702-479eb700-5eef-11ea-9372-eca219db0d1c.png" class="lazyload"></a></p><p>如图，绿色的线条是实验的RNN的loss，它震荡不收敛。造成这个的原因在于<strong>Error Surface is rough：</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/75954705-48374d80-5eef-11ea-9842-0f01ffaf2a53.png" data-fancybox="group" data-caption="1583388245860" class="fancybox"><img alt="1583388245860" style="zoom:80%;" title="1583388245860" data-src="https://user-images.githubusercontent.com/60562661/75954705-48374d80-5eef-11ea-9842-0f01ffaf2a53.png" class="lazyload"></a>**</p><p>如图，如果梯度下降法刚好更新到断层上，此时梯度骤升，学习率比较大就会飞出去很远，就直接没有了</p><p><a href="https://user-images.githubusercontent.com/60562661/75954707-48cfe400-5eef-11ea-86fd-f8fe8c1ab044.png" data-fancybox="group" data-caption="1583388353972" class="fancybox"><img alt="1583388353972" style="zoom:80%;" title="1583388353972" data-src="https://user-images.githubusercontent.com/60562661/75954707-48cfe400-5eef-11ea-86fd-f8fe8c1ab044.png" class="lazyload"></a></p><p>这就很直观的解释了原因，每次RNN都会用前一次的输出不加控制的影响下一个结果，也就是护送每次都hi把Memory的值完全变化掉。w是指数级就很容易有<code>Gradient Vanish(explore)</code> 的问题。</p><p>而LSTM，</p><ul><li><p>它把Memory的值<em>一个值+input\</em>一个值更新到Cell中，也就是说它的Memory和input是相加的；</p></li><li><p>如果weight影响到记忆细胞的值，则一直会存在，除非遗忘门决定遗忘。而RNN则会一直洗掉。因此LSTM解决了梯度弥散问题。</p></li><li><p>一般遗忘门经常要开着，bias要比较大。</p></li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RNN </tag>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积、反卷积图像尺寸计算</title>
      <link href="/2020/03/02/%E5%8D%B7%E7%A7%AF%E3%80%81%E5%8F%8D%E5%8D%B7%E7%A7%AF%E5%9B%BE%E5%83%8F%E5%B0%BA%E5%AF%B8%E8%AE%A1%E7%AE%97/"/>
      <url>/2020/03/02/%E5%8D%B7%E7%A7%AF%E3%80%81%E5%8F%8D%E5%8D%B7%E7%A7%AF%E5%9B%BE%E5%83%8F%E5%B0%BA%E5%AF%B8%E8%AE%A1%E7%AE%97/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>一般在设计深度网络架构时，神经网络中无非也就是卷积层和全连接层，而网络一般会对图像尺寸有限制，卷积改变图像的尺寸比较重要，需要会计算。因此来计算一下卷积和反卷积的尺寸计算。以<code>pytorch</code>为例.</p><h2 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2D(in_channels,out_channels,kernel_size,stride,padding,bias)</span><br><span class="line"><span class="comment">#以上几个参数比较常用</span></span><br></pre></td></tr></tbody></table></figure></div><p>卷积操作后图像计算尺寸为：</p><script type="math/tex; mode=display">W_{out} = \frac {W_{input} - W_{filter} + 2*padding}{stride + 1}</script><script type="math/tex; mode=display">H_{out} = \frac {H_{input} - H_{filter} + 2*padding}{stride + 1}</script><ul><li>一般情况下，均为正方形，即<code>W = H</code></li><li>如果计算结果不是正数，则向下取整</li></ul><h2 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ConvTranspose2D(in_channels, out_channels, kernel_size, stride, padding,  output_padding，bias)</span><br><span class="line"><span class="comment"># 其中，padding是输入图片补0个数，output_padding是输出图像补0个数</span></span><br><span class="line"><span class="comment"># 直观上是扩大图片，边上填充0也可以用来扩大图像</span></span><br></pre></td></tr></tbody></table></figure></div><p>反卷积图像尺寸计算公式：</p><script type="math/tex; mode=display">W_{out} = (W_{input} - 1)*stride + Padding_{output} - 2*Padding_{input} + W_{Kernelsize}</script><script type="math/tex; mode=display">H_{out} = (H_{input} - 1)*stride + Padding_{output} - 2*Padding_{input} + H_{Kernelsize}</script><p>其中，</p><ul><li>$Padding_{output}$ 指的是<code>ConvTranspose2D</code>这个函数中的<code>output_padding</code> 参数；</li><li>$Padding_{input}$ 指的是上述函数的<code>padding</code> 参数</li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Resnet 网络分析</title>
      <link href="/2020/03/01/Resnet-%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90/"/>
      <url>/2020/03/01/Resnet-%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>Resnet，网络层数非常深，是比较经典的CNN模型，Rennet实现了很深层次的网络的训练。本文主要讨论一下resnet的核心思想。</p><h2 id="Universal-approximation-theorem"><a href="#Universal-approximation-theorem" class="headerlink" title="Universal approximation theorem"></a>Universal approximation theorem</h2><p>万能近似定理，也称通用近似定理，指的是：一个具有两层的神经网络，即一个隐藏层、一个输出层，理论上就可以模拟任何的函数，无论多复杂的函数都可以，只要神经元数量够多。</p><p>这个可以理解为 每个神经元都是一个线性的函数，<code>w*x+b</code>，但是当很多神经元组合在一起，就可以模拟出曲线，神经元越多，拟合出的曲线也就会越平滑，这里有点类似于微分，认为曲线是无线可切分的，切到最后会近似成直线，反之，无限多个线性函数自然可以拟合出很平滑的曲线。</p><p>但是这只是理论上的，实际上如果真的只采用两层的神经网络，在可行性上就会有很大的问题，比如由于参数量过多可能根本训练不起来。</p><h2 id="Residual-Block"><a href="#Residual-Block" class="headerlink" title="Residual Block"></a>Residual Block</h2><h3 id="退化问题"><a href="#退化问题" class="headerlink" title="退化问题"></a>退化问题</h3><p>首先，一般认为神经网络层数越深，表达能力会越强。但是实际上，随着神经网络层数加深，却可能出现准确率降低，注意不是过拟合，称为退化问题。因此这种网络结构设计主要是解决了退化问题。而这个方法的出现，也意味着 更深的神经网络可以训练起来了。</p><p><a href="https://user-images.githubusercontent.com/60562661/75651146-cba83300-5c92-11ea-89dd-d2079e49198f.png" data-fancybox="group" data-caption="1583123998907" class="fancybox"><img alt="1583123998907" title="1583123998907" data-src="https://user-images.githubusercontent.com/60562661/75651146-cba83300-5c92-11ea-89dd-d2079e49198f.png" class="lazyload"></a></p><p>上图便是残差模块。x是输入，经过中间的隐藏层后不直接输出F(x)，而是输入x两一个分支连接到输出出，最终输出F(x)+x,即：</p><script type="math/tex; mode=display">H(x) = F(x) + x</script><p>因为之前的层表现已经很好了，这个多出来的层的本来的学习目标是：让H(x)=x，也就是说尽量让输出不变，但是实际上很难学习到这一点，所以现在的目标就变成了让<code>F(x)->0</code></p><p>为什么说F(x)趋近于0更好训练呢？(借鉴别人)我认为可以从以下两方面考虑：</p><ul><li>Hidden Layer 权重初始化都在0附近，输出的值自然会比较靠近0，因此可能会更好的训练；</li><li>在该结构中，所有的激活函数都是 <code>RELU</code>, 而relu函数的特性是：小于等于0都置0，这就说明不管怎么样总有一半的概率可以直接置0，不用去计算</li></ul><p><a href="https://user-images.githubusercontent.com/60562661/75651148-ce0a8d00-5c92-11ea-95f6-543f0458f5d1.png" data-fancybox="group" data-caption="1583126154433" class="fancybox"><img alt="1583126154433" title="1583126154433" data-src="https://user-images.githubusercontent.com/60562661/75651148-ce0a8d00-5c92-11ea-95f6-543f0458f5d1.png" class="lazyload"></a></p><p>这张图可以看出来，解决了退化问题后，34层的误差明显低于18层</p><h3 id="梯度弥散"><a href="#梯度弥散" class="headerlink" title="梯度弥散"></a>梯度弥散</h3><p>关于梯度弥散这个问题我纠结了比较久，发现问题在于我把x理解的狭隘了。从数学上理解一下这个问题 ：</p><script type="math/tex; mode=display">X_{l+1} = X_l + F(X_l,W_l)</script><script type="math/tex; mode=display">X_{l+2} = X_{l+1} + F(X_{l+1},W_{l+1}) =X_l + F(X_l,W_l) + F(X_{l+1},W_{l+1})</script><p>故可以推出：</p><script type="math/tex; mode=display">X_L = X_l + \sum_{i=1}^{L-1}F(X_i,W_i)</script><p>此时反向传播计算，假设损失函数是C，此时反向传播要对w求导，就先对后面一个节点求导，然后链式法则再求导W，即：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial X_l} = \frac{\partial C}{\partial X_L}*\frac{\partial X_L}{\partial X_l} =  \frac{\partial C}{\partial X_L}*(1+\frac{\partial}{\partial x} \sum_{i=1}^{L-1}F(X_i,W_i))</script><p>实际上X_l就是一个节点，比如 wx+b，所以对w、b求导就可以求出来了。</p><p>此时梯度里面多了一个1，而正是因为这个1的存在，使得梯度存在，可以很容易回流到千层，也就是说算出浅层 的梯度、更细参数。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAN3_手动实现网络生成动漫头像</title>
      <link href="/2020/03/01/%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0GAN%E7%BD%91%E7%BB%9C%E7%94%9F%E6%88%90%E5%8A%A8%E6%BC%AB%E5%A4%B4%E5%83%8F/"/>
      <url>/2020/03/01/%E6%89%8B%E5%8A%A8%E5%AE%9E%E7%8E%B0GAN%E7%BD%91%E7%BB%9C%E7%94%9F%E6%88%90%E5%8A%A8%E6%BC%AB%E5%A4%B4%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>学习了GAN的基本原理后，手动实现一个相当于GAN的 <code>hello，world</code> , 具体的工程见：</p><p><a href="https://github.com/tzwx/DeepLearning" target="_blank" rel="noopener">https://github.com/tzwx/DeepLearning</a> </p><p>这里主要分析一下代码逻辑，贴上训练代码。</p><p>写代码的时候发现自己很生疏了，所以总结一下流程</p><p>先上图：训练了69个epoch的结果，设定的时2500epoch</p><p><a href="https://user-images.githubusercontent.com/60562661/75624471-56cfed00-5bef-11ea-9518-2a8b9e6fc747.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/75624471-56cfed00-5bef-11ea-9518-2a8b9e6fc747.png" class="lazyload"></a></p><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><h3 id="定义网络结构"><a href="#定义网络结构" class="headerlink" title="定义网络结构"></a>定义网络结构</h3><ul><li>判别器就是几个卷积层（Conv）</li><li>生成器则是几个反卷积层（_deConv），最后接一个sigma函数分类</li></ul><h3 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">trans = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">.5</span>, <span class="number">.5</span>, <span class="number">.5</span>), (<span class="number">.5</span>, <span class="number">.5</span>, <span class="number">.5</span>))</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure></div><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><ul><li>数据预处理，数据要先经过以上代码</li><li>定义label，用1表示<strong>realimg</strong>，0表示<strong>fakeimg</strong>，维度就是<code>BATCH_SIZE</code></li><li>获取数据集，循环把100张图作为一个Batch，然后开始训练流程。</li></ul><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><ul><li>定义损失函数 <code>BCELoss</code> ，也就是二值的交叉熵</li><li>定义优化器Adam</li><li>定义学习率</li><li>优化器梯度清0</li><li>求得数据输出</li><li>用<code>loss function</code>计算损失loss</li><li>loss 反向传播</li><li>更新参数</li></ul><p>这个训练流程也是分类的流程。</p><h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>在训练过程中，判别器loss一直是比较小的(0.1,0.01,…)，而生成器loss很大，(7-11不等)，同时是一个互相博弈的过程，判别器loss减小时，生成器loss增加，反之亦然。</p><p><strong>因为之训练了69个epoch，所以看不到最终的比较优化的结果，最后的结果应该是判别器的loss很大，生成器的loss很小才算训练结束。</strong></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># @Time    : 2020/2/29 17:31</span></span><br><span class="line"><span class="comment"># @Author  : FRY--</span></span><br><span class="line"><span class="comment"># @FileName: train.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="comment"># @Blog    ：https://fryddup.github.io</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> data_loader <span class="keyword">import</span>  get_img</span><br><span class="line"><span class="keyword">from</span> net.net <span class="keyword">import</span> Generator</span><br><span class="line"><span class="keyword">from</span> net.net <span class="keyword">import</span> Discriminator</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line">G_LEARNING_RATE = <span class="number">0.0001</span></span><br><span class="line">D_LEARNING_RATE = <span class="number">0.0001</span></span><br><span class="line">EPOCHS = <span class="number">2500</span></span><br><span class="line"></span><br><span class="line">trans = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">.5</span>, <span class="number">.5</span>, <span class="number">.5</span>), (<span class="number">.5</span>, <span class="number">.5</span>, <span class="number">.5</span>))</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"><span class="comment"># Instance</span></span><br><span class="line">g = Generator().cuda()</span><br><span class="line">d = Discriminator().cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># define loss</span></span><br><span class="line">g_loss = nn.BCELoss() <span class="comment"># Binary crossEntropy</span></span><br><span class="line">d_loss = nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># define optimizer</span></span><br><span class="line">g_optimizer = torch.optim.Adam(g.parameters(),lr=G_LEARNING_RATE)</span><br><span class="line">d_optimizer = torch.optim.Adam(d.parameters(),lr=D_LEARNING_RATE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define labels</span></span><br><span class="line">label_real = torch.ones(BATCH_SIZE).cuda()</span><br><span class="line">label_fake = torch.zeros(BATCH_SIZE).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment">#all images</span></span><br><span class="line">images_truth = get_img()</span><br><span class="line">data_length = len(images_truth)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCHS):</span><br><span class="line">        <span class="comment"># shuffle</span></span><br><span class="line">        np.random.shuffle(images_truth)</span><br><span class="line">        images_real_loader = []</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(data_length):</span><br><span class="line">            count = count + <span class="number">1</span></span><br><span class="line">            images_real_loader.append(trans(images_truth[index]).numpy())</span><br><span class="line">            <span class="comment"># images_real_loader[100,3,96,96]</span></span><br><span class="line">            <span class="keyword">if</span> count == BATCH_SIZE:</span><br><span class="line">                count = <span class="number">0</span> <span class="comment"># reset</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Train Discriminator</span></span><br><span class="line">                <span class="comment"># train real data to gpu</span></span><br><span class="line">                <span class="comment"># if real -> d(real_img) = 1</span></span><br><span class="line">                images_real_loader_tensor = torch.Tensor(images_real_loader)</span><br><span class="line">                images_real_loader_tensor = images_real_loader_tensor.permute(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>).cuda()</span><br><span class="line">                images_real_loader.clear()</span><br><span class="line">                <span class="comment"># graddient _ zero</span></span><br><span class="line">                d_optimizer.zero_grad()</span><br><span class="line">                <span class="comment"># real image output_66</span></span><br><span class="line">                realimage_d = d(images_real_loader_tensor).squeeze() <span class="comment"># descent  [100，1，1，1] -> [100,1]</span></span><br><span class="line">                <span class="comment"># loss</span></span><br><span class="line">                d_realimg_loss = d_loss(realimage_d, label_real)</span><br><span class="line">                <span class="comment"># loss backward</span></span><br><span class="line">                d_realimg_loss.backward()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># train generate data</span></span><br><span class="line">                <span class="comment"># if generator d(generate_img) = 0</span></span><br><span class="line">                images_fake_loader = torch.randn(BATCH_SIZE, <span class="number">100</span>, <span class="number">1</span>, <span class="number">1</span>).cuda()</span><br><span class="line">                <span class="comment"># detach() g no gradient -> fix generator</span></span><br><span class="line">                images_fake_loader_tensor = g(images_fake_loader).detach()</span><br><span class="line">                fakeimg_d = d(images_fake_loader_tensor).squeeze()</span><br><span class="line">                d_fakeimg_loss = d_loss(fakeimg_d, label_fake)</span><br><span class="line">                d_fakeimg_loss.backward()</span><br><span class="line"></span><br><span class="line">                d_optimizer.step()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Train Generator</span></span><br><span class="line">                fake_data = torch.randn(BATCH_SIZE, <span class="number">100</span>, <span class="number">1</span>, <span class="number">1</span>).cuda()</span><br><span class="line">                g_optimizer.zero_grad()</span><br><span class="line">                generator_images = g(fake_data)</span><br><span class="line">                generator_images_score = d(generator_images).squeeze()</span><br><span class="line">                gen_loss = g_loss(generator_images_score, label_real)</span><br><span class="line">                gen_loss.backward()</span><br><span class="line">                g_optimizer.step()</span><br><span class="line"></span><br><span class="line">                print(<span class="string">"Current epoch:%d, Iteration: %d, Discriminator Loss: %f, Generator Loss: %f"</span></span><br><span class="line">                      % (epoch, (index//BATCH_SIZE)+<span class="number">1</span>,</span><br><span class="line">                         (d_realimg_loss+d_fakeimg_loss).detach().cpu().numpy(),</span><br><span class="line">                         gen_loss.detach().cpu().numpy()))</span><br><span class="line"></span><br><span class="line">                torch.save(g, <span class="string">"pkl"</span>+str(epoch)+<span class="string">"generator.pkl"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    train()</span><br></pre></td></tr></tbody></table></figure></div></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAN2_Basic_Theory</title>
      <link href="/2020/02/28/GAN-Basic-Theory/"/>
      <url>/2020/02/28/GAN-Basic-Theory/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>这篇文章主要来讨论下GAN基础的理论、公式推导。</p><h2 id="KL-散度"><a href="#KL-散度" class="headerlink" title="KL 散度"></a>KL 散度</h2><p><code>KL Divergence</code> 指的是相对熵，其用来衡量两个取值为正的函数或概率分布之间的差异；</p><p><code>相对熵 = 某个策略的交叉熵 - 信息熵（根据系统真实分布计算而得的信息熵，为最优策略）</code>  ,  公式如下:</p><script type="math/tex; mode=display">KL(P || Q) = H(P,Q) - H(P) = \sum_{k=1}^N P_k*log_2\frac {1} {Q_k} - \sum_{k=1}^N P_k*log_2\frac {1} {P_k} = \sum_{k=1}^N P_k*log_2\frac {P_k} {Q_k}</script><p>复习一下，交叉熵是使用非真实分布，信息熵是使用真实分布计算得到的。</p><h2 id="JS-散度"><a href="#JS-散度" class="headerlink" title="JS 散度"></a>JS 散度</h2><p><code>JS Divergence</code>  是 KL散度的变体，衡量两个分布之间的相似性。公式如下：</p><script type="math/tex; mode=display">JS(P||Q) = \frac 1 2 KL(P||\frac{P+Q}{2}) + \frac 1 2 KL(Q||\frac{P+Q}{2})</script><h2 id="Formula-Of-GAN"><a href="#Formula-Of-GAN" class="headerlink" title="Formula Of GAN"></a>Formula Of GAN</h2><p>首先，生成器要做的事情就是生成一个最靠近真实data的分布，这在之前可以用最大似然估计来找这样的一个分布，最大似然估计实际上可以与KL散度扯上关系：</p><p><a href="https://user-images.githubusercontent.com/60562661/75565315-af38aa80-5a88-11ea-9673-fcc477934c05.png" data-fancybox="group" data-caption="1582900061983" class="fancybox"><img alt="1582900061983" style="zoom:80%;" title="1582900061983" data-src="https://user-images.githubusercontent.com/60562661/75565315-af38aa80-5a88-11ea-9673-fcc477934c05.png" class="lazyload"></a></p><p>这里面需要注意的是<strong>约等于</strong> 这一步，意思是最大值求和m笔data的概率其实就是求x服从于真实分布的情况下概率的最大值，所以就 = 下面的积分，而下面的积分后边减去的一项不影响求最大值，所以再意思上是相等的，但是真实值上并不相等。然后上面的公式主要是来说明：</p><p><code>最大化最大似然估计概率就是最小化KL散度</code>，即：</p><script type="math/tex; mode=display">G^* = arg\underset G min Div(P_G,P_{data})</script><p>所以要训练<strong>Generator</strong>，也就是要计算出真实数据和生成数据之间的散度，而<strong>Discriminator</strong>则可以做到这一件事。</p><p>在训练Discriminator时，在固定住Generator的情况下进行，定义对象方程：</p><p><a href="https://user-images.githubusercontent.com/60562661/75565318-afd14100-5a88-11ea-8c7f-a035df1a9034.png" data-fancybox="group" data-caption="1582901609500" class="fancybox"><img alt="1582901609500" title="1582901609500" data-src="https://user-images.githubusercontent.com/60562661/75565318-afd14100-5a88-11ea-8c7f-a035df1a9034.png" class="lazyload"></a></p><p>这里后面一项之所以要定义为1-D(x),是因为判别器做的事情是给Pdata高分，给Pg低分，也就是使得生成的数据和真实的数据分布的散度最大。因此在x服从Pg分布时，写1-D(x) 就可以最大化这个式子，比较容易train，即：</p><script type="math/tex; mode=display">D^* = arg\underset D max V(D,G)</script><p>实际上，当生成的图像和真实图像散度比较大，相似度比较小时，判别器是很好训练的，而相似度比较大就比较难训练，所以训练好的判别器其实就是在最大化生成的图像和真实图像的散度。下面证明这一项就是在最大化JS 散度：</p><p><a href="https://user-images.githubusercontent.com/60562661/75565319-b069d780-5a88-11ea-8f2a-a9497fe42922.png" data-fancybox="group" data-caption="1582902722545" class="fancybox"><img alt="1582902722545" style="zoom: 40%;" title="1582902722545" data-src="https://user-images.githubusercontent.com/60562661/75565319-b069d780-5a88-11ea-8f2a-a9497fe42922.png" class="lazyload"></a></p><p><a href="https://user-images.githubusercontent.com/60562661/75565323-b19b0480-5a88-11ea-9fe5-14395438c926.png" data-fancybox="group" data-caption="1582902806605" class="fancybox"><img alt="1582902806605" style="zoom: 90%;" title="1582902806605" data-src="https://user-images.githubusercontent.com/60562661/75565323-b19b0480-5a88-11ea-9fe5-14395438c926.png" class="lazyload"></a></p><p><a href="https://user-images.githubusercontent.com/60562661/75565327-b2cc3180-5a88-11ea-993a-4992d7b378dd.png" data-fancybox="group" data-caption="1582902903882" class="fancybox"><img alt="1582902903882" style="zoom:90%;" title="1582902903882" data-src="https://user-images.githubusercontent.com/60562661/75565327-b2cc3180-5a88-11ea-993a-4992d7b378dd.png" class="lazyload"></a></p><p><a href="https://user-images.githubusercontent.com/60562661/75565330-b364c800-5a88-11ea-92e3-38aadf080966.png" data-fancybox="group" data-caption="1582902953241" class="fancybox"><img alt="1582902953241" style="zoom:90%;" title="1582902953241" data-src="https://user-images.githubusercontent.com/60562661/75565330-b364c800-5a88-11ea-92e3-38aadf080966.png" class="lazyload"></a></p><p>这里最后一步可以参考一开始文章提到的JS散度对比一下公式就会明白。所以此时：</p><script type="math/tex; mode=display">G^* = arg\underset G min (\space \underset D maxV(D,G))</script><p>此时梯度下降法求解G*:</p><p><a href="https://user-images.githubusercontent.com/60562661/75565333-b495f500-5a88-11ea-83a5-f46a4ee2b53b.png" data-fancybox="group" data-caption="1582904545756" class="fancybox"><img alt="1582904545756" style="zoom: 90%;" title="1582904545756" data-src="https://user-images.githubusercontent.com/60562661/75565333-b495f500-5a88-11ea-83a5-f46a4ee2b53b.png" class="lazyload"></a></p><p>这里的max取微分可以看作分段函数求微分。所以现在已经可以铜鼓哦梯度下降法来更新生成器了，然后进行算法：</p><p><a href="https://user-images.githubusercontent.com/60562661/75565339-b52e8b80-5a88-11ea-8b7c-9caa8bed1e9e.png" data-fancybox="group" data-caption="1582905215369" class="fancybox"><img alt="1582905215369" title="1582905215369" data-src="https://user-images.githubusercontent.com/60562661/75565339-b52e8b80-5a88-11ea-8b7c-9caa8bed1e9e.png" class="lazyload"></a></p><p><strong>这里有一个问题，事实上更新G1参数真的是在减小JS散度吗？</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/75565340-b65fb880-5a88-11ea-8dcd-a5366afdb4bb.png" data-fancybox="group" data-caption="1582905319178" class="fancybox"><img alt="1582905319178" title="1582905319178" data-src="https://user-images.githubusercontent.com/60562661/75565340-b65fb880-5a88-11ea-8dcd-a5366afdb4bb.png" class="lazyload"></a></p><p>举个例子如上图，在G更新后，G的分布已经变了(由左到右)，此时的D0<em>依然是固定的未发生变化，因此此时的D0\</em>已经不再是max值了，因此此时<code>（伪）max V(G,D)</code>  便不再是JS散度了。所以有个假设，G的变化不能太大，每次更新一点点，否则误差会过大。</p><p><strong>以是就是GAN的基础理论公式推导。</strong></p><h2 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h2><p><a href="https://user-images.githubusercontent.com/60562661/75565341-b6f84f00-5a88-11ea-935e-af64fc9f2ab8.png" data-fancybox="group" data-caption="1582905751928" class="fancybox"><img alt="1582905751928" title="1582905751928" data-src="https://user-images.githubusercontent.com/60562661/75565341-b6f84f00-5a88-11ea-935e-af64fc9f2ab8.png" class="lazyload"></a></p><p>最后整体回顾一遍算法流程：</p><p><a href="https://user-images.githubusercontent.com/60562661/75565342-b8297c00-5a88-11ea-8dd5-6d115ff21f5a.png" data-fancybox="group" data-caption="1582905847010" class="fancybox"><img alt="1582905847010" title="1582905847010" data-src="https://user-images.githubusercontent.com/60562661/75565342-b8297c00-5a88-11ea-8dd5-6d115ff21f5a.png" class="lazyload"></a></p><p><strong>注意的是一般Generator只训练一次 </strong>。原因前面说了 。</p><p>以上就是GAN 对抗网络的 Basic Theory，还有其他的一些之后的细节可以看李宏毅老师的视频。有错误欢迎指正！</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAN对抗网络1_初识</title>
      <link href="/2020/02/26/GAN%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E5%88%9D%E8%AF%86/"/>
      <url>/2020/02/26/GAN%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C%E5%88%9D%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="GAN基本原理"><a href="#GAN基本原理" class="headerlink" title="GAN基本原理"></a>GAN基本原理</h2><p>参考链接：<a href="https://www.bilibili.com/video/av24011528?from=search&seid=17842693444993179137" target="_blank" rel="noopener">https://www.bilibili.com/video/av24011528?from=search&seid=17842693444993179137</a></p><p>首先，GAN全称是<strong>Generative Adversarial Nets</strong> ，一般叫对抗网络。由两部分组成：<code>Generator(生成器)</code>、<code>Discrimator(判别器)</code>。</p><ul><li><strong>Generator</strong></li></ul><p>生成器可以认为就是一个<code>Neural Network</code>。以CV为例，现在的目标是生成图片，则生成器的工作就是输入一个vector，然后输出一张图像(动漫头像)，其中某一维可能对应头发长短，另一维对应眼睛颜色，这样通过调整向量就可以控制自己需要的头像，类似于下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/75459614-eccf0200-59ba-11ea-97b2-7606b7c07d8a.png" data-fancybox="group" data-caption="1582812383567" class="fancybox"><img alt="1582812383567" style="zoom: 80%;" title="1582812383567" data-src="https://user-images.githubusercontent.com/60562661/75459614-eccf0200-59ba-11ea-97b2-7606b7c07d8a.png" class="lazyload"></a></p><ul><li><strong>Discrimator</strong></li></ul><p>判别器也是一个<code>Neural Network</code>,  顾名思义，是判断生成器生成的东西的好坏，输入是一张<code>image(动漫头像)</code>, 输出一个<code>scalar(标量，数字)</code> , 衡量生成器生成的image的好坏程度。如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/75459993-db3a2a00-59bb-11ea-9adb-38203a3cb3c2.png" data-fancybox="group" data-caption="1582813278331" class="fancybox"><img alt="1582813278331" style="zoom:80%;" title="1582813278331" data-src="https://user-images.githubusercontent.com/60562661/75459993-db3a2a00-59bb-11ea-9adb-38203a3cb3c2.png" class="lazyload"></a></p><h3 id="GAN算法流程-Algorithm"><a href="#GAN算法流程-Algorithm" class="headerlink" title="GAN算法流程 Algorithm"></a>GAN算法流程 Algorithm</h3><ol><li>初始化 Generator 、Discrimator</li><li>在每一个迭代中：</li></ol><ul><li>固定生成器，训练判别器，生成器目标是欺骗过判别器，也就是生成器生成的图片判别器给出的分数越高越好</li><li>固定判别器，训练生成器</li><li>重复以上步骤，判别器和生成器在同时进行学习，同时进化，相互博弈，直到生成器能瞒过判别器就算是训练好了</li></ul><h2 id="Structured-Learning"><a href="#Structured-Learning" class="headerlink" title="Structured Learning"></a>Structured Learning</h2><p>结构化学习是很有挑战性的，例如生成图像，很重要的一个东西是像素之间的关系，像素本身是没有什么错误的，但是它们之间的关系很重要。</p><ul><li>Generator 是先画每一个部件，最后形成一幅 image；</li></ul><ul><li>Discrimator 则是整体上判断图像好不好，并且可以挑选出来最好的图像；</li></ul><p>两者结合起来就是GAN对抗网络了，可以发挥各自的性能，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/75460000-de351a80-59bb-11ea-9c3f-1378a8ebfa19.png" data-fancybox="group" data-caption="1582815125699" class="fancybox"><img alt="1582815125699" style="zoom:80%;" title="1582815125699" data-src="https://user-images.githubusercontent.com/60562661/75460000-de351a80-59bb-11ea-9c3f-1378a8ebfa19.png" class="lazyload"></a></p><h2 id="Q1-为什么Generator-生成器-不自己学习呢？"><a href="#Q1-为什么Generator-生成器-不自己学习呢？" class="headerlink" title="Q1:为什么Generator(生成器)不自己学习呢？"></a>Q1:为什么Generator(生成器)不自己学习呢？</h2><p>首先生成器是输入一个vector，输出是image，这和<code>Auto Encoder</code> (自编码器)技术中的<code>Decoder(解码器)</code>工作室一样的，也就是说Decoder就是一个Generator。</p><p>Auto Encoder自编码器是有缺陷的，也就是说一张图片只是对应到一个点，从这个点解码出图像，如果取一个没有出现过的点进行解码，解码出来的就是noise了，也就是说只能解码出现过的，因此出现了VAE技术：</p><p><a href="https://user-images.githubusercontent.com/60562661/75460012-e0977480-59bb-11ea-809f-538c06783ee0.png" data-fancybox="group" data-caption="1582815415458" class="fancybox"><img alt="1582815415458" style="zoom:80%;" title="1582815415458" data-src="https://user-images.githubusercontent.com/60562661/75460012-e0977480-59bb-11ea-809f-538c06783ee0.png" class="lazyload"></a></p><p>也就是在编码图像时添加了噪音等，这样采样任何一个点都会解码出来比较真实的图像。在这两种技术中都有<code>Decoder</code>, 那么这种技术缺少了什么？</p><p>对于生成器来说，部件之间的关系很重要，而在解码器中神经元相互之间是没有关系的，也就是部件之间很难产生联系，这就直接导致很容易失去大局观，整体性就比较差。</p><p>关于部件之间的练习，可以看这个例子：</p><p><a href="https://user-images.githubusercontent.com/60562661/75460304-45eb6580-59bc-11ea-9cf3-80dfedd688cc.png" data-fancybox="group" data-caption="1582816190206" class="fancybox"><img alt="1582816190206" style="zoom:80%;" title="1582816190206" data-src="https://user-images.githubusercontent.com/60562661/75460304-45eb6580-59bc-11ea-9cf3-80dfedd688cc.png" class="lazyload"></a></p><p>人眼来判断的话当然觉得下面看两幅图比较好，但是Decoder则会认为上面两幅图比较好，因为逐像素比较确实是前两幅图误差小，这就很容易看出来没有考虑到整体性。事实上Decoder很难做到这一点。</p><h2 id="Q2-为什么Discrimator-判别器-不自己生成呢？"><a href="#Q2-为什么Discrimator-判别器-不自己生成呢？" class="headerlink" title="Q2:为什么Discrimator(判别器)不自己生成呢？"></a>Q2:为什么Discrimator(判别器)不自己生成呢？</h2><p>判别器既然可以判断生成的图像好不好，为什么不自己生成呢？</p><p><a href="https://user-images.githubusercontent.com/60562661/75460313-4a178300-59bc-11ea-963b-f7f8f4e5e992.png" data-fancybox="group" data-caption="1582816859692" class="fancybox"><img alt="1582816859692" title="1582816859692" data-src="https://user-images.githubusercontent.com/60562661/75460313-4a178300-59bc-11ea-963b-f7f8f4e5e992.png" class="lazyload"></a></p><p>还是用上面的例子，比如判别器可能有这样一个卷积核，即中心有颜色其他地方没有，看到这样的就给图像低分，判别器很容易判断图像整体性。</p><p>事实上判别器就像是批评家，比较形象的可以说是<strong>“键盘侠”</strong>，不管你说什么他只给说哪里不好，你要问他什么是好的他可能说不出来这样子。严密的论证可以看文章开始的视频链接讲到的，简单来说就是：</p><ol><li>训练判别器首先不能只是正样本，也需要一些负样本，而我们手上是没有负样本的，因此需要生成负样本</li><li>要产生比较好的负样本就需要一个比较好的判别器才能找到比较好的负样本</li></ol><p>因此，就陷入了鸡生蛋、蛋生鸡的问题。因此训练单独的一个判别器首先要假定已经有比较好的负样本。训练流程类似，可以看视频讲解。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>总结一下，生成器和判别器各自的优势劣势：</p><p><a href="https://user-images.githubusercontent.com/60562661/75460318-4ab01980-59bc-11ea-9ff5-b426c48d3c76.png" data-fancybox="group" data-caption="1582817820048" class="fancybox"><img alt="1582817820048" title="1582817820048" data-src="https://user-images.githubusercontent.com/60562661/75460318-4ab01980-59bc-11ea-9ff5-b426c48d3c76.png" class="lazyload"></a></p><h2 id="Additional"><a href="#Additional" class="headerlink" title="Additional"></a>Additional</h2><ol><li>上面提到GAN训练时要先训练判别器，这是为什么？后面文章会有公式理论说明这个问题。</li><li>上面提到的 <code>Auto Encoder</code> <code>VAE</code> 这两种编码技术随后文章会详细讨论细节问题。</li></ol></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cross Entropy 的前世今生</title>
      <link href="/2020/02/26/Cross-Entropy-%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/"/>
      <url>/2020/02/26/Cross-Entropy-%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>Deep Learning 中可以分为分类问题和回归问题，而分类问题也可以转化为回归问题，具体可以看<a href="https://huaqi.blue/2020/02/25/Classification/#%E5%88%86%E7%B1%BB%E4%B8%8E%E5%9B%9E%E5%BD%92%E7%9A%84%E8%BD%AC%E5%8C%96" target="_blank" rel="noopener">Classification</a> 这篇文章。回归问题一般用均方误差作为损失函数(显而易见)，而分类问题一般则用交叉熵作为损失函数，这是为什么？这一切要从<strong>信息熵</strong>说起…</p><h2 id="信息熵-amp-交叉熵"><a href="#信息熵-amp-交叉熵" class="headerlink" title="信息熵&交叉熵"></a>信息熵&交叉熵</h2><p><a href="https://www.zhihu.com/question/41252833/answer/195901726" target="_blank" rel="noopener">https://www.zhihu.com/question/41252833/answer/195901726</a> 这篇知乎文章很通俗的解释了交叉熵的概念，总结一下：</p><p>信息论中，使用信息熵来对概率分布进行量化。<strong>信息熵代表的是随机变量或整个系统的不确定性，熵越大，随机变量或系统的不确定性就越大。</strong>根据真实分布，我们能够找到一个最优策略，以最小的代价消除系统的不确定性，而这个代价大小就是信息熵，记住，信息熵衡量了系统的不确定性，而我们要消除这个不确定性，所要付出的【最小努力】（猜题次数、编码长度等）的大小就是信息熵。</p><p>但是并不是每次都知道真实分布，在深度学习中是不知道真实的分布的，这时候就引入交叉熵：<strong>交叉熵，其用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小</strong>。</p><p>交叉熵公式如下：</p><script type="math/tex; mode=display">\sum_{k=1}^Np_k * log_2^{\frac 1q_k}</script><p>其中 <a href="https://www.zhihu.com/equation?tex=p_k" data-fancybox="group" data-caption="[公式]" class="fancybox"><img alt="[公式]" title="[公式]" data-src="https://www.zhihu.com/equation?tex=p_k" class="lazyload"></a> 表示真实分布， <a href="https://www.zhihu.com/equation?tex=q_k" data-fancybox="group" data-caption="[公式]" class="fancybox"><img alt="[公式]" title="[公式]" data-src="https://www.zhihu.com/equation?tex=q_k" class="lazyload"></a> 表示非真实分布。交叉熵最小值就是采用真实分布预测出来的结果，此时</p><p><code>交叉熵 == 信息熵</code> , 因此在机器学习中要最小化交叉熵，此时就最近真最佳策略。</p><h2 id="深度学习中的交叉熵"><a href="#深度学习中的交叉熵" class="headerlink" title="深度学习中的交叉熵"></a>深度学习中的交叉熵</h2><p>举一个最简单的例子，<code>y = w * x + b</code>，目标是输入为1输出为0.经过 σ 函数进行二分类,首先定义损失函数(先用均方误差)：</p><script type="math/tex; mode=display">C = (y-a)^2/2</script><p>其中y为真实值，a为我们算出来的值，因为要经过 σ 函数进行分类，所以 a = σ (w*x + b),令 z =  w*x + b,a = σ(z)，用梯度下降法求w，b就需要先对其求微分，用到链式求导法则：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial w} = \frac{\partial C}{\partial a} *\frac{\partial a}{\partial w} \\\partial C / \partial a = a-y\\\partial a / \partial w =  \sigma'(z)*x</script><p>此时设x=1，相应的y=0，</p><script type="math/tex; mode=display">\partial C / \partial w = a *\sigma'(z)\\\partial C / \partial b = a *\sigma'(z)</script><p>因为经过了sigmoid函数，所以当z较大时，梯度很小，此时会发生梯度弥散,更新参数会很慢，那么如何加快参数更新？交叉熵是凭空出现的吗?若果没有σ这项：</p><script type="math/tex; mode=display">\partial C / \partial w_i = (a-y)*x_i\\\partial C / \partial b = a-y</script><p>考虑之前的推导：</p><script type="math/tex; mode=display">\partial C / \partial w_i = \frac {\partial C}{\partial a} *\sigma'(z)x_i=\frac {\partial C}{\partial a} *a(1-a)x_i</script><p>令（6）和（5）相等可得出:</p><script type="math/tex; mode=display">\partial C / \partial a = \frac {a-y}{a(1-a)}</script><p>此时用积分求出原函数即：</p><script type="math/tex; mode=display">C = -[y\ln a+(1-y)\ln (1-a)] + constant</script><p>这就是二分类中交叉熵的函数形式，伯努利二项分布。</p><p>由此可以知道交叉熵的来源、推导。也解释了分类问题为什么用交叉熵作为损失函数，简而言之，交叉熵作为损失函数，消去了σ这一项，从而变成线性的，解决了更新参数很慢的问题，这时候再引用一张之前的图就一目了然了：</p><p><a href="https://user-images.githubusercontent.com/60562661/74258385-2f44dd80-4d31-11ea-907b-5b5c2bc25f78.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74258385-2f44dd80-4d31-11ea-907b-5b5c2bc25f78.png" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 交叉熵 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分类到回归的过程</title>
      <link href="/2020/02/25/Classification/"/>
      <url>/2020/02/25/Classification/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>DeepLearning中另外一个常见的问题就是分类问题，这里以二分类为例。</p><p><strong>做二分类可不可以直接当作回归问题来处理呢？</strong>例如人为规定 calss1 对应输出值为1， class2对应值为-1，然后以均方误差为loss，做损失函数，这样直接train是可以train出来的，但是这样有一个问题，回归损失函数会惩罚比较大的正确项，例如一个数得出来结果是10远大于1，但是它和1明显是一类，而用回归做的话就会尽量减小这一loss，导致出现错误的结果，如下图：本来绿色的线是正确的分类，但是因为回归的惩罚会导致偏向紫色。 这是因为回归和分类的判断标准不一样。</p><p><a href="https://user-images.githubusercontent.com/60562661/75210259-88921f80-57bb-11ea-9985-19f163a398c7.png" data-fancybox="group" data-caption="1582596902251" class="fancybox"><img alt="1582596902251" style="zoom: 67%;" title="1582596902251" data-src="https://user-images.githubusercontent.com/60562661/75210259-88921f80-57bb-11ea-9985-19f163a398c7.png" class="lazyload"></a></p><h2 id="分类流程"><a href="#分类流程" class="headerlink" title="分类流程"></a>分类流程</h2><p><a href="https://user-images.githubusercontent.com/60562661/75210541-5208d480-57bc-11ea-8f59-6ba28470aa88.png" data-fancybox="group" data-caption="1582597091411" class="fancybox"><img alt="1582597091411" style="zoom:80%;" title="1582597091411" data-src="https://user-images.githubusercontent.com/60562661/75210541-5208d480-57bc-11ea-8f59-6ba28470aa88.png" class="lazyload"></a></p><ol><li>这里的model直接用的贝叶斯求出概率</li><li>假设x服从高斯分布，此时要求出  <strong>μ，σ</strong>  用最大似然估计使得这个高斯分布所采样出来的这些点的概率最大</li><li>求解最优的μ，σ。<strong>μ 就是一组书的均值mean</strong>  ，  μ，σ都有对应公式，也可以根据微分来求，如下图：</li></ol><p><a href="https://user-images.githubusercontent.com/60562661/75210260-8a5be300-57bb-11ea-9423-008452e862f7.png" data-fancybox="group" data-caption="1582597521938" class="fancybox"><img alt="1582597521938" title="1582597521938" data-src="https://user-images.githubusercontent.com/60562661/75210260-8a5be300-57bb-11ea-9423-008452e862f7.png" class="lazyload"></a></p><h2 id="分类与回归的转化"><a href="#分类与回归的转化" class="headerlink" title="分类与回归的转化"></a>分类与回归的转化</h2><p>这里用图片截图出所有的公式推导，可以从第一张图直接跳到最后一张图看结论</p><p><a href="https://user-images.githubusercontent.com/60562661/75210261-8af47980-57bb-11ea-873f-1021dc6ebb25.png" data-fancybox="group" data-caption="1582597604571" class="fancybox"><img alt="1582597604571" style="zoom: 67%;" title="1582597604571" data-src="https://user-images.githubusercontent.com/60562661/75210261-8af47980-57bb-11ea-873f-1021dc6ebb25.png" class="lazyload"></a></p><p><a href="https://user-images.githubusercontent.com/60562661/75210263-8b8d1000-57bb-11ea-8165-8be93e1efb95.png" data-fancybox="group" data-caption="1582597627023" class="fancybox"><img alt="1582597627023" style="zoom:67%;" title="1582597627023" data-src="https://user-images.githubusercontent.com/60562661/75210263-8b8d1000-57bb-11ea-8165-8be93e1efb95.png" class="lazyload"></a></p><p><a href="https://user-images.githubusercontent.com/60562661/75210265-8c25a680-57bb-11ea-9849-26770bf8584c.png" data-fancybox="group" data-caption="1582597710255" class="fancybox"><img alt="1582597710255" style="zoom:67%;" title="1582597710255" data-src="https://user-images.githubusercontent.com/60562661/75210265-8c25a680-57bb-11ea-9849-26770bf8584c.png" class="lazyload"></a></p><p><a href="https://user-images.githubusercontent.com/60562661/75210268-8d56d380-57bb-11ea-943d-fac3d39d44f8.png" data-fancybox="group" data-caption="1582597747533" class="fancybox"><img alt="1582597747533" style="zoom:67%;" title="1582597747533" data-src="https://user-images.githubusercontent.com/60562661/75210268-8d56d380-57bb-11ea-943d-fac3d39d44f8.png" class="lazyload"></a></p><p><a href="https://user-images.githubusercontent.com/60562661/75210270-8def6a00-57bb-11ea-89c0-bfd800f1c1c0.png" data-fancybox="group" data-caption="1582597839955" class="fancybox"><img alt="1582597839955" style="zoom: 37%;" title="1582597839955" data-src="https://user-images.githubusercontent.com/60562661/75210270-8def6a00-57bb-11ea-89c0-bfd800f1c1c0.png" class="lazyload"></a></p><p><strong>也就是说分类函数经过一系列的转换，最终变成了线性函数，即 <code>P(C1|X) = σ(w * x + b)</code>,之前的方法我们要求五个参数才可以计算到最终结果，现在我们可以直接计算w，b就可以得到最终的概率！此时就变成了回归问题。然后就有了后面的逻辑回归，用交叉熵求出w，b</strong></p><p><strong>同时以上公式也说明了为什么 σ函数可以用来分类!</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>梯度下降法 ③_数学公式起源深入</title>
      <link href="/2020/02/24/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E4%B8%89/"/>
      <url>/2020/02/24/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E4%B8%89/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>Gradient Descent 前面的文章讲了基本的流程，可以知道基本的梯度下降法更新参数流程是 <code>w_new = w_origin - のL/のw</code>， 这里为什么是负号呢？为甚要乘以梯度方向呢？这一切要从<strong>泰勒级数</strong>说起。</p><h2 id="Taylor-Series"><a href="#Taylor-Series" class="headerlink" title="Taylor Series"></a>Taylor Series</h2><p>假设函数 h(x) 在 x = x0 处<strong>无限可导</strong>，那么h(x)可以展开成如下：其中，x很接近x0时，后面的高次项边都可以忽略，因此约等于前两项。</p><p><a href="https://user-images.githubusercontent.com/60562661/75151086-d0c02c00-5740-11ea-99de-91d92c032776.png" data-fancybox="group" data-caption="1582543708651" class="fancybox"><img alt="1582543708651" style="zoom:80%;" title="1582543708651" data-src="https://user-images.githubusercontent.com/60562661/75151086-d0c02c00-5740-11ea-99de-91d92c032776.png" class="lazyload"></a></p><p>上图是泰勒级数在只有<strong>一个变量</strong>时的展开式，同样多个变量也可以展开：</p><p><a href="https://user-images.githubusercontent.com/60562661/75151091-d3228600-5740-11ea-94dc-c6122f01fd81.png" data-fancybox="group" data-caption="1582543838152" class="fancybox"><img alt="1582543838152" style="zoom: 80%;" title="1582543838152" data-src="https://user-images.githubusercontent.com/60562661/75151091-d3228600-5740-11ea-94dc-c6122f01fd81.png" class="lazyload"></a></p><h2 id="Based-Taylor-Series"><a href="#Based-Taylor-Series" class="headerlink" title="Based Taylor Series"></a>Based Taylor Series</h2><p><a href="https://user-images.githubusercontent.com/60562661/75151093-d3bb1c80-5740-11ea-86f9-ea1c98811b0b.png" data-fancybox="group" data-caption="1582544427626" class="fancybox"><img alt="1582544427626" style="zoom:80%;" title="1582544427626" data-src="https://user-images.githubusercontent.com/60562661/75151093-d3bb1c80-5740-11ea-86f9-ea1c98811b0b.png" class="lazyload"></a></p><p>如图，在点(a,b)处取一个足够小的圆，此时损失函数 L(θ) 便可以根据泰勒公式在(a,b)处展开，其中 s，u，v, 三项均为常数。u是L在θ1方向的偏导数，v是L在θ2方向的偏导数。</p><p><a href="https://user-images.githubusercontent.com/60562661/75151095-d453b300-5740-11ea-8f7b-016c1949553c.png" data-fancybox="group" data-caption="1582544740168" class="fancybox"><img alt="1582544740168" style="zoom:80%;" title="1582544740168" data-src="https://user-images.githubusercontent.com/60562661/75151095-d453b300-5740-11ea-8f7b-016c1949553c.png" class="lazyload"></a></p><p>此时要最小化L，把上图公式中 <strong>θ 1 -  a </strong>用▲θ1表示，<strong>θ 2 - b </strong>用▲θ2表示，s与θ无关因此可以忽略，公式L 看起来就比较简单了，可以看成是 vector(u, v)  与  vector(▲θ1, ▲θ2) 的内积，则当(▲θ1, ▲θ2)处于圆上(长度最大)，方向相反(cos  = -1)时内积最小，因此可以得到图中最下面的推导公式，乘以 <strong>-n</strong> 表示的是比例，使得(▲θ1, ▲θ2)位于圆上，<strong>负号</strong>则是与(u, v) 方向相反。将(▲θ1, ▲θ2)换算之后便会得出通常的更新参数的公式，因此得出了文章最初的<code>w_new = w_origin - n*のL/のw</code>，因此梯度方向下降最快，要乘以负的学习率。</p><p><strong>至此梯度下降法的来源也都已清楚！</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 梯度下降法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>斐波那契数列几种实现方式</title>
      <link href="/2020/02/23/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E9%A2%9D%E6%95%B0%E5%88%97%E5%87%A0%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/"/>
      <url>/2020/02/23/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E9%A2%9D%E6%95%B0%E5%88%97%E5%87%A0%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>今天在刷题时碰到了斐波那契额数列，然后发现有点忘记了，来记录一下几种实现方式。</p><h2 id="递归法"><a href="#递归法" class="headerlink" title="递归法"></a>递归法</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Fibonacci</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> i <= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> i</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> Fibonacci(i<span class="number">-1</span>) + Fibonacci(i<span class="number">-2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    print(Fibonacci(<span class="number">100</span>))</span><br></pre></td></tr></tbody></table></figure></div><p>这种方法非常慢，有大量重复计算，复杂度是指数级</p><h2 id="递推式"><a href="#递推式" class="headerlink" title="递推式"></a>递推式</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Fibonacci</span><span class="params">(i)</span>:</span></span><br><span class="line">   a, b = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(i):</span><br><span class="line">       a, b = b, a+b</span><br><span class="line">   <span class="keyword">return</span> a</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">39</span>):</span><br><span class="line">        print(Fibonacci(i))</span><br></pre></td></tr></tbody></table></figure></div><p>该方法复杂度是线性的</p><h2 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Fibonacci</span><span class="params">(n)</span>:</span></span><br><span class="line">   a, b = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">   <span class="keyword">while</span>(n><span class="number">0</span>):</span><br><span class="line">       a, b = b, a+b</span><br><span class="line">       <span class="keyword">yield</span> a</span><br><span class="line">       n = n - <span class="number">1</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> Fibonacci(<span class="number">10</span>):</span><br><span class="line">        print(i)</span><br></pre></td></tr></tbody></table></figure></div><p>暂时记录三种方法。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 算法刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 斐波那契数列 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Why Deep</title>
      <link href="/2020/02/23/Why-Deep/"/>
      <url>/2020/02/23/Why-Deep/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>相对于机器学习，深度学习为什么要 <strong>Deep</strong> 呢？</p><p>直觉上是因为神经网络层数越多，参数就越多，拟合能力越强，表现也就更好，但是事实上并不是这样。</p><p>要对比<strong>Deep</strong>与<strong>Shallow</strong>  ，首先要保证他们的参数两是一样的，这样对比才比较公平，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/75113138-f5a39900-5685-11ea-8aa6-e8668f869b82.png" data-fancybox="group" data-caption="1582463394370" class="fancybox"><img alt="1582463394370" style="zoom: 50%;" title="1582463394370" data-src="https://user-images.githubusercontent.com/60562661/75113138-f5a39900-5685-11ea-8aa6-e8668f869b82.png" class="lazyload"></a></p><p>更深的网络模型确实是有更好的表现，误差会更低。</p><p>事实上更加Deep，有更多的 <strong>Hidden Layer</strong>  ，是在做<code>模组化(Modularization)</code> 这件事，这和模块化、函数式编程很相似，也就是说每层有具体的功能，例如：</p><ul><li>第一层的神经元是比较基础的分类器；第二层则是比较高级的…</li><li>其中第二层以第一层的输出作为输入，第三层以第二层的输出作为输入，以此类推…</li><li>模组化是机器自己学习出来的，模组化可以想象CNN —比较底层可以识别细节信息，高层则可以更好的识别语义信息</li></ul><p><a href="https://user-images.githubusercontent.com/60562661/75113140-05bb7880-5686-11ea-81c6-421e7cc536fd.png" data-fancybox="group" data-caption="1582463874502" class="fancybox"><img alt="1582463874502" title="1582463874502" data-src="https://user-images.githubusercontent.com/60562661/75113140-05bb7880-5686-11ea-81c6-421e7cc536fd.png" class="lazyload"></a></p><p>做<code>Modularization</code>  这件事的好处就是网络可以举一反三，不再需要很多很多的<code>data</code>.</p><p>有一种流行的说法：<code>AI = BigData + DeepLearning</code>  大数据与深度学习相结合所以可以做识别等任务(<strong>深度学习需要很大的数据支撑</strong>)，恰恰相反，深度学习只需要更少的Data，这里的<strong>BigData </strong> 即使有100g，1000g 依然不能包含所有的Data，因此才需要<code>DeepLearning</code>来举一反三 ，如果真的有能包含所有东西的大数据那根本不需要深度学习这件事了，直接做标签分类就实现了。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Batch Normalization</title>
      <link href="/2020/02/22/Batch-Normalization/"/>
      <url>/2020/02/22/Batch-Normalization/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><p>Batch Normalization 批标准化 ，在传统的深度学习没有批标准化时，存在以下一些问题：</p><ul><li><strong>难以训练、拟合</strong>   例如下图，x1值为比较小，x2则很大， 整体公式是 <code>a = w1*x1 + w2*x2 + b</code> ,此时就是图中左下角的图，w1影响就非常小了，相应的w1方向梯度就很小，w2方向梯度变化就非常大，这会使得模型难以训练、难以拟合。</li></ul><p><a href="https://user-images.githubusercontent.com/60562661/75095600-9be19700-55d1-11ea-86e1-f81c5df25f8b.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:40%;" data-src="https://user-images.githubusercontent.com/60562661/75095600-9be19700-55d1-11ea-86e1-f81c5df25f8b.png" class="lazyload"></a></p><ul><li><p><strong>Internal Covariate Shift 内部协变量偏移</strong> </p><p>神经网络有很多的隐藏层，每层都是以前一层的输出作为自己的输入，因此要对每一层做<code>Feature Scaling</code>, 这样会比较有效的解决该问题。</p><p>Internal Covariate Shift问题 大致意思是强一层便宜会对后续产生很大的影响，每层的输入如果不做特征缩放，很大很小模型就需要一直调整去适应，就需要把每层的输入都固定下来</p></li></ul><h2 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h2><p>这种方法比较简单，对于每一维数据，计算平均数、标准差，然后每个数减去平均数除以标准差即可标准化。</p><p><a href="https://user-images.githubusercontent.com/60562661/75095605-a8fe8600-55d1-11ea-967a-7596e215d084.png" data-fancybox="group" data-caption="1582385736895" class="fancybox"><img alt="1582385736895" style="zoom: 67%;" title="1582385736895" data-src="https://user-images.githubusercontent.com/60562661/75095605-a8fe8600-55d1-11ea-967a-7596e215d084.png" class="lazyload"></a></p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><h3 id="Note"><a href="#Note" class="headerlink" title="Note :"></a><strong>Note :</strong></h3><ul><li>batch_size 要够大才有意义，估算batch的分布。如batch_size 为1，则没有意义做这件事。</li><li>一般先做标准化在进入激活函数</li></ul><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><ol><li>首先计算出 一个batch的平均值、标准差</li></ol><p><a href="https://user-images.githubusercontent.com/60562661/75095608-ab60e000-55d1-11ea-80bc-f10457f65fa2.png" data-fancybox="group" data-caption="1582386527546" class="fancybox"><img alt="1582386527546" style="zoom: 67%;" title="1582386527546" data-src="https://user-images.githubusercontent.com/60562661/75095608-ab60e000-55d1-11ea-80bc-f10457f65fa2.png" class="lazyload"></a></p><ol><li>具体的每个输出 - mean / 标准差</li></ol><p><a href="https://user-images.githubusercontent.com/60562661/75095607-aac84980-55d1-11ea-9a33-963dd56edd8b.png" data-fancybox="group" data-caption="1582386468794" class="fancybox"><img alt="1582386468794" style="zoom: 67%;" title="1582386468794" data-src="https://user-images.githubusercontent.com/60562661/75095607-aac84980-55d1-11ea-9a33-963dd56edd8b.png" class="lazyload"></a></p><ol><li><p>train：训练时反向传播需要考虑到 <strong>σ μ</strong></p></li><li><p>test：测试</p><p><a href="https://user-images.githubusercontent.com/60562661/75095604-a7cd5900-55d1-11ea-829b-06ca53b71ee1.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/75095604-a7cd5900-55d1-11ea-829b-06ca53b71ee1.png" class="lazyload"></a></p><ul><li>首先计算出训练过程中所有的 <strong>σ μ </strong></li><li>如上图的右上角 取三个 μ，靠近后面的 μ 给一个比较大的权值，靠近初始化的给比较小的权值以此来估计整个网络的 <strong>μ</strong></li></ul></li></ol><h2 id="Batch-Normalization好处"><a href="#Batch-Normalization好处" class="headerlink" title="Batch Normalization好处"></a>Batch Normalization好处</h2><ol><li>解决了  Internal Covariate Shift 问题，可以设置比较大的学习率，增加训练速度</li><li>可以有效防止梯度弥散 。之前讲sigmoid函数在比较深的网络是训练不起来的，因为梯度会消失，而每次采取了标准化，数据分布会比较靠近0附近，函数在此处斜率比较大，梯度会一直比较大</li><li>对参数初始化不是那么敏感。例如所有的w都变成 w*k，但是经过公式推导经过标准化最终并无任何影响</li><li>一定程度上解决过拟合  标准化从其实一定程度相当于正则化。</li></ol><p><strong>Batch Normalization 在训练效果不好时可以使用，而测试效果不好则不一定要采用该方法。</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Batch Normal </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>win10环境下detectron2配置</title>
      <link href="/2020/02/18/win10%E7%8E%AF%E5%A2%83%E4%B8%8Bdetectron2%E9%85%8D%E7%BD%AE/"/>
      <url>/2020/02/18/win10%E7%8E%AF%E5%A2%83%E4%B8%8Bdetectron2%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>最强目标检测平台Detectron2 ，基于PyTorch完全重构，windows上很不友好，很难配置，配置好就算装好这个库依然不能使用<code>nms</code>,回头再想办法解决。安装过程曲折，记录一哈。</p><p><a href="https://user-images.githubusercontent.com/60562661/74744277-ee067d80-529c-11ea-8e3c-40dbae5c9348.jpg" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:50%;" data-src="https://user-images.githubusercontent.com/60562661/74744277-ee067d80-529c-11ea-8e3c-40dbae5c9348.jpg" class="lazyload"></a></p><p><a href="http://www.luyixian.cn/news_show_240401.aspx我主要参考的这个链接" target="_blank" rel="noopener">http://www.luyixian.cn/news_show_240401.aspx我主要参考的这个链接</a></p><p><a href="https://github.com/conansherry/detectron2" target="_blank" rel="noopener">https://github.com/conansherry/detectron2</a> 这个链接在我踩坑也不可或缺</p><p>我自己电脑是cuda9.0,cudnn7,上面第二个链接要墙置换成cuda10，很难搞，所以我就按第一个来</p><h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><p>依赖的库：<code>pytorch1.3</code> <code>opencv</code>  <code>pycocotools</code>  <code>fvcore</code>，其中最后两个安装不同于以往，这里提一下。</p><p><code>pip install git+https://github.com/facebookresearch/fvcore</code></p><p><code>pip install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'</code></p><p>其中，pycocotools安装很是麻烦，这个命令可能不会成功，所以需要自己探索一下，装不好可以评论留言找我要。</p><h2 id="确认gcc-gt-4-9"><a href="#确认gcc-gt-4-9" class="headerlink" title="确认gcc>=4.9"></a>确认gcc>=4.9</h2><p><code>gcc --version</code></p><h2 id="修改lib文件"><a href="#修改lib文件" class="headerlink" title="修改lib文件"></a>修改lib文件</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bat</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bat"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">file1: </span></span><br><span class="line"><span class="function">  {<span class="title">your</span> <span class="title">evn</span> <span class="title">path</span>}\<span class="title">Lib</span>\<span class="title">site</span>-<span class="title">packages</span>\<span class="title">torch</span>\<span class="title">include</span>\<span class="title">torch</span>\<span class="title">csrc</span>\<span class="title">jit</span>\<span class="title">argument_spec.h</span></span></span><br><span class="line"><span class="function">  <span class="title">example</span>:</span></span><br><span class="line"><span class="function">  {<span class="title">C</span>:\<span class="title">Miniconda3</span>\<span class="title">envs</span>\<span class="title">py36</span>}\<span class="title">Lib</span>\<span class="title">site</span>-<span class="title">packages</span>\<span class="title">torch</span>\<span class="title">include</span>\<span class="title">torch</span>\<span class="title">csrc</span>\<span class="title">jit</span>\<span class="title">argument_spec.h</span>(190)</span></span><br><span class="line"><span class="function">    <span class="title">static</span> <span class="title">constexpr</span> <span class="title">size_t</span> <span class="title">DEPTH_LIMIT</span> = 128;</span></span><br><span class="line"><span class="function">      <span class="title">change</span> <span class="title">to</span> --></span></span><br><span class="line"><span class="function">    <span class="title">static</span> <span class="title">const</span> <span class="title">size_t</span> <span class="title">DEPTH_LIMIT</span> = 128;</span></span><br><span class="line"><span class="function"><span class="title">file2</span>: </span></span><br><span class="line"><span class="function">  {<span class="title">your</span> <span class="title">evn</span> <span class="title">path</span>}\<span class="title">Lib</span>\<span class="title">site</span>-<span class="title">packages</span>\<span class="title">torch</span>\<span class="title">include</span>\<span class="title">pybind11</span>\<span class="title">cast.h</span></span></span><br><span class="line"><span class="function">  <span class="title">example</span>:</span></span><br><span class="line"><span class="function">  {<span class="title">C</span>:\<span class="title">Miniconda3</span>\<span class="title">envs</span>\<span class="title">py36</span>}\<span class="title">Lib</span>\<span class="title">site</span>-<span class="title">packages</span>\<span class="title">torch</span>\<span class="title">include</span>\<span class="title">pybind11</span>\<span class="title">cast.h</span>(1449)</span></span><br><span class="line"><span class="function">    <span class="title">explicit</span> <span class="title">operator</span> <span class="title">type</span>&() { <span class="title">return</span> *(<span class="title">this</span>-><span class="title">value</span>); }</span></span><br><span class="line"><span class="function">      <span class="title">change</span> <span class="title">to</span> --></span></span><br><span class="line"><span class="function">    <span class="title">explicit</span> <span class="title">operator</span> <span class="title">type</span>&() { <span class="title">return</span> *((<span class="title">type</span>*)<span class="title">this</span>-><span class="title">value</span>); }</span></span><br></pre></td></tr></tbody></table></figure></div><h2 id="克隆检测器并安装"><a href="#克隆检测器并安装" class="headerlink" title="克隆检测器并安装"></a>克隆检测器并安装</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/facebookresearch/detectron2.git</span><br><span class="line">cd detectron2</span><br><span class="line">python setup.py build develop</span><br></pre></td></tr></tbody></table></figure></div><p>这期间提示缺少什么 就补什么就行。</p><p>至此安装完成，<code>pip list</code> 可以看到 <code>detectron2</code> 这个包</p><h2 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h2><p>按着第一个链接教程搞，编译一直出错，各种莫名其妙的错误，编译不成功；然后我按照第二个教程修改了文件，就成功安装了。</p><p>不同电脑环境不同，可能是环境问题导致。这个可能也只适用于我自己的电脑。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 软件环境配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>win10 home docker-destop安装</title>
      <link href="/2020/02/16/win10-home-docker-destop-%E5%AE%89%E8%A3%85%EE%97%8A/"/>
      <url>/2020/02/16/win10-home-docker-destop-%E5%AE%89%E8%A3%85%EE%97%8A/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>docker容器是一种虚拟化技术，我自己也不是很懂，但是最近有一些项目要用，所以我就在使用了。docker-desktop在win10专业版是可以很轻易安装的，但是在win10 home 也就是家庭版就很难了，缺失一堆功能，等等，所以记录一下安装过程。</p><h2 id="开启-Hyper-V"><a href="#开启-Hyper-V" class="headerlink" title="开启 Hyper-V"></a>开启 Hyper-V</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bat</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bat"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">pushd</span> "%~dp0"</span><br><span class="line"></span><br><span class="line"><span class="built_in">dir</span> /b <span class="variable">%SystemRoot%</span>\servicing\Packages\*Hyper-V*.mum >hyper-v.txt</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> /f <span class="variable">%%i</span> <span class="keyword">in</span> ('<span class="built_in">findstr</span> /i . hyper-v.txt <span class="number">2</span>^><span class="built_in">nul</span>') <span class="keyword">do</span> dism /online /norestart /add-package:"<span class="variable">%SystemRoot%</span>\servicing\Packages\<span class="variable">%%i</span>"</span><br><span class="line"></span><br><span class="line"><span class="built_in">del</span> hyper-v.txt</span><br><span class="line"></span><br><span class="line">Dism /online /enable-feature /featurename:Microsoft-Hyper-V-All /LimitAccess /ALL</span><br></pre></td></tr></tbody></table></figure></div><h2 id="伪装成专业版"><a href="#伪装成专业版" class="headerlink" title="伪装成专业版"></a>伪装成专业版</h2><p>docker-desktop安装会检测，因此需要伪装为专业版绕过检测</p><p>打开注册表，定位到HKEY_LOCAL_MACHINE\software\Microsoft\Windows NT\CurrentVersion，点击current version，在右侧找到EditionId，右键点击EditionId 选择“修改“，在弹出的对话框中将第二项”数值数据“的内容改为Professional，然后点击确定。</p><p><strong>注意：</strong>  </p><ul><li>重启电脑此项value会还原，但并不影响docker; </li><li>同时如果需要重装，那就再改一次即可</li></ul><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p><a href="https://hub.docker.com/editions/community/docker-ce-desktop-windows" target="_blank" rel="noopener">https://hub.docker.com/editions/community/docker-ce-desktop-windows</a></p><p>到这个网址来下载docker一步步安装</p><h2 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h2><ol><li>我安装的时候提示container不可用，经过百度这也是和hyper-V方法一样，到csdn百度一下问题就有结果，很容易；没遇到该问题就正好；</li><li>我当时安装完 打开docker一直提示<code>hyper-V</code>没有完全开启，是因为我之前在其他网站找的开启<code>hyper-V</code>的代码，代码有问题，按照文章中的代码就没问题</li><li>之前安装的<code>Docker Toolbox</code> pull 镜像很慢，但是安装了桌面版之后很快(一直可以翻墙)</li></ol><p><a href="https://user-images.githubusercontent.com/60562661/74741075-23a86800-5297-11ea-9e55-827a2d34176b.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74741075-23a86800-5297-11ea-9e55-827a2d34176b.jpg" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 软件环境配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tips For DeepLearning_2</title>
      <link href="/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%978-Tips-For-DeepLearning-2-%E5%85%A8%E7%A8%8B%E9%AB%98%E8%83%BD/"/>
      <url>/2020/02/14/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%978-Tips-For-DeepLearning-2-%E5%85%A8%E7%A8%8B%E9%AB%98%E8%83%BD/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>前一篇中文讨论到，在深度学习过程中如果在训练集效果差怎么办，这里接着讨论后半部分，在训练集得到了想要了的效果，但是测试集（<strong>验证集，或者是有标签的一些数据</strong>）效果并不理想应该怎么办？有如下几种方案、方法可以参考一下：</p><ul><li>Early Stopping</li><li>Regularization</li><li>Dropout</li></ul><h2 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h2><p><a href="https://user-images.githubusercontent.com/60562661/74507861-997b9f00-4f38-11ea-91fa-f16cb7862228.png" data-fancybox="group" data-caption="1581651442213" class="fancybox"><img alt="1581651442213" style="zoom: 80%;" title="1581651442213" data-src="https://user-images.githubusercontent.com/60562661/74507861-997b9f00-4f38-11ea-91fa-f16cb7862228.png" class="lazyload"></a></p><p>这个方法大致提一下。深度学习中我们有一个假设：训练集和测试集分布是一样的。但是实际上可能并不会如此，我们训练模型应该在验证机loss最低的时候停下来，这就是这个方法的基本思想，具体的可以查阅下 文档。这里我自己主要也是了解一下即可，知道有这么一回事。</p><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><ol><li>正则化的目的是使得函数更加的平滑，因此正则化一般不对 bias 偏置做；</li><li>正则化会使得参数变小</li><li>正则化并不是非常的重要，效果不会非常显著</li></ol><h3 id="L2-正则化"><a href="#L2-正则化" class="headerlink" title="L2 正则化"></a>L2 正则化</h3><p><a href="https://user-images.githubusercontent.com/60562661/74507869-9d0f2600-4f38-11ea-9383-46421037d9da.png" data-fancybox="group" data-caption="1581651915129" class="fancybox"><img alt="1581651915129" style="zoom: 67%;" title="1581651915129" data-src="https://user-images.githubusercontent.com/60562661/74507869-9d0f2600-4f38-11ea-9383-46421037d9da.png" class="lazyload"></a></p><p>首先是右上角L2范数，是w的平方和。</p><p>然后是下面的公式推导，L‘是损失函数，L’对某一个w微分，就是后面的结果，然后更新参数公式也很顺理成章，整理到最后就是会在w之前✖一个系数，并且这个系数通常是一个很小的值，整个这个系数接近1，因此每个参数每次更新前会越来越接近0；第一项与第二项最后会实现平衡。</p><p><strong>这个手段也成为 Weight Decay，权重衰减</strong></p><h3 id="L1-正则化"><a href="#L1-正则化" class="headerlink" title="L1 正则化"></a>L1 正则化</h3><p>L1范数是绝对值求和。那么绝对值微分问题就是在真的走到0时直接随便丢一个value比如0当作微分。</p><p><a href="https://user-images.githubusercontent.com/60562661/74507871-9ed8e980-4f38-11ea-9820-1248212f60f4.png" data-fancybox="group" data-caption="1581652966101" class="fancybox"><img alt="1581652966101" title="1581652966101" data-src="https://user-images.githubusercontent.com/60562661/74507871-9ed8e980-4f38-11ea-9820-1248212f60f4.png" class="lazyload"></a></p><p>L‘对于某一个w的微分就是上图中所推导的，后面的sgn(w)意为：w为正则为1，w为负则为-1.</p><p>更新参数时总是在后面减去一项学习率 <em> 权重 </em> （1或-1），w为正数时减去一个数，w为负数时加上一个数，总之就是使得参数更加地接近0</p><h3 id="Contrast"><a href="#Contrast" class="headerlink" title="Contrast"></a>Contrast</h3><p>L1、L2正则化都是使得参数变小，但是略有不同；</p><ul><li>L1每次剪掉固定的值，</li><li>L2则是每次乘以一个固定的值，</li></ul><p>如果有一个参数很大，那么用L2正则化更新就比较快；用L1正则化更新依然很慢；</p><p>如果有一个参数很小，那么L2正则化更新就很慢；L1正则化 更新会比较快</p><p><strong>正则化中的权值衰减，与人的神经网络有异曲同工之妙</strong></p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p><a href="https://user-images.githubusercontent.com/60562661/74508193-5f5ecd00-4f39-11ea-9ce8-7ec0eb6d8384.png" data-fancybox="group" data-caption="1581653771487" class="fancybox"><img alt="1581653771487" title="1581653771487" data-src="https://user-images.githubusercontent.com/60562661/74508193-5f5ecd00-4f39-11ea-9ce8-7ec0eb6d8384.png" class="lazyload"></a></p><p>Dropout方法训练流程是这样：</p><ul><li><p>设置一个 dropout rate ：p%,也就是说在训练时每层会有 p% 的神经元被丢掉；</p></li><li><p>然后每次更新参数都会重新采样丢掉的神经元。</p></li></ul><p>Dropout的测试：</p><p>测试期间所有的神经元保留，然后最后丢失率是多少，每个w 都要乘上（1 - 丢失率）。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>dropout可以解释为训练了一系列的神经网络，然后最后输出的值取了个平均</p><p><a href="https://user-images.githubusercontent.com/60562661/74507874-9f718000-4f38-11ea-8097-eedc538b23e9.png" data-fancybox="group" data-caption="1581661977792" class="fancybox"><img alt="1581661977792" style="zoom: 50%;" title="1581661977792" data-src="https://user-images.githubusercontent.com/60562661/74507874-9f718000-4f38-11ea-8097-eedc538b23e9.png" class="lazyload"></a></p><p>如上图，dropout在所有的weight✖ (1-p%) 就取得了数个神经网络做平均的相近的结果。这也就是这个方法最神奇的地方。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在深度学习中遇到的问题分为 <code>training data set</code>  <code>test set data</code> 效果差</p><h3 id="在训练集效果差"><a href="#在训练集效果差" class="headerlink" title="在训练集效果差"></a>在训练集效果差</h3><h4 id="梯度弥散"><a href="#梯度弥散" class="headerlink" title="梯度弥散"></a>梯度弥散</h4><ul><li>relu 激活函数</li><li>maxout 激活函数</li></ul><h4 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h4><ul><li>Adagrad</li><li>RMSProp</li></ul><h4 id="局部最小化"><a href="#局部最小化" class="headerlink" title="局部最小化"></a>局部最小化</h4><ul><li>Momentum 算法</li><li>Adam </li></ul><h3 id="在训练集效果好但是在测试集效果差-过拟合"><a href="#在训练集效果好但是在测试集效果差-过拟合" class="headerlink" title="在训练集效果好但是在测试集效果差(过拟合)"></a>在训练集效果好但是在测试集效果差(过拟合)</h3><ul><li>Easy Stopping</li><li>L1、L2正则化</li><li>Dropout</li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 正则化 </tag>
            
            <tag> Dropout </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tips For DeepLearning_1</title>
      <link href="/2020/02/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%977-Tips-For-DeepLearning-%E5%85%A8%E7%A8%8B%E9%AB%98%E8%83%BD/"/>
      <url>/2020/02/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%977-Tips-For-DeepLearning-%E5%85%A8%E7%A8%8B%E9%AB%98%E8%83%BD/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>本文主要讲深度学习中常见的一些但是又理解不透彻的方法原理以及用途，全程高能！</p><p>主要有：</p><ul><li>深度学习流程</li><li>Activation Function（各种）</li><li>各种梯度下降法</li><li>L1、L2 正则化</li><li>dropout</li></ul><h2 id="DeepLearning-流程"><a href="#DeepLearning-流程" class="headerlink" title="DeepLearning 流程"></a>DeepLearning 流程</h2><p><a href="https://user-images.githubusercontent.com/60562661/74453826-ed956d80-4ebd-11ea-8ce3-89ece01340d1.png" data-fancybox="group" data-caption="1581604363019" class="fancybox"><img alt="1581604363019" style="zoom:50%;" title="1581604363019" data-src="https://user-images.githubusercontent.com/60562661/74453826-ed956d80-4ebd-11ea-8ce3-89ece01340d1.png" class="lazyload"></a></p><ol><li>首先是之前文章提到过的深度学习的三个步骤，这三个步骤会搭建起一个神经网络；</li><li>训练神经网络，检测网络对于训练集的效果,此时如果效果很差就要回去调整模型，继续训练，知道得到比较好的结果</li><li>此时在训练集效果达到了，开始在测试集(<strong>验证集</strong>)进行测试，如果效果不好就说明过拟合了，需要采取一些措施。</li></ol><p><strong>这里要注意，不是说效果一差就是过拟合，要理性分析。</strong></p><h2 id="Training-set-效果差的原因及其解决"><a href="#Training-set-效果差的原因及其解决" class="headerlink" title="Training set 效果差的原因及其解决"></a>Training set 效果差的原因及其解决</h2><h3 id="Vanishing-Gradient"><a href="#Vanishing-Gradient" class="headerlink" title="Vanishing Gradient"></a>Vanishing Gradient</h3><p>首先思考一个问题，为什么需要激活函数？ </p><p>一个普通的全连接网络，如果没有激活函数，那么所有的操作都是乘积、求和，这就是一个线性的网络，表达能力非常差，也不是我们所需要的，因此传统的在每个层后面输出时会加个sigmoid这样的非线性函数。也就是说，激活函数可以使我们的网络编程非线性的。</p><h4 id="Sigmoid-函数"><a href="#Sigmoid-函数" class="headerlink" title="Sigmoid 函数"></a>Sigmoid 函数</h4><p><a href="https://user-images.githubusercontent.com/60562661/74453819-ea9a7d00-4ebd-11ea-9848-b5bbf6ac7e49.jpg" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:50%;" data-src="https://user-images.githubusercontent.com/60562661/74453819-ea9a7d00-4ebd-11ea-9848-b5bbf6ac7e49.jpg" class="lazyload"></a></p><p>如图所示，所有的输入经过sigmoid会强行压缩到0-1之间，这样会使函数变成非线性的，但是也随之带来了问题：强行将所有之压缩到0-1，随着网络层数的加深，反向传播时靠近输出层的梯度值还比较大，但是输入层附近的层梯度都会很小，梯度从后往前越来越小直到消失，这就是<strong>梯度弥散</strong> (<strong>gradient Vanish</strong> )问题.</p><ul><li>靠近输出层的参数更新幅度比较快，但是此时靠近输入层的梯度已经很小接近于0，参数基本无更新，也就是还是随机值的状态来更新后面的参数，所以此时后面更新的参数问题就很大！</li><li>当输入层某个w发生变化就算变化非常大，而sigmoid的值变化幅度确非常的小，因为函数曲线很缓，如下图</li></ul><p><a href="https://user-images.githubusercontent.com/60562661/74453863-f8e89900-4ebd-11ea-9a69-9dbe824afeab.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom: 67%;" data-src="https://user-images.githubusercontent.com/60562661/74453863-f8e89900-4ebd-11ea-9a69-9dbe824afeab.png" class="lazyload"></a></p><h4 id="ReLu-函数"><a href="#ReLu-函数" class="headerlink" title="ReLu 函数"></a>ReLu 函数</h4><p><a href="https://user-images.githubusercontent.com/60562661/74453834-eff7c780-4ebd-11ea-92b4-a3e606985972.png" data-fancybox="group" data-caption="1581605904109" class="fancybox"><img alt="1581605904109" style="zoom: 50%;" title="1581605904109" data-src="https://user-images.githubusercontent.com/60562661/74453834-eff7c780-4ebd-11ea-92b4-a3e606985972.png" class="lazyload"></a></p><p>如图所示，输出值a小于0，则 <code>Relu(a) = 0</code>  否则，<code>Relu(a) = a</code></p><p>这个激活函数的出现解决了梯度弥散问题</p><p><a href="https://user-images.githubusercontent.com/60562661/74453841-f2f2b800-4ebd-11ea-9560-0a34ec5c413e.png" data-fancybox="group" data-caption="1581606094265" class="fancybox"><img alt="1581606094265" style="zoom:50%;" title="1581606094265" data-src="https://user-images.githubusercontent.com/60562661/74453841-f2f2b800-4ebd-11ea-9560-0a34ec5c413e.png" class="lazyload"></a></p><p>解决方式如图，值为0的神经元可以去掉，整个网络就是一个线性函数了。</p><p>这里有一个问题是：0的神经元消除，整个网络就是一个线性函数，这不符合我们要求啊？</p><p>可以这样解释，改变了数据输入，神经元的连线发生变化，整个网络依然是非线性的。</p><p><strong>着重理解Relu，它的一系列的变体函数就不说了，比较相似</strong></p><p>CNN文章提到的<code>Max Pooling</code> 不可导，和这个就很相似了，每次都是取最大值，然后其他的神经元都丢掉，整个网络又是一个细长的线性神经网络。<strong>其实是和下面的 Max-out函数比较相似</strong></p><h4 id="Max-Out-函数"><a href="#Max-Out-函数" class="headerlink" title="Max-Out 函数"></a>Max-Out 函数</h4><p><a href="https://user-images.githubusercontent.com/60562661/74454478-ede23880-4ebe-11ea-820a-d34dbd2c3cf6.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74454478-ede23880-4ebe-11ea-820a-d34dbd2c3cf6.png" class="lazyload"></a></p><p>这个方法是打组思想，Relu就是它的一个个例，这里就不细说了，relu用的多一点。</p><p><strong>有个问题：这个每次只取最大的这样有的神经元就不会被train到。因为每次数据不同得到的最大值也会不同，所以所有的weight都会得到更新，都会被train到。</strong></p><h3 id="Learning-Rate"><a href="#Learning-Rate" class="headerlink" title="Learning Rate"></a>Learning Rate</h3><p>梯度下降法的时候，会有局学习率问题，所以出现了一些梯度下降法的变体。</p><p><a href="https://huaqi.blue/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/#%E6%A6%82%E8%BF%B0%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%B5%81%E7%A8%8B" target="_blank" rel="noopener">Gradient Descent</a>、<a href="https://huaqi.blue/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96" target="_blank" rel="noopener">Adagrad Gradient Descent</a> 、以及 <a href="[https://huaqi.blue/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96](https://huaqi.blue/2020/02/10/深度学习入门系列一-梯度下降法-②/#梯度下降法及其优化">SGD</a>) 在之前文章都有讲过,不清楚可以返回看一下。</p><p>Adagrad梯度下降法实现了自适应的学习率，但是实际上我们面对的比这个所能解决的问题更加复杂，如下图。</p><p><a href="https://user-images.githubusercontent.com/60562661/74453867-fab25c80-4ebd-11ea-9131-f4a5cc825046.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74453867-fab25c80-4ebd-11ea-9131-f4a5cc825046.jpg" class="lazyload"></a></p><ul><li>左边的在w1方向，从左到右是比较平坦的，所以一个学习率就可以；</li><li>右边的图同样在w1方向比较缓的地方学习率就应该小点儿，突然陡峭学习率又应该大一点。<strong>因此需要更加动态的调整学习率。</strong></li></ul><h4 id="Root-Mean-Square-Prop-RMSProp"><a href="#Root-Mean-Square-Prop-RMSProp" class="headerlink" title="Root Mean Square Prop(RMSProp)"></a>Root Mean Square Prop(RMSProp)</h4><p><a href="https://user-images.githubusercontent.com/60562661/74453848-f5551200-4ebd-11ea-8508-f858952d1793.png" data-fancybox="group" data-caption="1581608584478" class="fancybox"><img alt="1581608584478" style="zoom: 67%;" title="1581608584478" data-src="https://user-images.githubusercontent.com/60562661/74453848-f5551200-4ebd-11ea-8508-f858952d1793.png" class="lazyload"></a></p><p>与之前的Adagrad方法非常类似，不同的是，这个方法可以控制权重，更偏向于过去的信息还是新的梯度信息，可以通过设置权重值来控制</p><h3 id="Local-Minimum"><a href="#Local-Minimum" class="headerlink" title="Local  Minimum"></a>Local  Minimum</h3><h4 id="Momentum-冲量、惯性-算法"><a href="#Momentum-冲量、惯性-算法" class="headerlink" title="Momentum (冲量、惯性)算法"></a>Momentum (冲量、惯性)算法</h4><p>如下图，在求出当前阶段的梯度值后，不只是考虑当前的梯度方向，同时考虑了前一个梯度方向，然后做个加权和得到新的方向。</p><p><a href="https://user-images.githubusercontent.com/60562661/74453853-f6863f00-4ebd-11ea-959d-a1dc06789b2e.png" data-fancybox="group" data-caption="1581609144187" class="fancybox"><img alt="1581609144187" style="zoom: 40%;" title="1581609144187" data-src="https://user-images.githubusercontent.com/60562661/74453853-f6863f00-4ebd-11ea-959d-a1dc06789b2e.png" class="lazyload"></a></p><p>这可以说是根据现实生活中的惯性，会继续向前走一点。</p><h4 id="Adam-优化算法"><a href="#Adam-优化算法" class="headerlink" title="Adam 优化算法"></a>Adam 优化算法</h4><p>Adam则是 RMSProp、Momentum两个梯度下降算法的集大成者。在深度学习框架中是已经封装好的。这个并未深究，我觉得理解了基础的就可以。</p><h2 id="关于测试集效果差，且听下回分解。"><a href="#关于测试集效果差，且听下回分解。" class="headerlink" title="关于测试集效果差，且听下回分解。"></a>关于测试集效果差，且听下回分解。</h2><p><a href="https://user-images.githubusercontent.com/60562661/74454502-f5094680-4ebe-11ea-9a65-ca37f68a6660.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74454502-f5094680-4ebe-11ea-9a65-ca37f68a6660.png" class="lazyload"></a></p><p>主要讲正则化、Dropout这两块。上图中前两种方法都是比较经典的，不只是深度学习的，而正则化用的多一点，所以主要理解一下正则化； Dropout则是非常具有深度学习风格，所以需要深究。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sigmoid </tag>
            
            <tag> relu </tag>
            
            <tag> max-out </tag>
            
            <tag> Learning-Rate </tag>
            
            <tag> Local Minimum </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convolution Neural Network(CNN)卷积神经网络</title>
      <link href="/2020/02/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%976-Convolution-Neural-Network-CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2020/02/13/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%976-Convolution-Neural-Network-CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="CNN的提出"><a href="#CNN的提出" class="headerlink" title="CNN的提出"></a>CNN的提出</h2><p><strong>其一，</strong>之前所提到的线性回归、比较简单逻辑回归都是全连接层<strong>(Full-Connected)</strong> ,那么在图像处理领域输入数据都是图像，现实中一张很小的图 <strong>100 * 100</strong> ，分辨率已经很低了，然是依然有<strong>30000 维数据</strong>，(这里默认图象是彩色图三通道的)，然后后面再堆几层网络，参数量实在是巨大，这是全连接网络的缺陷；</p><p><strong>其二，</strong>基于现实的观察有以下基点：</p><ul><li><p>假设CNN中每一个神经元都是用来识别某一个<strong>pattern</strong>[例如：鼻子，嘴，手臂] (实际上大概也是这样工作的)</p></li><li><p>人们在辨识一些小的部分比如鸟喙时，并不需要遍历一张图的所有信息，而是看到图片的一小部分就可以捕捉到需要的信息；</p></li><li>同一个部分(鸟喙)在图像中可能会出现在不同的位置，因此CNN的神经元以相同的参数就可以发现不同位置的鸟喙而不用重新学习参数</li><li>图像进行下采样，并不会影响我们对图片的观察(不包括比较极端的)；而图像较小的时候像素比较少此时也会减少参数</li></ul><p>基于以上，CNN卷积神经网络就正式提出了，并且在计算机视觉领域(影像处理)非常有效，几乎所有的任务，第一步都是要用卷积神经网络来提取特征。</p><h2 id="CNN一般架构"><a href="#CNN一般架构" class="headerlink" title="CNN一般架构"></a>CNN一般架构</h2><p>卷积神经网络一般是输入图像，然后经过 （卷积层、池化层）这两个一直重复，然后输出的像素拉平(flatten操作将当前值转变为一维向量)，连接上全连结网络输出，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/74443982-1e21db00-4eaf-11ea-973c-7b46e241d959.png" data-fancybox="group" data-caption="1581600351984" class="fancybox"><img alt="1581600351984" style="zoom: 67%;" title="1581600351984" data-src="https://user-images.githubusercontent.com/60562661/74443982-1e21db00-4eaf-11ea-973c-7b46e241d959.png" class="lazyload"></a></p><h2 id="Convolution计算流程"><a href="#Convolution计算流程" class="headerlink" title="Convolution计算流程"></a>Convolution计算流程</h2><p><strong>首先，CNN中要训练的参数就是卷积核的每个像素的数值</strong></p><h3 id="单通道卷积计算"><a href="#单通道卷积计算" class="headerlink" title="单通道卷积计算"></a>单通道卷积计算</h3><p><a href="https://user-images.githubusercontent.com/60562661/74443985-1f530800-4eaf-11ea-8059-661371e1f691.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom: 67%;" data-src="https://user-images.githubusercontent.com/60562661/74443985-1f530800-4eaf-11ea-8059-661371e1f691.png" class="lazyload"></a></p><p>如上图，用filter1在图像6<em>6图像上滑动，从左上角开始，步长为1，在每个窗格对应位置相乘然后加起来输出一个新的值，此时就会形成一个新的4 <strong> 4的 img ，称为特征图</strong>Feature Map *</em>。</p><p>此时有一个卷积核，就输出一张特征图，两个卷积核就输出两张特征图，以此类推。</p><h3 id="多通道卷积计算"><a href="#多通道卷积计算" class="headerlink" title="多通道卷积计算"></a>多通道卷积计算</h3><p><a href="https://user-images.githubusercontent.com/60562661/74443978-1c581780-4eaf-11ea-923b-a7975171f4d7.png" data-fancybox="group" data-caption="1581598043317" class="fancybox"><img alt="1581598043317" style="zoom: 67%;" title="1581598043317" data-src="https://user-images.githubusercontent.com/60562661/74443978-1c581780-4eaf-11ea-923b-a7975171f4d7.png" class="lazyload"></a></p><p>如果输入的图像是三通道的，那么每个卷积核对应的也是三通道的，注意此时计算可能是：</p><p>卷积核的第一个通道与图像红色通道进行卷积运算，卷积核的第二个通道与图像绿色通道进行卷积运算，卷积核的第三个通道与图像蓝色通道进行卷积运算，然后 卷积核三个通道输出的img对应位置相加，形成一个新的1个通道的img，就是这个卷积核所输出的 <strong>Feature Map</strong>  。这里注意的是： 对于多通道图像，<code>一个卷积核进行卷积运算后所输出的依然是一个 feature map，而不是9个(3*3).</code></p><h2 id="Convolution-amp-Neural-Network"><a href="#Convolution-amp-Neural-Network" class="headerlink" title="Convolution & Neural Network"></a>Convolution & Neural Network</h2><p>以上讲了卷积的运算方式，那么卷积与神经网络，与全连接网络有什么关系呢？</p><p><strong>卷积实际上就是全连接网络(去掉一些weight) !</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/74443980-1d894480-4eaf-11ea-81f5-dde23c1db352.png" data-fancybox="group" data-caption="1581598946132" class="fancybox"><img alt="1581598946132" style="zoom: 67%;" title="1581598946132" data-src="https://user-images.githubusercontent.com/60562661/74443980-1d894480-4eaf-11ea-81f5-dde23c1db352.png" class="lazyload"></a></p><p>分析一下这张图，</p><ul><li><p>首先右边蓝色的 1 2 3 4 ···一直到16，表示的是将左边6*6的图像拉平（这里没有画完），蓝色的框里的数字是每个像素的值；</p></li><li><p>然后上面是个3*3的卷积核，每个像素用不同颜色的⚪圈了起来；</p></li><li><p>然后上图右边部分橙色的 3，-1 就是 卷积核与图像滑动过的区域做的卷积计算得到的数值，将卷积核卷积后的4*4的img也拉平，就得到了右边的 3 ，-1  （这里用3和-1举例子所以没有画完）</p></li></ul><h3 id="大量参数的减少"><a href="#大量参数的减少" class="headerlink" title="大量参数的减少"></a>大量参数的减少</h3><p>卷积之后得到的图像的每个像素也就是右边的3，-1 等，可以看作是一个神经元，其中 卷积核做图像左上角的时候，计算刚好是与原来的6<em>6图像的 编号为 1 2 3 7 8 9 13 14 15 的像素进行的，因此“<strong>3</strong>”这个神经元就连接到了编号为 1 2 3 7 8 9 13 14 15 的像素，-1是同样的道理。这时候，如果计算参数量，就是 16 </em> 9 = 144 个参数，而此时如果用全连接层的话，就是 36 * 16 = 576 个参数，已经少了很多了</p><h3 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h3><p>上图中右边部分的神经元，并不是说所有的参数都要计算。一个卷积核中同一个像素滑动过的值他们之间的权重都是强迫相等的。举个例子，卷积核中的第一个像素(深红色圆圈)，与6<em>6的图像在左上角计算卷积时对应的编号为1的像素，卷积核向右滑动一次后，该像素(深红色圆圈)对应的是编号为2的像素，因此 上图右边部分 1 号像素和 右边的神经元3 ， 2号像素与右边的神经元-1之间连接都用的是深红色，这两条线的参数就是相等的。所以同理，上图右边部分连线中颜色相同的权值都是共享的。(<em>*Share Weights</em></em>) 此来再来计算一下参数量，就只有9个了。</p><p>这其实也不难理解，一开始文章就提到，卷积神经网络的参数就是卷积核的像素值，这里是3*3的卷积核9个像素，所以也就只有9个参数了。到这里已经是全连接网络的 1/64 了，也就是减少了64倍的参数，这在 上百万参数是减少的就更明显了！</p><p><strong>到这里，已经理解了卷积神经网络的计算方式以及如何减少参数</strong></p><h2 id="池化-Max-pooling"><a href="#池化-Max-pooling" class="headerlink" title="池化 Max pooling"></a>池化 Max pooling</h2><p><a href="https://user-images.githubusercontent.com/60562661/74443974-1a8e5400-4eaf-11ea-91d0-250625a1d4ad.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74443974-1a8e5400-4eaf-11ea-91d0-250625a1d4ad.jpg" class="lazyload"></a></p><p>在卷积输出的特征图基础上，以2*2为单位，每个红色框里选出最的值组成一个新的img，这就是最大池化；</p><p>平均池化就是一个红色框里所有的像素值取平均。 </p><p>经过池化，图像尺寸变为 2*2 </p><p><strong>池化层采用最大池化方式，那么怎么求微分呢？不可导就不能梯度下降，这个下一篇文章会说。</strong></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>以上就是CNN，卷积神经网络，工作方式可以理解为某一层的神经元识别一个 pattern ，然后全连接层组合这些个 pattern 最后提取出高质量的特征。 这个可以自己求证一下。大概可以这么解释。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>反向传播BP算法</title>
      <link href="/2020/02/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%974-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADBP%E7%AE%97%E6%B3%95/"/>
      <url>/2020/02/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%974-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADBP%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>之前讲过机器学习的三个步骤，<strong>深度学习</strong><code>Deep Learning</code>非常的类似，可以概括为以下几步：</p><ul><li>(设置函数) -> 搭建<strong>神经网络</strong></li><li>(函数的好坏定义) -> 设置<strong>损失函数</strong></li><li>(找出最优函数) -> <strong>反向传播</strong>更新参数</li></ul><p>第一步之前的设置函数，在这里用神经网络来替代了；</p><p>在线性回归逻辑回归中可以直接计算梯度，但是深度学习神经网络比较深，不能一下子求出梯度，因此本文主要来探讨一下反向传播 <code>Back Propagation</code>算法。</p><p><strong>同时，本文会附上手动搭建神经网络、计算梯度、实现反向传播的代码，纯手写只用到了numpy库！</strong></p><h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><p>神经网络的反向传播并不需要很高深的数学知识，需要掌握<strong>链式求导法则 (Chain Rule)</strong>。下面会一步步从数学上求出微分 ，并且理解这种算法的精妙之处。</p><h2 id="案例背景"><a href="#案例背景" class="headerlink" title="案例背景"></a>案例背景</h2><p><a href="https://user-images.githubusercontent.com/60562661/74358378-50263500-4dfc-11ea-8f03-2fac002d131d.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74358378-50263500-4dfc-11ea-8f03-2fac002d131d.png" class="lazyload"></a></p><p>一个有代表性的例子，输入层经过神经网络得到输出值，与真实值之间存在误差，这里用交叉熵作为损失函数，因此在这里，要求梯度也就是求损失函数对于w的偏微分。</p><h2 id="层层深入"><a href="#层层深入" class="headerlink" title="层层深入"></a>层层深入</h2><p><a href="https://user-images.githubusercontent.com/60562661/74358360-4b618100-4dfc-11ea-8411-9478cb1f4a0f.png" data-fancybox="group" data-caption="1581522297605" class="fancybox"><img alt="1581522297605" title="1581522297605" data-src="https://user-images.githubusercontent.com/60562661/74358360-4b618100-4dfc-11ea-8411-9478cb1f4a0f.png" class="lazyload"></a></p><p>把上面具体的神经网络展开，假设只有两个神经元，这里只对w求微分，b的方式是一样的，所以以w为例。</p><p>首先，根据上图的函数以及chain rules，损失函数C对w的偏微分可以拆解为两部分</p><ul><li>C 对于 z 的偏导数 </li><li>z 对于 w 的偏导数</li></ul><h3 id="Forward-Pass"><a href="#Forward-Pass" class="headerlink" title="Forward Pass"></a>Forward Pass</h3><p>（先讲第二部分）其中，z 对于 w 的偏导数比较容易，可以很容易的看出来 z 对于 w1 的偏导数就是w之前的输入值，也就是 x1，同理 z 对于 w2 的偏导数就是 x2，下面的图强化一下 z 对于 w 的偏导数</p><p><a href="https://user-images.githubusercontent.com/60562661/74358383-51eff880-4dfc-11ea-9f7b-87aea43f3483.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:50%;" data-src="https://user-images.githubusercontent.com/60562661/74358383-51eff880-4dfc-11ea-9f7b-87aea43f3483.png" class="lazyload"></a></p><p>可以观察到z对于每个w的偏导数就是 当前权重所之前的输入值，这样比较拗口，英语会比较好理解：</p><p><strong>The value of the input connected by the weight</strong> .通俗的说就是这条线从哪里出来，出来的那个节点值就是z对于这条线也就是这个w的偏微分。</p><p>这里也可以看出，前向传播可以算出每个中间值，也就是计算出了每个梯度的上述第二部分。</p><h3 id="Backward-Pass"><a href="#Backward-Pass" class="headerlink" title="Backward Pass"></a>Backward Pass</h3><p>下面看第一部分比较复杂的，也就是 C 对于 z 的偏导数。</p><p><a href="https://user-images.githubusercontent.com/60562661/74358367-4dc3db00-4dfc-11ea-81b3-7bf35d6c3b66.png" data-fancybox="group" data-caption="1581523046931" class="fancybox"><img alt="1581523046931" title="1581523046931" data-src="https://user-images.githubusercontent.com/60562661/74358367-4dc3db00-4dfc-11ea-81b3-7bf35d6c3b66.png" class="lazyload"></a></p><p>上图中 z 经过 sigmoid 函数 得到 a，a 继续传播到下一层，此时 C 对于 z 的偏导数可以转化为上图中的下面公式所写的。在求和的两部分中，同样的各自又都分为两部分，与上述的两部分类似。z’ 对于 a 的偏导数很容易，就直接是 w3 ，相应的 z’‘ 就是 w4.</p><p>所以此时问题就转化为 C对于z’ C对于z‘’ 的偏微分，如果这两部分知道那么就可以求出来 C对a的偏微分，同样的C对z的偏微分也就求出来了，也就解决了这部分问题。以下内容是<strong>关键：</strong></p><h3 id="反求"><a href="#反求" class="headerlink" title="反求"></a>反求</h3><p><a href="https://user-images.githubusercontent.com/60562661/74358708-e195a700-4dfc-11ea-978f-34ace433b4ce.png" data-fancybox="group" data-caption="1581523822435" class="fancybox"><img alt="1581523822435" style="zoom:50%;" title="1581523822435" data-src="https://user-images.githubusercontent.com/60562661/74358708-e195a700-4dfc-11ea-978f-34ace433b4ce.png" class="lazyload"></a></p><p>上图的下面的公式只是把值带入了前一步中的公式，但是可以想象一下，这里面的 乘法、加法 操作 很像是神经网络的前向传播，所以这里就可以想象成一个新的神经网络，只不过是反过来计算的，这时候就会计算出来 C 对于 z 的偏微分。值得注意的是 上图中 <code>sigmoid’(z)</code> 是个常数，因为z前向传播时已经计算出来了，所以这里就是计算一个数而已。因此现在到这一步，<strong>说明知道后面两项的偏微分可以求出前面的。</strong> 此时问题依然是 C对于z’ C对于z‘’ 的偏微分。</p><h3 id="大胆假设、细心求证"><a href="#大胆假设、细心求证" class="headerlink" title="大胆假设、细心求证"></a>大胆假设、细心求证</h3><h4 id="假设一-easy"><a href="#假设一-easy" class="headerlink" title="假设一 (easy)"></a>假设一 (easy)</h4><p>假设 <strong>z‘ z’‘</strong> 之后经过激活函数直接是最终的输出，此时求微分就很简单了，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/74358371-4ef50800-4dfc-11ea-851d-3f8bb542090d.png" data-fancybox="group" data-caption="1581524647050" class="fancybox"><img alt="1581524647050" title="1581524647050" data-src="https://user-images.githubusercontent.com/60562661/74358371-4ef50800-4dfc-11ea-851d-3f8bb542090d.png" class="lazyload"></a></p><p>其中，C 对于 y1 的偏微分就是损失函数的偏微分，y1 对于 z’ 的偏微分就是根据<strong>激活函数</strong>（上图最后的橙色圆圈）求出微分很容易，z‘’ 同理。</p><p>在此种假设下，此时已经得出了 C 对于 z‘、 C 对于 z’‘ 的偏微分，回溯到前一个步骤，就求出了 C对于z的偏导，在往前回到最初步，发现此时已经求出了两个 需要的条件，此时就可以算出 C 对于 w1 ，C  对于 w2 的偏微分。</p><p><strong>也就是说，忙活到现在，也就只是算出来了第一个神经元的两条线(2个w)的梯度！</strong></p><h4 id="假设二-Normal"><a href="#假设二-Normal" class="headerlink" title="假设二 (Normal)"></a>假设二 (Normal)</h4><p>假设 <strong>z‘ z’‘</strong> 之后 依然有很多层，如下图：</p><p> <a href="https://user-images.githubusercontent.com/60562661/74358374-4f8d9e80-4dfc-11ea-9285-7f803a3a4c5f.png" data-fancybox="group" data-caption="1581525161904" class="fancybox"><img alt="1581525161904" title="1581525161904" data-src="https://user-images.githubusercontent.com/60562661/74358374-4f8d9e80-4dfc-11ea-9285-7f803a3a4c5f.png" class="lazyload"></a></p><p>由<code>反求</code>部分我们已经知道想要求 C对于 z’（or z‘’） 的偏导数，需要知道后面 C 对于 Za 及 Zb 的偏导数，所以需求会一直往后寻找，递归这个过程，知道到达输出层，然后一层层往前就会求出来最初始的梯度，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/74358380-51576200-4dfc-11ea-9ec1-958f51f62321.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74358380-51576200-4dfc-11ea-9ec1-958f51f62321.png" class="lazyload"></a></p><p>至此，如果理解了这些，就已经理解了反向传播的原理了。</p><h2 id="Conclusion-amp-Question"><a href="#Conclusion-amp-Question" class="headerlink" title="Conclusion & Question"></a>Conclusion & Question</h2><h3 id="BP算法总结"><a href="#BP算法总结" class="headerlink" title="BP算法总结"></a>BP算法总结</h3><p><code>Back Propagation</code>  算法分为两部分</p><ul><li>前向传播 求出 z 对于 w 的偏导数</li><li>反向传播 求出 C 对于 z 的偏导数 </li><li>两个值相乘就是梯度</li></ul><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>从前到后的传播直接计算每个参数的梯度为什么比BP算法差?</p><h4 id="前向传播计算梯度"><a href="#前向传播计算梯度" class="headerlink" title="前向传播计算梯度"></a>前向传播计算梯度</h4><p>从前到后直接传播计算梯度，第一层的w需要知道后面所有的层的梯度，此时会进行一趟计算；2-end；</p><p>继续求第二层w 的梯度，需要知道后面所有层的梯度，也就是 3 - end； 最后加起来就是：</p><p>end - 2 + end - 3 + end - 4 + ….. + end-0,z明显计算量不小！</p><h4 id="反向传播计算梯度"><a href="#反向传播计算梯度" class="headerlink" title="反向传播计算梯度"></a>反向传播计算梯度</h4><p>从最后一层开始计算，先计算出最后一层梯度，可以直接计算出来，这样每次往前计算不用再一直累加，因此计算量小很多。所以说BP算法刚好就是利用了原来的网络和参数而且可以用和前向传播相同的计算量计算出所有w的梯度。这就是BP算法的精妙之处！</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>在这里手动搭建了一个神经网络，暂时没有考虑b，因为只是用来加深理解，又一个输入层，两个隐藏层，一个输出层，每层四个神经元。所有参数都是手动计算梯度。</p><p>根据以上分析的反向传播算法可以总结出以下几步：</p><ul><li>前向传播一遍计算出所有节点的值</li><li>反向传播一遍计算出所有结点的偏微分</li><li>做乘法求出所有的梯度进行更新</li></ul><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#Generate data</span></span><br><span class="line"><span class="comment"># Forward Node</span></span><br><span class="line">x_F = np.random.rand(<span class="number">4</span>)</span><br><span class="line">y_F = np.random.rand(<span class="number">4</span>)</span><br><span class="line">z_F = np.random.rand(<span class="number">4</span>)</span><br><span class="line">p_F = np.random.rand(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#weight</span></span><br><span class="line">x_y_w = np.random.rand(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">y_z_w = np.random.rand(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">z_p_w = np.random.rand(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">#backward node</span></span><br><span class="line">x_B = np.random.rand(<span class="number">4</span>)</span><br><span class="line">y_B = np.random.rand(<span class="number">4</span>)</span><br><span class="line">z_B = np.random.rand(<span class="number">4</span>)</span><br><span class="line">p_B = np.random.rand(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#TARGET</span></span><br><span class="line">target = np.array([<span class="number">0.5</span>,<span class="number">0.7</span>,<span class="number">0.3</span>,<span class="number">0.1</span>])</span><br><span class="line"><span class="comment">#loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SquareErrorLoss</span><span class="params">(output, target)</span>:</span></span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(output)):</span><br><span class="line">        loss = loss + (output[i] - target[i])**<span class="number">2</span></span><br><span class="line">    loss = loss</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># Graident</span></span><br><span class="line">lr = <span class="number">0.0000001</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">500000</span>):</span><br><span class="line">    <span class="comment"># forward0</span></span><br><span class="line">    y_F = np.matmul(x_F, x_y_w)</span><br><span class="line">    z_F = np.matmul(y_F, y_z_w)</span><br><span class="line">    p_F = np.matmul(z_F, z_p_w)  <span class="comment"># 得到输出</span></span><br><span class="line">    loss_end = SquareErrorLoss(p_F, target)</span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    p_B = <span class="number">2</span>*p_F  <span class="comment"># end grad</span></span><br><span class="line">    z_B = np.matmul(p_B, z_p_w.T)</span><br><span class="line">    y_B = np.matmul(z_B, y_z_w.T)</span><br><span class="line">    <span class="comment"># print(z_F[0])</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># grad</span></span><br><span class="line">    z_p_w_grad = [np.dot(z_F[<span class="number">0</span>], p_B),</span><br><span class="line">                  np.dot(z_F[<span class="number">1</span>], p_B),</span><br><span class="line">                  np.dot(z_F[<span class="number">2</span>], p_B),</span><br><span class="line">                  np.dot(z_F[<span class="number">3</span>], p_B)]  <span class="comment"># 4*4</span></span><br><span class="line">    y_z_w_grad = [np.dot(y_F[<span class="number">0</span>], z_B),</span><br><span class="line">                  np.dot(y_F[<span class="number">1</span>], z_B),</span><br><span class="line">                  np.dot(y_F[<span class="number">2</span>], z_B),</span><br><span class="line">                  np.dot(y_F[<span class="number">3</span>], z_B)]  <span class="comment"># 4*4</span></span><br><span class="line">    x_y_w_grad = [np.dot(x_F[<span class="number">0</span>], y_B),</span><br><span class="line">                  np.dot(x_F[<span class="number">1</span>], y_B),</span><br><span class="line">                  np.dot(x_F[<span class="number">2</span>], y_B),</span><br><span class="line">                  np.dot(x_F[<span class="number">3</span>], y_B)]  <span class="comment"># 4*4</span></span><br><span class="line">    <span class="comment"># update</span></span><br><span class="line">    x_y_w = x_y_w - lr * np.array(x_y_w_grad)</span><br><span class="line">    y_z_w = y_z_w - lr * np.array(y_z_w_grad)</span><br><span class="line">    z_p_w = z_p_w - lr * np.array(z_p_w_grad)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> epoch % <span class="number">5000</span> == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"当前loss值为"</span>)</span><br><span class="line">        print(loss_end)</span><br><span class="line">        print(p_F)</span><br><span class="line"></span><br><span class="line"><span class="comment">#截取输出片段打印</span></span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">165.06777280622663</span></span><br><span class="line">[<span class="number">8.38287281</span> <span class="number">5.70538503</span> <span class="number">7.00228248</span> <span class="number">5.84052431</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">133.11940656598998</span></span><br><span class="line">[<span class="number">7.56299701</span> <span class="number">5.15781875</span> <span class="number">6.33931068</span> <span class="number">5.28536964</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">109.77775387801975</span></span><br><span class="line">[<span class="number">6.89894847</span> <span class="number">4.7147705</span>  <span class="number">5.8028157</span>  <span class="number">4.83622736</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">92.14372363473903</span></span><br><span class="line">[<span class="number">6.34836332</span> <span class="number">4.34779557</span> <span class="number">5.35839007</span> <span class="number">4.4642465</span> ]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">78.45934876430728</span></span><br><span class="line">[<span class="number">5.88318774</span> <span class="number">4.03806177</span> <span class="number">4.98325151</span> <span class="number">4.1503256</span> ]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">67.60388807433274</span></span><br><span class="line">[<span class="number">5.48405883</span> <span class="number">3.77257388</span> <span class="number">4.66167746</span> <span class="number">3.88128352</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">58.833090378271265</span></span><br><span class="line">[<span class="number">5.13715314</span> <span class="number">3.54205652</span> <span class="number">4.38244476</span> <span class="number">3.64771203</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">51.6356735906825</span></span><br><span class="line">[<span class="number">4.83232077</span> <span class="number">3.3397007</span>  <span class="number">4.13731374</span> <span class="number">3.44270456</span>]</span><br><span class="line"><span class="meta">... </span>... ... ... ... ... ... ... ... ... ... ...</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">0.34148441938663887</span></span><br><span class="line">[<span class="number">0.38019982</span> <span class="number">0.42063884</span> <span class="number">0.60690939</span> <span class="number">0.49356868</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">0.33762403308295663</span></span><br><span class="line">[<span class="number">0.37029135</span> <span class="number">0.41401346</span> <span class="number">0.59891746</span> <span class="number">0.48685882</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">0.3343170568629769</span></span><br><span class="line">[<span class="number">0.36058041</span> <span class="number">0.40751086</span> <span class="number">0.5910724</span>  <span class="number">0.48027121</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">0.3315343883568477</span></span><br><span class="line">[<span class="number">0.3510625</span>  <span class="number">0.40112821</span> <span class="number">0.58337076</span> <span class="number">0.47380298</span>]</span><br><span class="line">当前loss值为</span><br><span class="line"><span class="number">0.3292483287899748</span></span><br><span class="line">[<span class="number">0.3417333</span>  <span class="number">0.39486274</span> <span class="number">0.57580922</span> <span class="number">0.46745137</span>]</span><br></pre></td></tr></tbody></table></figure></div><p>以上代码可以看出，loss值不断不断的下降，并且最终趋于稳定，可以说明由之前的推导总结出的方法思路并无问题！</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Back Propagation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logistic Regression</title>
      <link href="/2020/02/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
      <url>/2020/02/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>逻辑回归，是用于处理因变量为分类变量的回归问题，比如二分类问题，其实是一种分类算法。本文主要对比逻辑回归回归与线性回归的处理过程并探讨其中一些公式、细节。 </p><h2 id="逻辑回归-amp-线性回归"><a href="#逻辑回归-amp-线性回归" class="headerlink" title="逻辑回归&线性回归"></a>逻辑回归&线性回归</h2><p><a href="https://user-images.githubusercontent.com/60562661/74258370-2bb15680-4d31-11ea-9d41-08b967ee8643.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom: 45%;" data-src="https://user-images.githubusercontent.com/60562661/74258370-2bb15680-4d31-11ea-9d41-08b967ee8643.png" class="lazyload"></a></p><p>如上图所示，</p><ul><li>step1确定函数时，逻辑回归函数输出的是个概率值，线性回归输出的可以是任何一个数</li><li>step2确定损失函数，逻辑回归用的<strong>交叉熵</strong>，线性回归则是均方误差</li><li><strong>step3比较神奇，两个损失函数长得差别很大，但是算到最后更新参数公式竟然是一样的！(后面有推导)</strong></li></ul><h2 id="Cross-Entropy"><a href="#Cross-Entropy" class="headerlink" title="Cross Entropy"></a>Cross Entropy</h2><p>背景，一系列函数设置：</p><p> <a href="https://user-images.githubusercontent.com/60562661/74258366-2b18c000-4d31-11ea-83e1-e5ba4074b47e.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:40%;" data-src="https://user-images.githubusercontent.com/60562661/74258366-2b18c000-4d31-11ea-83e1-e5ba4074b47e.png" class="lazyload"></a></p><p>设计损失函数：这里有一个假设， x1 x2 …xN 属于 C1 类，真值为1，其余为0</p><p><a href="https://user-images.githubusercontent.com/60562661/74258376-2d7b1a00-4d31-11ea-96ba-0cc3210328f7.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74258376-2d7b1a00-4d31-11ea-96ba-0cc3210328f7.png" class="lazyload"></a></p><p><strong>这里的之所以要取对数，是因为拆开之后求微分比较容易。</strong>蓝色线画的，即是交叉熵，也就是表格中step2的C函数。</p><p><a href="https://user-images.githubusercontent.com/60562661/74258361-26540c00-4d31-11ea-8693-ba5cdf8dbc64.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:45%;" data-src="https://user-images.githubusercontent.com/60562661/74258361-26540c00-4d31-11ea-8693-ba5cdf8dbc64.png" class="lazyload"></a></p><p>上述函数对w求偏微分之后，也就是求出了梯度，（过程可以自己算一下，挺简单的），就得到了下面的公式，此时更新参数就和线性回归相同也推出来了。</p><h2 id="交叉熵和均方误差"><a href="#交叉熵和均方误差" class="headerlink" title="交叉熵和均方误差"></a>交叉熵和均方误差</h2><p><a href="https://user-images.githubusercontent.com/60562661/74258385-2f44dd80-4d31-11ea-907b-5b5c2bc25f78.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74258385-2f44dd80-4d31-11ea-907b-5b5c2bc25f78.png" class="lazyload"></a></p><p>图中黑色曲面的代表<strong>交叉熵</strong>，红色的曲面代表<strong>均方误差</strong>，这幅图表达了参数的变化对于整个loss值的影响大小</p><ul><li>对于<strong>Cross Entropy</strong>，越边缘的点梯度越大，此时更新参数会比较快</li><li>对于 <strong>Square Error</strong>，边缘的点梯度并不大，而接近最小值的点梯度也比较小</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>由上面的对比可知：逻辑回归如果用均方误差，会导致更新参数非常慢，而直接提高学习率也并不是一个好的选择，因为真正靠近真值时，梯度本来就比较小，较大的学习率此时也并不合理，因此一般来说，逻辑回归(也就是分类问题)可以采用交叉熵作为损失函数。</p><hr><h2 id="2020-6-8更新"><a href="#2020-6-8更新" class="headerlink" title="2020/6/8更新"></a>2020/6/8更新</h2><p>说实话看以上的确实是一头雾水，可能看完还是不知道什么是逻辑回归，因为上述文章只是说到逻辑回归常用于二分类问题，这里做一些补充、再学习。</p><p>白话来说，二分类网络最后一层使用<code>sigmoid</code>函数就是逻辑回归方法，具体原因是：</p><hr><p>鉴于要写的东西比较多，决定 新写一篇文章。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二维数组中的查找</title>
      <link href="/2020/02/11/%E7%89%9B%E5%AE%A2%E7%BD%91%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0-%E5%89%91%E6%8C%87offer%E7%B3%BB/"/>
      <url>/2020/02/11/%E7%89%9B%E5%AE%A2%E7%BD%91%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0-%E5%89%91%E6%8C%87offer%E7%B3%BB/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>自己最近正好有空，因此开始了刷题系列，牛客网上的剑指offer题目。今天记录一个比较有思想的题目。因为这些都是算法题目，如果去用穷举蒙混过关就没意思了，因此尽量找比较优化的算法。我这里用python来解答。</p><p>最近正好也是有空，因此开始了刷题系列，牛客网上的剑指offer题目。今天记录一个比较有思想的题目。因为这些都是算法题目，如果去用穷举蒙混过关就没意思了，因此尽量找比较优化的算法。我这里用python来解答。</p><h2 id="题目："><a href="#题目：" class="headerlink" title="题目："></a>题目：</h2><p><code>在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。</code></p><h2 id="思路："><a href="#思路：" class="headerlink" title="思路："></a>思路：</h2><h4 id="思路一："><a href="#思路一：" class="headerlink" title="思路一："></a>思路一：</h4><p>因为数组的有序性，从左到右从上到下递增，利用这一个规律，选取左下角的数为基点，如果目标数大于该数，那么目标肯定在基点的右边，所以 <strong>column + 1 </strong>; 如果目标小于基点数，那么 <strong>row - 1 </strong>。</p><p>逻辑是这样，每移动一次，都会以目前所在的点为基点，因此会排除一行或者一列，这种复杂度就会比较低了。‘</p><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># array 二维列表</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Find</span><span class="params">(self, target, array)</span>:</span></span><br><span class="line">        hang = len(array)</span><br><span class="line">        lie = len(array[<span class="number">0</span>])</span><br><span class="line">        i = hang - <span class="number">1</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        result = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">while</span>(i>=<span class="number">0</span> <span class="keyword">and</span> j<lie):< span><br><span class="line">            <span class="keyword">if</span> target < array[i][j]:</span><br><span class="line">                i = i - <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> target > array[i][j]:</span><br><span class="line">                j = j + <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> target == array[i][j]:</span><br><span class="line">                result = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">return</span> result</span><br></lie):<></span></pre></td></tr></tbody></table></figure></div><h4 id="思路二："><a href="#思路二：" class="headerlink" title="思路二："></a>思路二：</h4><p>遍历每一行用二分查找法，这个代码还没写，所以明天补上，这种方法比较通俗易懂。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 算法刷题 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数组查找 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>模型的误差来源及其改进</title>
      <link href="/2020/02/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%AF%AF%E5%B7%AE%E6%9D%A5%E8%87%AA%E5%93%AA%E9%87%8C/"/>
      <url>/2020/02/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%AF%AF%E5%B7%AE%E6%9D%A5%E8%87%AA%E5%93%AA%E9%87%8C/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>之前文章讲到过用梯度下降法来更新参数。深度学习反向传播，根据误差调整参数，那么深度学习的<code>error</code>到底来自哪里？这个是比较必要的，可以给我们提高自己的模型提供方案。</p><p>这个李宏毅老师讲的很清楚，有需要可以学习一下，附上链接:<a href="https://www.bilibili.com/video/av48285039?p=8" target="_blank" rel="noopener">https://www.bilibili.com/video/av48285039?p=8</a></p><h2 id="Error来源概述"><a href="#Error来源概述" class="headerlink" title="Error来源概述"></a>Error来源概述</h2><p>深度学习种<code>Error</code> 来源于两方面：</p><ul><li><strong>bias</strong> 偏置</li><li><strong>variance</strong> 方差</li></ul><p><a href="https://user-images.githubusercontent.com/60562661/74254221-1cc7a580-4d2b-11ea-9b91-35b7d92a3efc.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom:50%;" data-src="https://user-images.githubusercontent.com/60562661/74254221-1cc7a580-4d2b-11ea-9b91-35b7d92a3efc.png" class="lazyload"></a></p><h2 id="Variance"><a href="#Variance" class="headerlink" title="Variance"></a>Variance</h2><p>直观上，方差可以了解为散布的分散程度。上图可以明显的看出比较高的方差，点的散布很分散，比较低的方差则比较紧凑。方差可以用来形容模型的稳定性。一般来说，</p><p>比较<strong>简单</strong>的模型，方差较<strong>小</strong>，分布比较<strong>紧密</strong>；比较<strong>复杂</strong>的模型，方差比较<strong>大</strong>，分布比较<strong>分散</strong></p><p>可以如下解释：</p><ul><li>比较简单的模型，比如 f(x) = c，受数据的影响为0，因此所有值相同，方差为0，分布都在一起</li><li>比价复杂的模型，比如五次方方程，受到数据的影响会比较大，取到的值很多，因此每次的预测值可能相差很远，此时方差就比较大了</li></ul><h2 id="Bias"><a href="#Bias" class="headerlink" title="Bias"></a>Bias</h2><p>将所有预测出来的函数求出期望值得到的函数距离真实函数依然会有一段距离，这就是<strong>bias</strong>，如下图所示，</p><p><code>红线表示100个预测出来的函数，蓝线表示这100个函数的平均，黑线表示真实函数</code></p><p><a href="https://user-images.githubusercontent.com/60562661/74254439-6912e580-4d2b-11ea-881f-9cfd6cbbafc9.png" data-fancybox="group" data-caption="undefined" class="fancybox"><img style="zoom: 50%;" data-src="https://user-images.githubusercontent.com/60562661/74254439-6912e580-4d2b-11ea-881f-9cfd6cbbafc9.png" class="lazyload"></a></p><p>可以看出，<strong>简单</strong>的模型bias比较<strong>大</strong>，而<strong>复杂</strong>的模型bias比较<strong>小</strong></p><p>可以解释为，上图的底部，<strong>我们所设计模型的复杂程度其实也就决定了函数所能表达的范围</strong>，过于简单的函数所包含的范围可能根本没有包含到真值，因此bias比较大；而比较复杂的函数范围比较大，包含到了真值，因此bias会比较小。</p><h2 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h2><div class="table-container"><table><thead><tr><th>模型复杂程度</th><th>方差 Variance(精确性)</th><th>偏置Bias(准确性)</th></tr></thead><tbody><tr><td>Simple Model</td><td>较小</td><td>较大</td></tr><tr><td>Complex Model</td><td>较大</td><td>较小</td></tr></tbody></table></div><h2 id="总结与改进"><a href="#总结与改进" class="headerlink" title="总结与改进"></a>总结与改进</h2><p>了解了误差的来源，那么如何判断自己的模型是什么问题呢？</p><ul><li><p>如果模型无法适应训练数据，也就是说在<strong>训练集上误差比较大</strong>，此时就是 <strong>bias</strong> 比较大，此时处于<strong>欠拟合状态，（underfitting）</strong>也就是模型过于简单，此时改进：</p><ul><li>载入更多的特征</li><li>创建更复杂的模型</li><li><strong>注意，此时收集数据集增加数据集并起不到作用</strong></li></ul></li><li><p>如果模型在训练集表现得很好，但是在测试集表现得很差，此时就是<strong>Variance比较大</strong>，也就是<strong>过拟合（overfitting）</strong>了，此时改进：</p><ul><li>增加数据集，对于过拟合是一个很好的解决办法，如果采集不到更多的数据，则可以利用现有的数据去生成更多新的数据</li><li><strong>Regularization</strong> 正则化 ，正则化的效果是使得到的曲线更加的平滑，如下图</li></ul></li></ul><p><a href="https://user-images.githubusercontent.com/60562661/74254459-6dd79980-4d2b-11ea-950a-df5101cc68af.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74254459-6dd79980-4d2b-11ea-950a-df5101cc68af.png" class="lazyload"></a></p><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>以线性回归为例，在损失函数的最后加上一项 参数 <strong>wi的平方和</strong>，这就会要求w越小越好，起到限制作用</p><p><a href="https://user-images.githubusercontent.com/60562661/74254484-77f99800-4d2b-11ea-8a9b-d9a6e32b63cf.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74254484-77f99800-4d2b-11ea-8a9b-d9a6e32b63cf.png" class="lazyload"></a></p><ul><li>此时函数会比较平滑，平滑即对输入不会非常敏感，至于为什么会比较平滑，可以看上图下面部分，当xi 变化时，输出结果就加了一项 wi * 变化量， 若wi 很小接近0，对结果影响并不大，因此会比较平滑。</li><li><p>“莱姆大” 正则化项的系数控制了函数的平滑程度，函数不是月平滑越好，有个度，因此有一个系数控制</p></li><li><p>对于偏置b，不用加正则化项是因为 ： 正则化使得函数更加平滑，能过滤掉杂色信息，而偏置 b 只能使得函数上下移动，对本来的目的并没有帮助。因此正则化项没有偏置 b 。</p></li></ul></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ERROR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>梯度下降法 ②_学习率</title>
      <link href="/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/"/>
      <url>/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-%E2%91%A1/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h1 id="梯度下降法及其优化"><a href="#梯度下降法及其优化" class="headerlink" title="梯度下降法及其优化"></a>梯度下降法及其优化</h1><p>在上一篇文章中已经讲述了梯度下降法的基本流程，也讲了梯度下降法存在的问题，这篇文章就来继续讲解梯度下降法的后续。</p><p>前一篇文章提到学习率的问题，如果学习率过大会导致震荡而难以收敛，过小的话又会收敛太慢，耗费时间，因此出现了Adagrad.</p><h2 id="Tip1：Adaptive-Learning-Rate"><a href="#Tip1：Adaptive-Learning-Rate" class="headerlink" title="Tip1：Adaptive Learning Rate"></a>Tip1：Adaptive Learning Rate</h2><p>学习率对于参数调整影响很大，因此需要仔细地调整，手动调节参数肯定是不太现实，而自适应的调整参数有以下两个原则：</p><ul><li>开始阶段学习率比较大</li><li>随着迭代次数的增加越来越小</li></ul><h2 id="Adagrad-Gradient-Descent"><a href="#Adagrad-Gradient-Descent" class="headerlink" title="Adagrad Gradient Descent"></a>Adagrad Gradient Descent</h2><h3 id="与Gradient-Descent的比较"><a href="#与Gradient-Descent的比较" class="headerlink" title="与Gradient Descent的比较"></a>与Gradient Descent的比较</h3><p><a href="https://user-images.githubusercontent.com/60562661/74164696-17048e00-4c5f-11ea-8f27-b56011c83c1a.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74164696-17048e00-4c5f-11ea-8f27-b56011c83c1a.png" class="lazyload"></a></p><h3 id="数学公式推导"><a href="#数学公式推导" class="headerlink" title="数学公式推导"></a>数学公式推导</h3><p>原始学习率变成了 <code>常数/梯度的平方和开根号</code>，这个公式也是又推导过程的，如下(笔记字比较丑)：</p><p><a href="https://user-images.githubusercontent.com/60562661/74164719-1cfa6f00-4c5f-11ea-985b-cca1a74e4a77.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74164719-1cfa6f00-4c5f-11ea-985b-cca1a74e4a77.png" class="lazyload"></a></p><h3 id="其中的矛盾"><a href="#其中的矛盾" class="headerlink" title="其中的矛盾"></a>其中的矛盾</h3><p>观察最后一步，可以看出后面的  <strong>梯度 </strong> 和  <strong>分母的梯度的平方和开根号 </strong> （最后一行红笔画出来的）是矛盾的，梯度越大，分母也会越大，约束整个函数，但是效果会比较好。这个可以解释为，不只是考虑了一阶偏导数g(t),同时也考虑了二阶偏导数，所以结果会更加准确。(具体的可以参考李宏毅深度学习视频，前一篇文章有提到。)</p><h3 id="Adagrad总结"><a href="#Adagrad总结" class="headerlink" title="Adagrad总结"></a>Adagrad总结</h3><p>Adagrad梯度下降法其实是实现了自适应的去调节学习率，不在需要手动的仔细去调节。</p><h2 id="Tip2：Stochastic-Gradient-Descent"><a href="#Tip2：Stochastic-Gradient-Descent" class="headerlink" title="Tip2：Stochastic Gradient Descent"></a>Tip2：Stochastic Gradient Descent</h2><p>随机梯度下降法也是比较常见的，是梯度下降法的一种变体，改变也不多，它与梯度下降法的不同之处在于：</p><p>​        假设有十组训练数据，也就是十个函数，梯度下降法会把每一组参数带进去计算损失求和，每组参数的梯度也相应会进行求和嘛，然后才更新一次参数；随机梯度下降法不再需要求和这一过程，即随便一组数据带入损失函数求出梯度直接更新参数。</p><p>这种方法听起来也比较一般，它的主要优点在于<strong>更新参数快，</strong>  天下武功，唯快不破嘛。</p><h2 id="Tip3：Feature-scaling"><a href="#Tip3：Feature-scaling" class="headerlink" title="Tip3：Feature scaling"></a>Tip3：Feature scaling</h2><p>特征归一化。在做梯度下降法时，只要梯度分不一样，可以归一化之后在做，主要是因为如果某一个特征值非常大，那么它所占的比重就会非常大，这对学习非常不利。</p><p>常用的一个归一化方法：</p><p>对于特征 x1,x2,x3······xn,每种特征的某一个维度求一下这列数的均值和标准差，然后原始数据减去均值除以标准差即可归一化，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/74164700-18ce5180-4c5f-11ea-880f-770b8016f100.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74164700-18ce5180-4c5f-11ea-880f-770b8016f100.png" class="lazyload"></a></p><p><strong>这样做之后这一维数据就会服从均值为0，方差为1的高斯分布。</strong></p><h2 id="Code-梯度下降法python实现"><a href="#Code-梯度下降法python实现" class="headerlink" title="Code:梯度下降法python实现"></a>Code:梯度下降法python实现</h2><h3 id="背景："><a href="#背景：" class="headerlink" title="背景："></a>背景：</h3><p><strong>函数（model）</strong>：y = w1<em> x^2 + w2</em>x + b ,计算出参数 w1 w2 b，给顶的真值是w1_truth = 1.8，w2_truth=2.4，b_truth = 5.6</p><p><strong>loss function</strong>   ：均方误差，公式写出太麻烦，就是y的真实值减去y的预测值的平方和</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写一个深度学习的回归案例</span></span><br><span class="line"><span class="comment">#y = w1*x^2 + w2*x + b ,计算出参数 w1 w2 b</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成数据 </span></span><br><span class="line">x_data = np.random.rand(<span class="number">10</span>)</span><br><span class="line">w1_truth = <span class="number">1.8</span></span><br><span class="line">w2_truth = <span class="number">2.4</span></span><br><span class="line">b_truth = <span class="number">5.6</span></span><br><span class="line">y_data = np.random.rand(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    y_data[i] = w1_truth*x_data[i]*x_data[i] + w2_truth*x_data[i] + b_truth</span><br><span class="line">print(y_data.shape)</span><br><span class="line">print(x_data.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">w1 = <span class="number">6</span></span><br><span class="line">w2 = <span class="number">4</span></span><br><span class="line">b = <span class="number">8</span></span><br><span class="line">lr = <span class="number">1</span></span><br><span class="line">steps = <span class="number">100000</span></span><br><span class="line">lr_w1 = <span class="number">0</span></span><br><span class="line">lr_w2 = <span class="number">0</span></span><br><span class="line">lr_b = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(steps):</span><br><span class="line">    w1_grad = <span class="number">0</span></span><br><span class="line">    w2_grad = <span class="number">0</span></span><br><span class="line">    b_grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        w1_grad = w1_grad - <span class="number">2</span>*(y_data[i] - b-w1*x_data[i]*x_data[i]-w2*x_data[i])* (x_data[i]**<span class="number">2</span>)</span><br><span class="line">        w2_grad = w2_grad - <span class="number">2</span>*(y_data[i] - b-w1*x_data[i]*x_data[i]-w2*x_data[i])* x_data[i]</span><br><span class="line">        b_grad = b_grad - <span class="number">2</span>*(y_data[i] - b-w1*x_data[i]*x_data[i]-w2*x_data[i])* <span class="number">1.0</span></span><br><span class="line">    lr_w1 = lr_w1 + w1_grad**<span class="number">2</span></span><br><span class="line">    lr_w2 = lr_w2 + w2_grad**<span class="number">2</span></span><br><span class="line">    lr_b = lr_b + b_grad**<span class="number">2</span></span><br><span class="line">    <span class="comment"># update</span></span><br><span class="line">    w1 = w1 - lr/np.sqrt(lr_w1) * w1_grad</span><br><span class="line">    w2 = w2 - lr/np.sqrt(lr_w2) * w2_grad</span><br><span class="line">    b  = b  - lr/np.sqrt(lr_b) * b_grad</span><br><span class="line">print(<span class="string">"w1= %f,w2 = %f, b= %f"</span> % (w1,w2,b))</span><br><span class="line"><span class="comment">#以下是代码的输出结果，可以自己测试一下，完美找到了真实值</span></span><br><span class="line"><span class="comment"># w1= 1.800000,w2 = 2.400000, b= 5.600000</span></span><br></pre></td></tr></tbody></table></figure></div><h3 id="代码中的一些细节"><a href="#代码中的一些细节" class="headerlink" title="代码中的一些细节"></a><strong>代码中的一些细节</strong></h3><ul><li><p>以上代码如果只用一个学习率，收敛会很慢，而且10万个迭代也不会迭代导最终结果，差距还是比较大的，因此代码用的是Adagrad梯度下降法，纯粹手工实现的，可以完美的预测出结果</p></li><li><p>至于代码中的计算 w1 w2 b 的梯度公式，可以手工推导出来，如下图：</p></li></ul><p><a href="https://user-images.githubusercontent.com/60562661/74164747-24217d00-4c5f-11ea-8506-fdaf73f9c02d.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74164747-24217d00-4c5f-11ea-8506-fdaf73f9c02d.png" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 梯度下降法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>梯度下降法 ①_概述</title>
      <link href="/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/"/>
      <url>/2020/02/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E4%B8%80-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>我是研究CV方向的，但是深度学习我只是在学习Tensorflow书的时候学过里面的东西，很多东西讲的过于简单，理解并不直观，所以最近就在学习深度学习的视频，是李宏毅老师讲的，内容很生动，解决了我很多疑惑，附上视频地址->，看视频就要跟着老师推导一遍公式，也是理解了很多内在的东西。</p><p><strong>bilibili</strong> : <a href="https://www.bilibili.com/video/av48285039?p=43" target="_blank" rel="noopener">https://www.bilibili.com/video/av48285039?p=43</a></p><p><strong>Youtube</strong>: <a href="https://www.youtube.com/watch?v=D_S6y0Jm6dQ" target="_blank" rel="noopener">https://www.youtube.com/watch?v=D_S6y0Jm6dQ</a></p><h2 id="概述：深度学习流程"><a href="#概述：深度学习流程" class="headerlink" title="概述：深度学习流程"></a>概述：深度学习流程</h2><p>深度学习过程于机器学习类似，这里就先讲一下机器学习的，深度学习后面会讲到。对于一个特定的<strong>机器学习</strong> <code>Machine Learning</code>任务，简单来说有以下固定的三个步骤：</p><ul><li><p><strong>定义</strong>一系列的函数 ；</p></li><li><p>评价函数的好坏，也就是定义损失函数；</p></li><li><p>找出最好的函数(model)</p></li></ul><p><a href="https://user-images.githubusercontent.com/60562661/74164678-14099d80-4c5f-11ea-961c-fd913422405a.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74164678-14099d80-4c5f-11ea-961c-fd913422405a.png" class="lazyload"></a></p><p>在定义好函数和确定损失函数之后，就要开始调整参数。这时候就需要用到<strong>梯度下降法</strong>了。</p><h2 id="梯度下降法概述"><a href="#梯度下降法概述" class="headerlink" title="梯度下降法概述"></a>梯度下降法概述</h2><p>调整参数就是为了师让损失函数最小，也就是选择出最好的函数 （Model）。先讲一下梯度下降法更新参数的真个流程。</p><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>首先，梯度Gradient，就是在这一点的微分（偏导数），也就是曲线在该点切线的斜率，高等数学就有讲过；不理解微分就可以当作是等高线图的法线方向。梯度代表了函数上升最快的方向，所以一个函数在一点求出梯度，然后按照梯度的反方向偏移就是减小最快的。</p><p><strong>所以这里也可以得知，首先需要一个可微的函数。</strong></p><h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p><a href="https://user-images.githubusercontent.com/60562661/74164735-208df600-4c5f-11ea-842d-dd446164c461.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74164735-208df600-4c5f-11ea-842d-dd446164c461.png" class="lazyload"></a></p><ol><li><p>假设对于参数w，要更新w，首先要初始化w的值，可以随机初始化一个值，当然也有其他方法这里并不关心</p></li><li><p>计算损失函数 Loss function，L(·)对于参数w在该点处的偏导数，这就是函数在这一点的梯度。为什么是偏导数，因为参数不只是w一个，也有bias偏置b。</p></li><li><p>计算出梯度就知道在那个方向上损失函数降幅最大，就开始更新参数，直到一个局部最小值。</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w_new = w_original - lr*w_original_graident</span><br></pre></td></tr></tbody></table></figure></div></li><li><p>重复 <strong>2、3</strong> 步骤，就可以不断调整参数</p></li></ol><p><strong>Note :</strong> </p><ul><li>上述伪代码中有一个 lr参数，这是学习率Learning Rate，学习率决定了每次下降的步长，学习率影响还是比较大的，过小需要的时间太久，过大的话又会震荡不能收敛，这也是梯度下降法的一个存在的问题，后续会继续讲。</li><li>上述第三步提到会函数会降低到局部最小值，这里整个是以线性函数为例，故不用考虑是否会降到局部最小值。</li></ul><hr><h3 id="关于梯度下降法，第二篇文章会继续讲解一些细节及其优化。"><a href="#关于梯度下降法，第二篇文章会继续讲解一些细节及其优化。" class="headerlink" title="关于梯度下降法，第二篇文章会继续讲解一些细节及其优化。"></a>关于梯度下降法，第二篇文章会继续讲解一些细节及其优化。</h3></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 梯度下降法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hexo_blog搭建2-主题相关</title>
      <link href="/2020/02/09/hexo-blog%E6%90%AD%E5%BB%BA2-%E4%B8%BB%E9%A2%98%E7%9B%B8%E5%85%B3/"/>
      <url>/2020/02/09/hexo-blog%E6%90%AD%E5%BB%BA2-%E4%B8%BB%E9%A2%98%E7%9B%B8%E5%85%B3/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>我的这个版本博客是基于Hexo搭建的，主题采用的是 <a href="https://jerryc.me/posts/21cfbf15/#" target="_blank" rel="noopener">Butterfly</a>，在这个网址有详细的安装配置教程。 这个主题我非非常喜欢，看了很多的主题就相中了这款。然而配置也是花了我整整一天，可能还算是比较顺利把！配置过程中有的地方安装文档讲的不是很详细，我自己也踩坑了，来记录一下！</p><p> <a href="https://user-images.githubusercontent.com/60562661/74105856-93826880-4b9c-11ea-8629-4b1b724d54f4.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74105856-93826880-4b9c-11ea-8629-4b1b724d54f4.png" class="lazyload"></a></p><h2 id="添加-Gallery（相册）"><a href="#添加-Gallery（相册）" class="headerlink" title="添加 Gallery（相册）"></a>添加 Gallery（相册）</h2><h3 id="新建页面"><a href="#新建页面" class="headerlink" title="新建页面"></a>新建页面</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page gallery <span class="comment">#此时会在source目录下产生gallery/index.md</span></span><br></pre></td></tr></tbody></table></figure></div><p>打开来编辑这个md文件，有两点要注意的：</p><ul><li>开头的 <strong>type: “gallery”</strong>   不能出错</li><li>内容格式 按照  <strong><div class="justified-gallery"><p>img1  img2  img3 </p>          </div></strong>    来写</li></ul><p><a href="https://user-images.githubusercontent.com/60562661/74105571-12c26d00-4b9a-11ea-8e58-9cb4520896fd.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74105571-12c26d00-4b9a-11ea-8e58-9cb4520896fd.png" class="lazyload"></a></p><h3 id="添加导航栏"><a href="#添加导航栏" class="headerlink" title="添加导航栏"></a>添加导航栏</h3><p>在 butterfly.yml 文件的menu下照着之前的添加相册，如下图</p><p><a href="https://user-images.githubusercontent.com/60562661/74105539-d5f67600-4b99-11ea-83f1-f856a034b9b3.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74105539-d5f67600-4b99-11ea-83f1-f856a034b9b3.png" class="lazyload"></a></p><p>至此，相册添加结束</p><h2 id="添加本地搜索"><a href="#添加本地搜索" class="headerlink" title="添加本地搜索"></a>添加本地搜索</h2><h3 id="安装hexo本地搜索的插件"><a href="#安装hexo本地搜索的插件" class="headerlink" title="安装hexo本地搜索的插件"></a>安装hexo本地搜索的插件</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></tbody></table></figure></div><h3 id="配置全局的config文件"><a href="#配置全局的config文件" class="headerlink" title="配置全局的config文件"></a>配置全局的config文件</h3><p><a href="https://user-images.githubusercontent.com/60562661/74105544-da229380-4b99-11ea-8ba8-7715f23704b9.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74105544-da229380-4b99-11ea-8ba8-7715f23704b9.png" class="lazyload"></a></p><h3 id="配置butterfly文件"><a href="#配置butterfly文件" class="headerlink" title="配置butterfly文件"></a>配置butterfly文件</h3><p>这里的配置和官方教程一样，不再赘述</p><h3 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h3><p>我自己在这里一直配置不好，刚开始发现是找不到 search.xml 文件，后来我在博客插件目录找到了，然后改上图的path，这样在本地启动blog是可以搜索的，但是推送到远程就不行了，后面我重装了插件，path改成默认路径,然后 <code>hexo clean</code>  <code>hexo g</code> <code>hexo d</code>  就很神奇的在github仓库生成了search.xml文件，也就随之可以用了，具体原理我还没搞明白，以后明白了再更新。</p><h2 id="添加评论"><a href="#添加评论" class="headerlink" title="添加评论"></a>添加评论</h2><p>我用的是 Valine 评论系统，这个很容易搭建起来，大致描述一下流程：</p><h3 id="配置-valine"><a href="#配置-valine" class="headerlink" title="配置 valine"></a>配置 valine</h3><ol><li><p>注册 <a href="https://www.leancloud.cn/" target="_blank" rel="noopener">https://www.leancloud.cn/</a> 在这个网站注册账号，然后进行实名认证</p></li><li><p>创建一个应用</p></li><li><p>进入设置页面，在应用keys中可以看到自己的 <strong>AppID</strong> <strong>AppKey</strong>   等会儿会用到</p></li><li><p><strong>在安全中心页面 web安全域名写入自己的博客域名</strong>（这一步很重要，我就是一开始在这里栽了）</p><p><strong>至此，valine配置结束</strong></p></li></ol><h3 id="配置主题文件"><a href="#配置主题文件" class="headerlink" title="配置主题文件"></a>配置主题文件</h3><p>这里的流程和官方教程一样，很容易，自己直接会明白</p><p><strong>我在这里卡住是因为写 APPID APPKey出了问题，卡了很久才发现</strong></p><p>然后重新更新生成就有了评论功能！</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
            <tag> 博客主题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>拥有自己优雅的图床</title>
      <link href="/2020/02/09/%E6%8B%A5%E6%9C%89%E8%87%AA%E5%B7%B1%E4%BC%98%E9%9B%85%E7%9A%84%E5%9B%BE%E5%BA%8A/"/>
      <url>/2020/02/09/%E6%8B%A5%E6%9C%89%E8%87%AA%E5%B7%B1%E4%BC%98%E9%9B%85%E7%9A%84%E5%9B%BE%E5%BA%8A/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p> 图床，就是专门用来存放图片的空间，不过与本地不同，图床是存储在网络上的。这样图片会有个地址，可以访问到图片，也可以引用。如果写个人博客等等肯定用得上。github这个天然的图床不用就太可惜了！ 所以选择github来作为自己图床。</p><p><a href="https://user-images.githubusercontent.com/60562661/74098587-0831b480-4b55-11ea-992f-e7dd9181abe7.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74098587-0831b480-4b55-11ea-992f-e7dd9181abe7.png" class="lazyload"></a></p><h2 id="仓库准备"><a href="#仓库准备" class="headerlink" title="仓库准备"></a>仓库准备</h2><p>可以新建一个仓库，也可以在现有的仓库下。具体怎么建仓库请自己寻找。</p><p><strong>下面以我自己的仓库为例</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/74098589-09fb7800-4b55-11ea-9ac8-6995b63f32e4.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74098589-09fb7800-4b55-11ea-9ac8-6995b63f32e4.png" class="lazyload"></a></p><h2 id="进入issues新建问题"><a href="#进入issues新建问题" class="headerlink" title="进入issues新建问题"></a>进入issues新建问题</h2><p>点击issues，然后点击右上角NewIssues，进入如下界面</p><p><a href="https://user-images.githubusercontent.com/60562661/74098590-0a940e80-4b55-11ea-9157-59279231e403.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74098590-0a940e80-4b55-11ea-9157-59279231e403.png" class="lazyload"></a></p><p>关键部分红色框已经框出来，可以在这里上传自己的图片，然后点击提交即可，图下图</p><p><a href="https://user-images.githubusercontent.com/60562661/74098591-0b2ca500-4b55-11ea-8540-b66aafb27fa7.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/74098591-0b2ca500-4b55-11ea-8540-b66aafb27fa7.png" class="lazyload"></a></p><p>此时点击图片，即可打开一个新的链接，此时图片的网址就可以被引用了。</p><p><strong>Enjoy！</strong></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图床 </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>自己的域名与github绑定</title>
      <link href="/2020/02/06/%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9F%9F%E5%90%8D%E4%B8%8Egithub%E7%BB%91%E5%AE%9A/"/>
      <url>/2020/02/06/%E8%87%AA%E5%B7%B1%E7%9A%84%E5%9F%9F%E5%90%8D%E4%B8%8Egithub%E7%BB%91%E5%AE%9A/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>一般情况下用github托管的博客很方便，但是域名访问只能用 <a href="https://xxxx.github.io来访问，这时候就有与自己的域名绑定的问题，即自己的域名指向github地址。（以腾讯云为例）" target="_blank" rel="noopener">https://xxxx.github.io来访问，这时候就有与自己的域名绑定的问题，即自己的域名指向github地址。（以腾讯云为例）</a></p><p><a href="https://user-images.githubusercontent.com/60562661/73957566-7ef66400-4941-11ea-87ee-e9c53fdbc305.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73957566-7ef66400-4941-11ea-87ee-e9c53fdbc305.png" class="lazyload"></a></p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>开始操作前，确保自己已经有：</p><ul><li><strong>可用</strong>的域名：注意不需要备案（我的是这样），但是大多数需要实名认证</li><li>有自己的<strong>github</strong>项目地址，确保可以访问</li></ul><h2 id="域名解析"><a href="#域名解析" class="headerlink" title="域名解析"></a>域名解析</h2><p>进入腾讯云控制台，在自己的域名解析处设置如下：</p><p><a href="https://user-images.githubusercontent.com/60562661/73957562-7dc53700-4941-11ea-81f5-472730958808.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73957562-7dc53700-4941-11ea-81f5-472730958808.png" class="lazyload"></a></p><p>这里要注意两点：</p><ul><li>记录类型选择 <strong>CNAME</strong></li><li>记录值为自己的github项目地址（<strong>github域名</strong>）</li></ul><p>此时如果解析成功，<strong>ping</strong>一下测试：</p><p><a href="https://user-images.githubusercontent.com/60562661/73957564-7e5dcd80-4941-11ea-93c1-501060db97b2.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73957564-7e5dcd80-4941-11ea-93c1-501060db97b2.png" class="lazyload"></a></p><h2 id="github仓库设置"><a href="#github仓库设置" class="headerlink" title="github仓库设置"></a>github仓库设置</h2><p>进入github项目仓库设置，往下翻在 Github Page 中自定义地址写自己的域名 然后保存，此时主页会自动生成 CNAME 文件，如下图：</p><p><a href="https://user-images.githubusercontent.com/60562661/73957559-7c940a00-4941-11ea-8f4c-b56ac851d659.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73957559-7c940a00-4941-11ea-8f4c-b56ac851d659.png" class="lazyload"></a></p><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><h3 id="等待时间"><a href="#等待时间" class="headerlink" title="等待时间"></a>等待时间</h3><p>等待10-15min，就可以通过自己的域名来访问了。注意能ping通说明已经成功，此时就不要频繁修改设置了，影响同步 时间</p><h3 id="普适性"><a href="#普适性" class="headerlink" title="普适性"></a>普适性</h3><p>以上流程在hugo博客上毫无问题，原因是hugo博客推送到服务器是推送public中的内容，而CNAME在根目录并不会影响；</p><p>但是在hexo博客上就有问题了，我查了一下发现如果直接在gtihub直接指定，会在github仓库上直接生成CNAME文件，而这时候有个问题就是在写文章同步的话，本地并没有CNAME文件，会直接覆盖掉已经生成的CNAME 文件，此时指定的域名就不生效了。因此有了以下的章节。</p><h2 id="Hexo博客绑定域名"><a href="#Hexo博客绑定域名" class="headerlink" title="Hexo博客绑定域名"></a>Hexo博客绑定域名</h2><p>首先在本地 博客根目录中source文件夹下新建一个CNAME文件，内容就是自己的域名，然后再按照上面去github指定。</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 域名 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从0开始搭建私人云盘</title>
      <link href="/2020/02/06/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E7%A7%81%E4%BA%BA%E4%BA%91%E7%9B%98/"/>
      <url>/2020/02/06/%E4%BB%8E0%E5%BC%80%E5%A7%8B%E6%90%AD%E5%BB%BA%E7%A7%81%E4%BA%BA%E4%BA%91%E7%9B%98/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>现在的云盘有很多，百度云、蓝奏云、腾讯微云等等，看起来似乎是没有搭建云盘的必要，但是百度云限速让我觉得恶心，不开超级会员就非常非常非常慢，开了又觉得亏，所以就想到了能不能自己搭建一个云盘。自己搭建的私有云还有一个特点就是比较安全，其实只是自己比较喜欢乱鼓捣新东西。现在的下载速度并不快，我还没研究，但是上传速度经过修改可以达到2M/s以上，其实我的云服务器比较垃圾，毕竟是1块钱白嫖了一个月—。</p><p><strong>先上链接：</strong></p><p><a href="http://111.229.74.50/kodexplorer/index.php?explorer" target="_blank" rel="noopener">http://111.229.74.50/kodexplorer/index.php?explorer</a></p><p><a href="https://user-images.githubusercontent.com/60562661/73954603-02618680-493d-11ea-9ebd-b33ee3002b8e.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73954603-02618680-493d-11ea-9ebd-b33ee3002b8e.png" class="lazyload"></a></p><h2 id="购买一个云服务器"><a href="#购买一个云服务器" class="headerlink" title="购买一个云服务器"></a>购买一个云服务器</h2><p>云服务器性能越好，与上传下载速度都有很大关系，结合自己个人需求、经济状况适当购买。当然也可以先白嫖一个用用体验一下。购买云服务器国内比较火的也就几个：<strong>阿里云，腾讯云，京东云，华为云。</strong>我用的腾讯云的服务器。<strong>↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓</strong></p><p><a href="https://user-images.githubusercontent.com/60562661/73954616-068da400-493d-11ea-9634-e7cad5cde6eb.png" data-fancybox="group" data-caption="photo" class="fancybox"><img alt="photo" title="photo" data-src="https://user-images.githubusercontent.com/60562661/73954616-068da400-493d-11ea-9634-e7cad5cde6eb.png" class="lazyload"></a></p><h2 id="开始配置服务器"><a href="#开始配置服务器" class="headerlink" title="开始配置服务器"></a>开始配置服务器</h2><p><strong>Note：服务器以windows操作系统为例；本私有云基于可道云kodexplorer</strong></p><h3 id="远程登陆到服务器"><a href="#远程登陆到服务器" class="headerlink" title="远程登陆到服务器"></a>远程登陆到服务器</h3><p>以腾讯云为例，上图中主机后面就有登录，可以直接登录，登录之后界面是这样的：(每个人的服务器界面会有点不一样，win比较方便，但是 linux更适合作为服务器系统)</p><p><a href="https://user-images.githubusercontent.com/60562661/73954608-04c3e080-493d-11ea-8fcb-52f59f4acc8d.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73954608-04c3e080-493d-11ea-8fcb-52f59f4acc8d.png" class="lazyload"></a></p><h3 id="配置PHP环境"><a href="#配置PHP环境" class="headerlink" title="配置PHP环境"></a>配置PHP环境</h3><p>下载安装Xampp (XAMPP 是一个把 Apache 网页服务器与 PHP、Perl 及 MariaDB 集合在一起的安装包，允许用户可以在自己的电脑上轻易的建立网页服务器环境。)</p><p>中文官网下载地址 ： <a href="https://www.apachefriends.org/zh_cn/download.html。" target="_blank" rel="noopener">https://www.apachefriends.org/zh_cn/download.html。</a> </p><p>安装下一步下一步就可以，安装完之后开启<strong>Apache、 MySQL</strong>两项，如下图</p><p><a href="https://user-images.githubusercontent.com/60562661/73954610-055c7700-493d-11ea-879d-3a9c99c2e006.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73954610-055c7700-493d-11ea-879d-3a9c99c2e006.png" class="lazyload"></a></p><p>点击Admin进入相应页面（如下图）即说明已经配置好。就是这么容易 。</p><p><a href="https://user-images.githubusercontent.com/60562661/73954613-05f50d80-493d-11ea-86f3-da2ae7e8a8b0.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73954613-05f50d80-493d-11ea-86f3-da2ae7e8a8b0.png" class="lazyload"></a></p><h3 id="下载和安装可道云kodexplorer"><a href="#下载和安装可道云kodexplorer" class="headerlink" title="下载和安装可道云kodexplorer"></a>下载和安装可道云kodexplorer</h3><p>下载地址：<a href="http://kodcloud.com/download.html" target="_blank" rel="noopener">http://kodcloud.com/download.html</a></p><p>下载完成后解压，将可道云文件夹复制到xmapp安装文件夹下的htdocs文件夹，此时已经完成</p><p><strong>Note: 可道云文件夹命名应为</strong><code>kodexplorer</code>(有点常识都会理解)</p><h3 id="访问自己的私有云"><a href="#访问自己的私有云" class="headerlink" title="访问自己的私有云"></a>访问自己的私有云</h3><p><a href="http://112.xxx.xx.xx/kodexplorer/index.php?user/login" target="_blank" rel="noopener">http://112.xxx.xx.xx/kodexplorer/index.php?user/login</a></p><h2 id="可道云配置优化"><a href="#可道云配置优化" class="headerlink" title="可道云配置优化"></a>可道云配置优化</h2><p>操作完2已经可以访问上传下载添加用户了。但是此时上传速度很慢，此时就需要配置一下了，以下操作我亲自测试有用，但是下载还是比较慢，我再研究一下。</p><h3 id="修改php-ini上传限制"><a href="#修改php-ini上传限制" class="headerlink" title="修改php.ini上传限制"></a>修改php.ini上传限制</h3><p>在xmapp安装目录直接搜索该文件即可</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">php</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight php"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">max_execution_time = <span class="number">3600</span></span><br><span class="line">max_input_time = <span class="number">3600</span></span><br><span class="line">post_max_size = <span class="number">150</span>M</span><br><span class="line">upload_max_filesize = <span class="number">150</span>M</span><br></pre></td></tr></tbody></table></figure></div><h3 id="修改可道云配置"><a href="#修改可道云配置" class="headerlink" title="修改可道云配置"></a>修改可道云配置</h3><p>在config/下新建 setting_user.php文件;粘贴如下内容；(已存在则略过)</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">php</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight php"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"><!--?php</span--></span><br><span class="line"></span><br><span class="line"><span class="comment">//分片上传: 每个切片5M,需要php.ini 中upload_max_filesize大于此值</span></span><br><span class="line">$GLOBALS[<span class="string">'config'</span>][<span class="string">'settings'</span>][<span class="string">'updloadChunkSize'</span>] = <span class="number">1024</span>*<span class="number">1024</span>*<span class="number">5</span>;   </span><br><span class="line"></span><br><span class="line"><span class="comment">//上传并发数量; 推荐15个并发;</span></span><br><span class="line">$GLOBALS[<span class="string">'config'</span>][<span class="string">'settings'</span>][<span class="string">'updloadThreads'</span>] = <span class="number">15</span>;</span><br></span></pre></td></tr></tbody></table></figure></div><hr><p>完结，撒花，下载速度慢研究出来原因再更新</p><p><a href="https://user-images.githubusercontent.com/60562661/73954934-7d2aa180-493d-11ea-989f-f16d19aca743.jpg" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73954934-7d2aa180-493d-11ea-989f-f16d19aca743.jpg" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cloud </tag>
            
            <tag> Web Application </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo_blog搭建部署笔记</title>
      <link href="/2020/02/05/MyBlog-Hexo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/"/>
      <url>/2020/02/05/MyBlog-Hexo%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><h2 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h2><p><strong>NodeJS</strong> + <strong>hexo</strong> + <strong>git</strong>, 这三个安装教程可以自行百度到，很多博客里也有，这里便不再赘述</p><h2 id="搭建博客"><a href="#搭建博客" class="headerlink" title="搭建博客"></a>搭建博客</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></tbody></table></figure></div><h3 id="写一篇新博客"><a href="#写一篇新博客" class="headerlink" title="写一篇新博客"></a>写一篇新博客</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new <span class="string">"helloworld"</span> <span class="comment">#此时在source/_posts下会生成文章</span></span><br></pre></td></tr></tbody></table></figure></div><h3 id="运行测试"><a href="#运行测试" class="headerlink" title="运行测试"></a>运行测试</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></tbody></table></figure></div><p><strong>此时基本搭建完成</strong></p><h2 id="部署到github"><a href="#部署到github" class="headerlink" title="部署到github"></a>部署到github</h2><h3 id="安装deploy插件"><a href="#安装deploy插件" class="headerlink" title="安装deploy插件"></a>安装deploy插件</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save hexo-deployer-git</span><br></pre></td></tr></tbody></table></figure></div><h3 id="新建github仓库"><a href="#新建github仓库" class="headerlink" title="新建github仓库"></a>新建github仓库</h3><p>这步比较简单，可以自己查查如何新建</p><h3 id="修改全局配置文件-config-yml："><a href="#修改全局配置文件-config-yml：" class="headerlink" title="修改全局配置文件  _config.yml："></a>修改全局配置文件  _config.yml：</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">yml</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight yml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">https://github.com/fryddup/fryddup.github.io.git</span> <span class="comment">#自己的仓库地址</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></tbody></table></figure></div><h3 id="推到远程服务器"><a href="#推到远程服务器" class="headerlink" title="推到远程服务器"></a>推到远程服务器</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo -d</span><br></pre></td></tr></tbody></table></figure></div><p>此时可以通过 <a href="https://huaqi.blue" target="_blank" rel="noopener">https://huaqi.blue</a> 来访问我的博客</p><hr><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>每次写完博客 hexo clean  hexo g 两步必不可少，然后在 hexo d</p><p><a href="C:\Users\33196\Desktop\0.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="C:\Users\33196\Desktop\0.png" class="lazyload"></a></p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windwos下几款实用又美观的软件记录</title>
      <link href="/2020/02/02/windwos%E4%B8%8B%E5%87%A0%E6%AC%BE%E5%AE%9E%E7%94%A8%E5%8F%88%E7%BE%8E%E8%A7%82%E7%9A%84%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/"/>
      <url>/2020/02/02/windwos%E4%B8%8B%E5%87%A0%E6%AC%BE%E5%AE%9E%E7%94%A8%E5%8F%88%E7%BE%8E%E8%A7%82%E7%9A%84%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>本人是windows重症患者，经常在搜集乱七八糟（好看）的软件，因此来记录几款实用的软件。</p><h2 id="1-Typora"><a href="#1-Typora" class="headerlink" title="1. Typora"></a>1. Typora</h2><p>markdown笔记软件，美观又实用！</p><p>官网：<a href="https://typora.io/" target="_blank" rel="noopener">https://typora.io/</a></p><p><a href="https://user-images.githubusercontent.com/60562661/73609608-62d58880-460a-11ea-8b27-655765e84224.gif" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73609608-62d58880-460a-11ea-8b27-655765e84224.gif" class="lazyload"></a></p><h2 id="2-Choaolatey"><a href="#2-Choaolatey" class="headerlink" title="2. Choaolatey"></a>2. Choaolatey</h2><p>这是一款windows下的包管理器，可以像 linux 一样安装各种包，很是方便！</p><p>官网：<a href="https://chocolatey.org/" target="_blank" rel="noopener">https://chocolatey.org/</a></p><p><a href="https://user-images.githubusercontent.com/60562661/73609604-61a45b80-460a-11ea-923c-16414aa000a9.gif" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73609604-61a45b80-460a-11ea-923c-16414aa000a9.gif" class="lazyload"></a></p><h2 id="3-hyper"><a href="#3-hyper" class="headerlink" title="3. hyper"></a>3. hyper</h2><p>hyper是 windows下面很炫酷的一个终端嘛，有各种皮肤、插件！</p><p>官网：<a href="https://hyper.is/" target="_blank" rel="noopener">https://hyper.is/</a></p><p><a href="https://user-images.githubusercontent.com/60562661/73609605-623cf200-460a-11ea-9ac7-f9c937f919fd.gif" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73609605-623cf200-460a-11ea-9ac7-f9c937f919fd.gif" class="lazyload"></a></p><h2 id="4-Listary"><a href="#4-Listary" class="headerlink" title="4. Listary"></a>4. Listary</h2><p>这是一款很方便搜索打开本地文件的软件，有很多功能值得探索</p><p>官网：<a href="https://www.listary.com/" target="_blank" rel="noopener">https://www.listary.com/</a></p><p><a href="https://user-images.githubusercontent.com/60562661/73609607-623cf200-460a-11ea-915a-f9e90a77cfd6.gif" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73609607-623cf200-460a-11ea-915a-f9e90a77cfd6.gif" class="lazyload"></a></p><h2 id="5-IobitUninstaller"><a href="#5-IobitUninstaller" class="headerlink" title="5. IobitUninstaller"></a>5. IobitUninstaller</h2><p>windows上卸载软件用的一款软件，超好用</p><p><a href="https://user-images.githubusercontent.com/60562661/73609606-623cf200-460a-11ea-9942-0fc4b33b02ba.png" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="https://user-images.githubusercontent.com/60562661/73609606-623cf200-460a-11ea-9942-0fc4b33b02ba.png" class="lazyload"></a></p><p>后续再有好的继续补充…..</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 软件环境配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> windows </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hugo_blog搭建部署笔记</title>
      <link href="/2020/02/02/hugo_blog%E6%90%AD%E5%BB%BA%E9%83%A8%E7%BD%B2%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/02/02/hugo_blog%E6%90%AD%E5%BB%BA%E9%83%A8%E7%BD%B2%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>之前一直用的  Hexo Blog ,奈何写出来太丑了，不习惯，部署起来也比较麻烦，就找到了新的好看的，还真的是好看是第一生产力！hugo部署过程如下。</p><p><a href="https://user-images.githubusercontent.com/60562661/73609078-10459d80-4605-11ea-93b4-d26fa0f93625.png" data-fancybox="group" data-caption="hugo" class="fancybox"><img alt="hugo" title="hugo" data-src="https://user-images.githubusercontent.com/60562661/73609078-10459d80-4605-11ea-93b4-d26fa0f93625.png" class="lazyload"></a></p><h2 id="描述版："><a href="#描述版：" class="headerlink" title="描述版："></a>描述版：</h2><h4 id="1-安装-hugo-（有很多方法，可以到官方网站看一下，我是采用chocolate来安装的）"><a href="#1-安装-hugo-（有很多方法，可以到官方网站看一下，我是采用chocolate来安装的）" class="headerlink" title="1.安装 hugo （有很多方法，可以到官方网站看一下，我是采用chocolate来安装的）"></a>1.安装 hugo （有很多方法，可以到官方网站看一下，我是采用chocolate来安装的）</h4><p><code>choco install hugo</code></p><h4 id="2-检测go语言是否安装成功"><a href="#2-检测go语言是否安装成功" class="headerlink" title="2.检测go语言是否安装成功"></a>2.检测go语言是否安装成功</h4><p><code>hugo version</code></p><h4 id="3-开始建站-建立一个-myblog-站点（博客的根目录）"><a href="#3-开始建站-建立一个-myblog-站点（博客的根目录）" class="headerlink" title="3.开始建站,建立一个 myblog 站点（博客的根目录）"></a>3.开始建站,建立一个 myblog 站点（博客的根目录）</h4><p><code>hugo new site myblog</code></p><h4 id="4-安装hugo博客主题-官网-https-themes-gohugo-io-每个主题有对应的方式，我以我自己的为例-此时目录在博客的根目录"><a href="#4-安装hugo博客主题-官网-https-themes-gohugo-io-每个主题有对应的方式，我以我自己的为例-此时目录在博客的根目录" class="headerlink" title="4.安装hugo博客主题 官网 https://themes.gohugo.io/,每个主题有对应的方式，我以我自己的为例,此时目录在博客的根目录"></a>4.安装hugo博客主题 官网 <a href="https://themes.gohugo.io/,每个主题有对应的方式，我以我自己的为例,此时目录在博客的根目录" target="_blank" rel="noopener">https://themes.gohugo.io/,每个主题有对应的方式，我以我自己的为例,此时目录在博客的根目录</a></h4><p><code>git clone https://github.com/flysnow-org/maupassant-hugo themes/maupassant</code></p><h4 id="5-写一篇新博客，位于content-post"><a href="#5-写一篇新博客，位于content-post" class="headerlink" title="5.写一篇新博客，位于content/post/"></a>5.写一篇新博客，位于content/post/</h4><p><code>hugo new post/Hello,world.md</code></p><h4 id="6-启动本地调试，此时位于根目录"><a href="#6-启动本地调试，此时位于根目录" class="headerlink" title="6.启动本地调试，此时位于根目录"></a>6.启动本地调试，此时位于根目录</h4><p><code>hugo server --buildDrafts</code></p><h4 id="7-新建github仓库，这个比较容易，自己解决"><a href="#7-新建github仓库，这个比较容易，自己解决" class="headerlink" title="7.新建github仓库，这个比较容易，自己解决"></a>7.新建github仓库，这个比较容易，自己解决</h4><h4 id="8-关联到GitHub-此步骤会生成public文件夹"><a href="#8-关联到GitHub-此步骤会生成public文件夹" class="headerlink" title="8.关联到GitHub,此步骤会生成public文件夹"></a>8.关联到GitHub,此步骤会生成public文件夹</h4><p><code>hugo --themes=maupassant --baseURL="https://huaqi19.github.io/"</code></p><h4 id="9-将-public-文件夹内容推送到空的github仓库中"><a href="#9-将-public-文件夹内容推送到空的github仓库中" class="headerlink" title="9.将 public 文件夹内容推送到空的github仓库中"></a>9.将 public 文件夹内容推送到空的github仓库中</h4><p>此处步骤省略，如何推送可以参考我的上一篇博客  git推送文件</p><h4 id="10-更新博客"><a href="#10-更新博客" class="headerlink" title="10.更新博客"></a>10.更新博客</h4><p><code>hugo -D</code></p><h4 id="至此博客搭建完成，主题配置请自行研究"><a href="#至此博客搭建完成，主题配置请自行研究" class="headerlink" title="至此博客搭建完成，主题配置请自行研究"></a>至此博客搭建完成，主题配置请自行研究</h4><h2 id="代码版："><a href="#代码版：" class="headerlink" title="代码版："></a>代码版：</h2><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.安装 hugo （有很多方法，可以到官方网站看一下，我是采用chocolate来安装的）</span></span><br><span class="line">choco install hugo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.检测go语言是否安装成功</span></span><br><span class="line">hugo version</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.开始建站,建立一个 myblog 站点（博客的根目录）</span></span><br><span class="line">hugo new site myblog</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.安装hugo博客主题 官网 https://themes.gohugo.io/</span></span><br><span class="line"><span class="comment"># 每个主题有对应的方式，我以我自己的为例</span></span><br><span class="line"><span class="comment"># 此时目录在博客的根目录</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/flysnow-org/maupassant-hugo themes/maupassant</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.写一篇新博客，位于content/post/</span></span><br><span class="line">hugo new post/Hello,world.md</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.启动本地调试，此时位于根目录</span></span><br><span class="line">hugo server --buildDrafts</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7.新建github仓库，这个比较容易，自己解决</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 8.关联到GitHub,此步骤会生成public文件夹</span></span><br><span class="line">hugo --themes=maupassant --baseURL=<span class="string">"https://huaqi19.github.io/"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.将 public 文件夹内容推送到空的github仓库中</span></span><br><span class="line"><span class="comment"># 此处步骤省略，如何推送可以参考我的上一篇博客  git推送文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 10.更新博客</span></span><br><span class="line">hugo -D</span><br><span class="line"></span><br><span class="line"><span class="comment"># 至此博客搭建完成，主题配置请自行研究</span></span><br></pre></td></tr></tbody></table></figure></div></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> blog </tag>
            
            <tag> hugo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git常用操作整合</title>
      <link href="/2020/02/02/git%E6%8E%A8%E9%80%81%E6%96%87%E4%BB%B6/"/>
      <url>/2020/02/02/git%E6%8E%A8%E9%80%81%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<html><head><link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script></head><body><p>github远程托管文件 非常的方便。git操作种比较多的就是本地文件推送到远程，仓库等。而我一直也是没有搞明白，github本地文件远程推送是如何操作的，以至于今天卡了很久，来记录一下。</p><p><a href="https://user-images.githubusercontent.com/60562661/73609075-07ed6280-4605-11ea-9971-35fe0663a9e0.jpg" data-fancybox="group" data-caption="git" class="fancybox"><img alt="git" title="git" data-src="https://user-images.githubusercontent.com/60562661/73609075-07ed6280-4605-11ea-9971-35fe0663a9e0.jpg" class="lazyload"></a></p><h2 id="git推送本地文件"><a href="#git推送本地文件" class="headerlink" title="git推送本地文件"></a>git推送本地文件</h2><h3 id="一般流程"><a href="#一般流程" class="headerlink" title="一般流程"></a>一般流程</h3><p><strong>Note </strong>:  默认远程仓库是空的</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.新建远程仓库 ，这个可以自己去查阅，很容易</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.把本地需要推送的文件夹设置为git仓库</span></span><br><span class="line">git init</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.添加文件</span></span><br><span class="line">git add .</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.提交文件</span></span><br><span class="line">git commit -m <span class="string">'first_commit'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.添加镜像源</span></span><br><span class="line">git remote add origin https://github.com/fryddup/fryddup.github.io.git</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.推送文件</span></span><br><span class="line">git push -u origin master</span><br></pre></td></tr></tbody></table></figure></div><h3 id="同步问题"><a href="#同步问题" class="headerlink" title="同步问题"></a>同步问题</h3><p>以上代码即可实现本地文件推送到远程，但是注意当仓库不是空的，仓库有改动，应当如下操作：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br><span class="line"></span><br><span class="line">git commit -m <span class="string">'first_commit'</span></span><br><span class="line"></span><br><span class="line">git pull  <span class="comment">#注意，此命令即是远程文件同步到本地。</span></span><br><span class="line"></span><br><span class="line">git push -u origin master</span><br></pre></td></tr></tbody></table></figure></div><h2 id="git-代理相关"><a href="#git-代理相关" class="headerlink" title="git 代理相关"></a>git 代理相关</h2><h3 id="设置代理"><a href="#设置代理" class="headerlink" title="设置代理"></a>设置代理</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.proxy <span class="string">'socks5://127.0.0.1:1080'</span> </span><br><span class="line">git config --global https.proxy <span class="string">'socks5://127.0.0.1:1080'</span></span><br></pre></td></tr></tbody></table></figure></div><h3 id="查看代理"><a href="#查看代理" class="headerlink" title="查看代理"></a>查看代理</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global --get http.proxy</span><br><span class="line">git config --global --get https.proxy</span><br></pre></td></tr></tbody></table></figure></div><h3 id="取消代理"><a href="#取消代理" class="headerlink" title="取消代理"></a>取消代理</h3><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global --<span class="built_in">unset</span> http.proxy</span><br><span class="line">git config --global --<span class="built_in">unset</span> https.proxy</span><br></pre></td></tr></tbody></table></figure></div><h2 id="踩坑"><a href="#踩坑" class="headerlink" title="踩坑"></a>踩坑</h2><p>新建仓库，新建<code>README.md</code>，本地 add，commit 之后，push遇到了如下错误：</p><div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">bash</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> ! [rejected]        master -> master (non-fast-forward)</span><br><span class="line">error: failed to push some refs to <span class="string">'https://github.com/tzwx/DeepLearning.git'</span></span><br></pre></td></tr></tbody></table></figure></div><ol><li><code>git pull origin master --allow-unrelated-histories //把远程仓库和本地同步，消除差异</code></li><li><code>git push -u origin master</code></li></ol><p>git其他操作、问题以后遇到了再补充**</p></body></html>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Git </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
